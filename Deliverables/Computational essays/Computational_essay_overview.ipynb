{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae4239d",
   "metadata": {},
   "source": [
    "# Circuits in mechanistic interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881343a",
   "metadata": {},
   "source": [
    "Mechanistic interpretability is an exciting new field of study, aimed at understanding a model's underlying frameworks. With all the different approaches to the relevant problems, and the rapid pace at which the field is moving, it can be difficult to get a grip on the central methods of the field and what they might be used for. This is a complementary essay to our report \"Circuits in Mechanistic interpretability\" (Aronsen, Telle & Ã˜stern), where we implement the methods discussed with specific examples. \n",
    "\n",
    "This essay consists of three parts: \n",
    "1.  Circuit analysis tool for BERT embedding encoder models\n",
    "2.  Attribution with caption\n",
    "3.  Decoder models with TransformerLens\n",
    "\n",
    "Part 1 is a thorough run-through of our work on BERT encoder embedding models, showing central methods such as ablation, activation patching and path patching. It concludes with the discovery of a possible color-detection circuit.\n",
    "\n",
    "Part 2 is ... JONAS!!!\n",
    "\n",
    "Part 3 is a walk-through of how to implement important interpretability methods for decoder models by using the libraries TransformerLens and CIrcuitsVis. This part includes finding induction heads in small scale models, activation patching for GPT-style models and visualizing prediction developments with logit lens. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

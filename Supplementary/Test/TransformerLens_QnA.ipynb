{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144ce5e8",
   "metadata": {},
   "source": [
    "# Q&A example with TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdbc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618b204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27be1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attn_patterns(model, text, layers, compact=True):\n",
    "    str_tokens = model.to_str_tokens(text)\n",
    "    logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "    if compact:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "    \n",
    "    else:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_heads(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c254",
   "metadata": {},
   "source": [
    "We set our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdb20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1449c1",
   "metadata": {},
   "source": [
    "We import and embed the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c2de2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mwith open(\"Test.json\",\"r\") as infile: \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    infile.readline()\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    lines = infile.readlines()\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[32m     10\u001b[39m     data = json.load(infile)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#data = [json.loads(line.strip(\"\\n\").rstrip(',').rstrip(\"]\")) for line in lines]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, *args, **kwargs)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Test.json'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "'''\n",
    "with open(\"Test.json\",\"r\") as infile: \n",
    "    infile.readline()\n",
    "    lines = infile.readlines()\n",
    "    '''\n",
    "\n",
    "with open(\"Test.json\", \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "#data = [json.loads(line.strip(\"\\n\").rstrip(',').rstrip(\"]\")) for line in lines]\n",
    "\n",
    "questions = [line[\"question\"] for line in data]\n",
    "answers = [line[\"rationale\"] for line in data]\n",
    "\n",
    "#Embedding the questions and answers\n",
    "question_embeds = model.to_tokens(questions)\n",
    "answer_embeds = model.to_tokens(answers)\n",
    "\n",
    "str_questions = model.to_str_tokens(questions)\n",
    "str_answers = model.to_str_tokens(answers)\n",
    "\n",
    "print(len(question_embeds), len(answer_embeds))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829baee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vis_attn_patterns(model, questions[\u001b[32m0\u001b[39m], layers=[\u001b[32m0\u001b[39m])\n\u001b[32m      2\u001b[39m vis_attn_patterns(model, questions[\u001b[32m1\u001b[39m], layers=[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "vis_attn_patterns(model, questions[0], layers=[0])\n",
    "vis_attn_patterns(model, questions[1], layers=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc0a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<|endoftext|>', 'The', ' capital', ' of', ' Norway', ' is'], ['<|endoftext|>', ' Oslo']]\n",
      "[['<|endoftext|>', 'The', ' capital', ' of', ' Sweden', ' is'], ['<|endoftext|>', ' Stockholm']]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "clean = \"The capital of Norway is\"\n",
    "answer = \" Oslo\"\n",
    "corrupted = \"The capital of Sweden is\"\n",
    "answer_2 = \" Stockholm\"\n",
    "\n",
    "print(model.to_str_tokens([clean, answer]))\n",
    "print(model.to_str_tokens([corrupted, answer_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9d17c",
   "metadata": {},
   "source": [
    "# Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc528f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean answer logit difference: 2.1247\n",
      "Corrupted answer logit difference: -5.3342\n"
     ]
    }
   ],
   "source": [
    "clean_prompt = \"What is the capital of France?\"\n",
    "corrupted_prompt = \"What is the capital of England?\"\n",
    "\n",
    "clean_answer = \"Paris\"\n",
    "corrupted_answer = \"London\"\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "corrupted_logits = model(corrupted_prompt)\n",
    "\n",
    "clean_index = model.to_single_token(clean_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "print(f\"Clean answer logit difference: {clean_diff:.4f}\")\n",
    "\n",
    "corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "print(f\"Corrupted answer logit difference: {corrupted_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383d40d",
   "metadata": {},
   "source": [
    "Then we want to patch the clean prompt onto the corrupted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54862200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(resid_pre, hook, position, clean_cache):\n",
    "    clean_activation = clean_cache[hook.name]\n",
    "    resid_pre[:, position, :] = clean_activation[:, position, :]\n",
    "    return resid_pre\n",
    "\n",
    "def model_data(model, prompt):\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    return tokens, logits, cache\n",
    "\n",
    "def activation_patching(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer):\n",
    "    '''\n",
    "    Performs activation patching of the clean prompt onto the corrupted prompt. The prompts must have the same number of tokens.\n",
    "\n",
    "    Returns: \n",
    "    patching_results (list[tensor[layers, positions]]): The logit difference after patching\n",
    "    patched_logits (list[tensor[num_tokens, logits]]): The logits of the tokens after patching\n",
    "    '''\n",
    "    \n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "    corrupted_logits = model(corrupted_prompt)\n",
    "    print(\"Clean answer:\",clean_answer)\n",
    "    print(\"Corrupted answer:\", corrupted_answer)\n",
    "    clean_index = model.to_single_token(clean_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "    clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "    corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "\n",
    "    clean_tokens = model.to_tokens(clean_prompt)\n",
    "    corrupted_tokens= model.to_tokens(corrupted_prompt)\n",
    "    num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "\n",
    "    assert len(clean_tokens[0]) == len(corrupted_tokens[0]), \"The prompts must have the same number of tokens.\"\n",
    "\n",
    "   \n",
    "    patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "    for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(num_positions):\n",
    "            # We use a temporary hook with functool.partial to patch at each position\n",
    "            temp_hook = partial(activation_patching_hook, position=position, clean_cache=clean_cache)\n",
    "            # We then run the model with hooks as usual\n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, \n",
    "                                                  fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), temp_hook)])\n",
    "            \n",
    "            # We then calculate the logit difference\n",
    "            patched_diff = (patched_logits[0, -1, clean_index] - patched_logits[0, -1, corrupted_index]).detach()\n",
    "            # We then store the result in the patching_result tensor, normalizing it\n",
    "            if abs(clean_diff-corrupted_diff) < 1e-16:\n",
    "                patching_result[layer, position] = 0\n",
    "            else:\n",
    "                patching_result[layer, position] = abs((patched_diff - corrupted_diff) / (clean_diff - corrupted_diff))\n",
    "    print(patched_logits.shape)\n",
    "    return patching_result, patched_logits\n",
    "\n",
    "def activation_patching_mult(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer):\n",
    "    ''' \n",
    "    Performs activation patching on prompts with multi-word answers by using separate run-throughs.\n",
    "    The answers must have the same number of tokens\n",
    "    '''\n",
    "    patching_result = []\n",
    "    patched_logits = []\n",
    "    clean_answers_tokens = model.to_str_tokens(clean_answer)[1:]\n",
    "    corrupted_answers_tokens = model.to_str_tokens(corrupted_answer)[1:]\n",
    "\n",
    "    if len(clean_answers_tokens) == 1:\n",
    "        return activation_patching(model, clean_prompt, corrupted_prompt, clean_answers_tokens, corrupted_answers_tokens)\n",
    "    print(model.to_str_tokens(clean_prompt))\n",
    "    print(model.to_str_tokens(corrupted_prompt))\n",
    "    print(\"Number of run throughs:\", len(clean_answers_tokens))\n",
    "    for i in range(len(clean_answers_tokens)):\n",
    "        p_result, p_logits = activation_patching(model, clean_prompt, corrupted_prompt, clean_answers_tokens[0], corrupted_answers_tokens[0])\n",
    "        patching_result.append(p_result)\n",
    "        patched_logits.append(p_logits)\n",
    "        clean_prompt += clean_answers_tokens[0]\n",
    "        clean_answers_tokens = clean_answers_tokens[1:]\n",
    "        corrupted_prompt += corrupted_answers_tokens[0]\n",
    "        corrupted_answers_tokens = corrupted_answers_tokens[1:]\n",
    "\n",
    "    return patching_result, patched_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bd1c97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(clean_cache.keys()))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLooking for; \u001b[39m\u001b[33m\"\u001b[39m, utils.get_act_name(\u001b[33m\"\u001b[39m\u001b[33mactivation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m      3\u001b[39m patching_result = activation_patching(model, clean_prompt, model.to_tokens(corrupted_prompt))\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_cache' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(clean_cache.keys()))\n",
    "print(\"Looking for; \", utils.get_act_name(\"activation\", 0))\n",
    "patching_result = activation_patching(model, clean_prompt, model.to_tokens(corrupted_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75333f",
   "metadata": {},
   "source": [
    "When patching the clean results onto the corrupted results, the change is first very localized, but is then brought to the end in the last few layers. \n",
    "We now want to try with a more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89cd6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_physics_prompt = \"The action of adding numbers is called\"\n",
    "corrupted_physics_prompt = \"The action of multiplying numbers is called\"\n",
    "clean_answer = \" addition, and the result is their sum\"\n",
    "corrupted_answer = \" multiplication, and the result is their product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87dc8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b073b8da40604e989d46892263425921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'The', ' action', ' of', ' adding', ' numbers', ' is', ' called']\n",
      "Tokenized answer: [' addition', ',', ' and', ' the', ' result', ' is', ' their', ' sum']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104</span><span style=\"font-weight: bold\">      Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.62</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07</span><span style=\"font-weight: bold\">% Token: | addition|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m104\u001b[0m\u001b[1m      Logit:  \u001b[0m\u001b[1;36m7.62\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.07\u001b[0m\u001b[1m% Token: | addition|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 12.96 Prob: 14.15% Token: | \"|\n",
      "Top 1th token. Logit: 12.67 Prob: 10.58% Token: | the|\n",
      "Top 2th token. Logit: 12.00 Prob:  5.38% Token: | a|\n",
      "Top 3th token. Logit: 10.96 Prob:  1.91% Token: | '|\n",
      "Top 4th token. Logit: 10.87 Prob:  1.74% Token: | an|\n",
      "Top 5th token. Logit: 10.21 Prob:  0.90% Token: | for|\n",
      "Top 6th token. Logit:  9.94 Prob:  0.69% Token: | adding|\n",
      "Top 7th token. Logit:  9.89 Prob:  0.65% Token: | as|\n",
      "Top 8th token. Logit:  9.73 Prob:  0.56% Token: | in|\n",
      "Top 9th token. Logit:  9.69 Prob:  0.54% Token: | by|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.66</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.83</span><span style=\"font-weight: bold\">% Token: |,|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.66\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m19.83\u001b[0m\u001b[1m% Token: |,|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.07 Prob: 30.04% Token: |.|\n",
      "Top 1th token. Logit: 15.66 Prob: 19.83% Token: |,|\n",
      "Top 2th token. Logit: 15.09 Prob: 11.24% Token: | and|\n",
      "Top 3th token. Logit: 14.39 Prob:  5.60% Token: | or|\n",
      "Top 4th token. Logit: 14.15 Prob:  4.39% Token: | to|\n",
      "Top 5th token. Logit: 13.51 Prob:  2.32% Token: | (|\n",
      "Top 6th token. Logit: 13.45 Prob:  2.19% Token: | .|\n",
      "Top 7th token. Logit: 13.28 Prob:  1.83% Token: | of|\n",
      "Top 8th token. Logit: 12.91 Prob:  1.26% Token: | ,|\n",
      "Top 9th token. Logit: 12.78 Prob:  1.11% Token: |/|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.73</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.26</span><span style=\"font-weight: bold\">% Token: | and|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.73\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m50.26\u001b[0m\u001b[1m% Token: | and|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.73 Prob: 50.26% Token: | and|\n",
      "Top 1th token. Logit: 13.25 Prob:  4.21% Token: | which|\n",
      "Top 2th token. Logit: 12.94 Prob:  3.08% Token: | but|\n",
      "Top 3th token. Logit: 12.70 Prob:  2.44% Token: | as|\n",
      "Top 4th token. Logit: 12.65 Prob:  2.32% Token: | so|\n",
      "Top 5th token. Logit: 12.64 Prob:  2.30% Token: | or|\n",
      "Top 6th token. Logit: 12.64 Prob:  2.29% Token: | because|\n",
      "Top 7th token. Logit: 12.33 Prob:  1.68% Token: | not|\n",
      "Top 8th token. Logit: 12.24 Prob:  1.54% Token: | the|\n",
      "Top 9th token. Logit: 11.77 Prob:  0.96% Token: | a|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.28</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.13</span><span style=\"font-weight: bold\">% Token: | the|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.28\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m7.13\u001b[0m\u001b[1m% Token: | the|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.61 Prob: 26.87% Token: | it|\n",
      "Top 1th token. Logit: 14.54 Prob:  9.26% Token: | is|\n",
      "Top 2th token. Logit: 14.28 Prob:  7.13% Token: | the|\n",
      "Top 3th token. Logit: 13.37 Prob:  2.86% Token: | in|\n",
      "Top 4th token. Logit: 13.15 Prob:  2.30% Token: | when|\n",
      "Top 5th token. Logit: 13.06 Prob:  2.11% Token: | adding|\n",
      "Top 6th token. Logit: 13.02 Prob:  2.01% Token: | this|\n",
      "Top 7th token. Logit: 12.67 Prob:  1.43% Token: | there|\n",
      "Top 8th token. Logit: 12.65 Prob:  1.39% Token: | we|\n",
      "Top 9th token. Logit: 12.61 Prob:  1.34% Token: | as|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.69</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.79</span><span style=\"font-weight: bold\">% Token: | result|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m11.69\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.79\u001b[0m\u001b[1m% Token: | result|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 12.76 Prob:  5.24% Token: | number|\n",
      "Top 1th token. Logit: 12.15 Prob:  2.84% Token: | process|\n",
      "Top 2th token. Logit: 11.85 Prob:  2.12% Token: | action|\n",
      "Top 3th token. Logit: 11.70 Prob:  1.81% Token: | more|\n",
      "Top 4th token. Logit: 11.69 Prob:  1.79% Token: | result|\n",
      "Top 5th token. Logit: 11.62 Prob:  1.68% Token: | numbers|\n",
      "Top 6th token. Logit: 11.61 Prob:  1.65% Token: | first|\n",
      "Top 7th token. Logit: 11.32 Prob:  1.24% Token: | idea|\n",
      "Top 8th token. Logit: 11.28 Prob:  1.19% Token: | effect|\n",
      "Top 9th token. Logit: 11.27 Prob:  1.18% Token: | amount|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.75</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67.82</span><span style=\"font-weight: bold\">% Token: | is|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.75\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m67.82\u001b[0m\u001b[1m% Token: | is|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.75 Prob: 67.82% Token: | is|\n",
      "Top 1th token. Logit: 16.08 Prob: 12.78% Token: | of|\n",
      "Top 2th token. Logit: 14.52 Prob:  2.69% Token: | can|\n",
      "Top 3th token. Logit: 13.58 Prob:  1.05% Token: | will|\n",
      "Top 4th token. Logit: 13.40 Prob:  0.88% Token: | depends|\n",
      "Top 5th token. Logit: 13.23 Prob:  0.74% Token: | should|\n",
      "Top 6th token. Logit: 13.15 Prob:  0.69% Token: | has|\n",
      "Top 7th token. Logit: 12.89 Prob:  0.53% Token: |,|\n",
      "Top 8th token. Logit: 12.82 Prob:  0.49% Token: | in|\n",
      "Top 9th token. Logit: 12.73 Prob:  0.45% Token: | may|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">189</span><span style=\"font-weight: bold\">      Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.79</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04</span><span style=\"font-weight: bold\">% Token: | their|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m189\u001b[0m\u001b[1m      Logit:  \u001b[0m\u001b[1;36m8.79\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1m% Token: | their|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.24 Prob: 22.41% Token: | that|\n",
      "Top 1th token. Logit: 14.77 Prob: 14.08% Token: | a|\n",
      "Top 2th token. Logit: 14.66 Prob: 12.51% Token: | the|\n",
      "Top 3th token. Logit: 13.40 Prob:  3.56% Token: | an|\n",
      "Top 4th token. Logit: 12.75 Prob:  1.87% Token: | what|\n",
      "Top 5th token. Logit: 12.39 Prob:  1.31% Token: | to|\n",
      "Top 6th token. Logit: 12.37 Prob:  1.27% Token: | usually|\n",
      "Top 7th token. Logit: 12.23 Prob:  1.11% Token: | often|\n",
      "Top 8th token. Logit: 12.19 Prob:  1.07% Token: | something|\n",
      "Top 9th token. Logit: 12.08 Prob:  0.95% Token: | not|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.80</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.82</span><span style=\"font-weight: bold\">% Token: | sum|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m10.80\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.82\u001b[0m\u001b[1m% Token: | sum|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 11.77 Prob:  2.14% Token: | own|\n",
      "Top 1th token. Logit: 11.67 Prob:  1.94% Token: | number|\n",
      "Top 2th token. Logit: 11.67 Prob:  1.94% Token: | effect|\n",
      "Top 3th token. Logit: 11.54 Prob:  1.70% Token: | value|\n",
      "Top 4th token. Logit: 11.01 Prob:  1.00% Token: | numbers|\n",
      "Top 5th token. Logit: 10.98 Prob:  0.98% Token: | addition|\n",
      "Top 6th token. Logit: 10.98 Prob:  0.98% Token: | total|\n",
      "Top 7th token. Logit: 10.87 Prob:  0.87% Token: | ratio|\n",
      "Top 8th token. Logit: 10.85 Prob:  0.86% Token: | combined|\n",
      "Top 9th token. Logit: 10.80 Prob:  0.82% Token: | sum|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' addition'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' and'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' the'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' result'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' is'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">189</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' sum'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' addition'\u001b[0m, \u001b[1;36m104\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m','\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m' and'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m' the'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m' result'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m' is'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[32mtheir'\u001b[0m, \u001b[1;36m189\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m' sum'\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate(clean_physics_prompt, max_new_tokens=10, temperature=0.0, top_p=1.0, do_sample=False)\n",
    "utils.test_prompt(clean_physics_prompt, clean_answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28764a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_patching_result(model, patching_results, corrupted_prompt, corrupted_answer):\n",
    "    '''\n",
    "    Visualizes the logit differences caused by activation patching in a heat map. If the answer has more than one token, \"patching_results\" must be a list of results.\n",
    "    '''\n",
    "    if isinstance(patching_results, list):\n",
    "        print(f\"The list has {len(patching_results)} elements.\")\n",
    "        len_ans = len(patching_results)\n",
    "        for i in range(len_ans):\n",
    "            tokens = model.to_str_tokens(corrupted_prompt+corrupted_answer)\n",
    "            labels = [f'{token}_{index}' for index, token in enumerate(tokens)][:-len(patching_results)+i+1]\n",
    "            print(labels)\n",
    "            if not torch.all(patching_results[i] == 0):\n",
    "                print(patching_results[i].shape)\n",
    "                px.imshow(patching_results[i].detach(), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", x=labels, labels={\"x\": \"Position\", \"y\": \"Layer\"}, title=\"Patching Results\").show()\n",
    "    else:\n",
    "        print(\"There is only one patching result.\")\n",
    "        tokens = model.to_str_tokens(corrupted_prompt)\n",
    "        labels = [f'{token}_{index}' for index, token in enumerate(tokens)]\n",
    "        print(labels)\n",
    "        px.imshow(patching_results.detach(), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", x=labels, labels={\"x\": \"Position\", \"y\": \"Layer\"}, title=\"Patching Results\").show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "027c66ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean answer: [' Oslo']\n",
      "Corrupted answer: [' Stockholm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b364ac466b24483a68291109b7bb760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "patching_result_physics = activation_patching_mult(model, clean,corrupted, answer, answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0bfe9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is only one patching result.\n",
      "['<|endoftext|>_0', 'The_1', ' capital_2', ' of_3', ' Sweden_4', ' is_5']\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "<|endoftext|>_0",
          "The_1",
          " capital_2",
          " of_3",
          " Sweden_4",
          " is_5"
         ],
         "xaxis": "x",
         "yaxis": "y",
         "z": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAgD8AAAAAAAAAAAAAAAAAAAAAAAAAAPQlgD/0VMo5AAAAAAAAAAAAAAAAAAAAAFcBgD9jCuo5AAAAAAAAAAAAAAAAAAAAAH6Nfz8JFyI7AAAAAAAAAAAAAAAAAAAAANesfj8ppzY7AAAAAAAAAAAAAAAAAAAAALYCgj8pFgM6AAAAAAAAAAAAAAAAAAAAAMNphj/W2pM6AAAAAAAAAAAAAAAAAAAAAOA0hj/Q9hY7AAAAAAAAAAAAAAAAAAAAAJTVhD92iRY7AAAAAAAAAAAAAAAAAAAAALvIgj+OFOQ6AAAAAAAAAAAAAAAAAAAAAB1Sgj9JyH46AAAAAAAAAAAAAAAAAAAAAOUCgj82KCs8AAAAAAAAAAAAAAAAAAAAALk8gj85QHI8AAAAAAAAAAAAAAAAAAAAANp8gz+3Plk8AAAAAAAAAAAAAAAAAAAAAKFvgj8NAhw8AAAAAAAAAAAAAAAAAAAAAD6mgj/gF5o5AAAAAAAAAAAAAAAAAAAAAPsdcD/24JY9AAAAAAAAAAAAAAAAAAAAAG5Maz//ibY9AAAAAAAAAAAAAAAAAAAAAN7gXD9dmgY+AAAAAAAAAAAAAAAAAAAAAL2qVz8PAx0+AAAAAAAAAAAAAAAAAAAAABj3HT8JntM+AAAAAAAAAAAAAAAAAAAAADGetD5iQjE/AAAAAAAAAAAAAAAAAAAAAFOqJD0ty3Q/AAAAAAAAAAAAAAAAAAAAAKMBJjzw0IE/",
          "dtype": "f4",
          "shape": "24, 6"
         }
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Patching Results"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Position"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_patching_result(model, patching_result_physics[0], corrupted, answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b97611",
   "metadata": {},
   "source": [
    "It seems that even after multiple separate runs, the old patchings still influence the results slightly.\n",
    "\n",
    "Question: If we always took the absolute value of the difference (ie no red), woul this lead to the same interpretation or is there a difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6795e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              \": 0.1507,             the: 0.1075, \n",
      "              .: 0.3018,               ,: 0.2002, \n",
      "            and: 0.5176,           which: 0.0428, \n",
      "             it: 0.2846,              is: 0.1009, \n",
      "         number: 0.0612,         process: 0.0323, \n",
      "             is: 0.6871,              of: 0.1305, \n",
      "           that: 0.2379,               a: 0.1375, \n",
      "            own: 0.0264,  multiplication: 0.0235, \n"
     ]
    }
   ],
   "source": [
    "# What about the probability distribution now?\n",
    "def get_prediction(model, logits, num_top=5):\n",
    "    if isinstance(logits, list):\n",
    "        for i in range(len(logits)):\n",
    "            logits_i = logits[i][0, -1, :]\n",
    "            probs = logits_i.softmax(dim=-1)\n",
    "            top_probs, top_indices = probs.topk(num_top)\n",
    "            top_tokens = [model.to_string(index.item()) for index in top_indices]\n",
    "\n",
    "            for token, prob in zip(top_tokens, top_probs):\n",
    "                print(f'{token:>15}: {prob.item():.4f}, ', end='')\n",
    "            print()\n",
    "    \n",
    "    else:\n",
    "        logits_i = logits[0, -1, :]\n",
    "        probs = logits_i.softmax(dim=-1)\n",
    "        top_probs, top_indices = probs.topk(num_top)\n",
    "        top_tokens = [model.to_string(index.item()) for index in top_indices]\n",
    "\n",
    "        for token, prob in zip(top_tokens, top_probs):\n",
    "            print(f'{token}: {prob.item():.4f}, ', end='')\n",
    "\n",
    "get_prediction(model, patching_result_physics[1], num_top=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a46879",
   "metadata": {},
   "source": [
    "Analysis of results from activation patching:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b733a",
   "metadata": {},
   "source": [
    "# Attribution patching\n",
    "\n",
    "A technique for patching multip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4838706e",
   "metadata": {},
   "source": [
    "# Induction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dca7933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run with hooks (this is where we write to the `induction_score_store` tensor`)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model.run_with_hooks(question_token, \n\u001b[32m     35\u001b[39m     return_type=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# For efficiency, we don't need to calculate the logits\u001b[39;00m\n\u001b[32m     36\u001b[39m     fwd_hooks=[(\n\u001b[32m     37\u001b[39m         pattern_hook_names_filter,\n\u001b[32m     38\u001b[39m         induction_score_hook\n\u001b[32m     39\u001b[39m     )]\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#print(induction_score_store)\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get the global max value and its flat index\u001b[39;00m\n\u001b[32m     43\u001b[39m max_value = np.max(induction_score_store)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hooked_model.forward(*model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = block(\n\u001b[32m    613\u001b[39m         residual,\n\u001b[32m    614\u001b[39m         \u001b[38;5;66;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;00m\n\u001b[32m    615\u001b[39m         \u001b[38;5;66;03m# block\u001b[39;00m\n\u001b[32m    616\u001b[39m         past_kv_cache_entry=past_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    617\u001b[39m         shortformer_pos_embed=shortformer_pos_embed,\n\u001b[32m    618\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    619\u001b[39m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    161\u001b[39m         query_input=\u001b[38;5;28mself\u001b[39m.ln1(query_input)\n\u001b[32m    162\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    163\u001b[39m         key_input=\u001b[38;5;28mself\u001b[39m.ln1(key_input)\n\u001b[32m    164\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    165\u001b[39m         value_input=\u001b[38;5;28mself\u001b[39m.ln1(value_input),\n\u001b[32m    166\u001b[39m         past_kv_cache_entry=past_kv_cache_entry,\n\u001b[32m    167\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\abstract_attention.py:261\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    259\u001b[39m pattern = F.softmax(attn_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    260\u001b[39m pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m pattern = \u001b[38;5;28mself\u001b[39m.hook_pattern(pattern)  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n\u001b[32m    263\u001b[39m pattern = pattern.to(v.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1818\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1816\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1821\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:109\u001b[39m, in \u001b[36mHookPoint.add_hook.<locals>.full_hook\u001b[39m\u001b[34m(module, module_input, module_output)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mdir\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mbwd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[32m    108\u001b[39m     module_output = module_output[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hook(module_output, hook=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36minduction_score_hook\u001b[39m\u001b[34m(activation_pattern, hook)\u001b[39m\n\u001b[32m     26\u001b[39m induction_score = einops.reduce(induction_stripe, \u001b[33m\"\u001b[39m\u001b[33mbatch head_index position -> head_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Store the result.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m induction_score_store[hook.layer(), :] = induction_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\_tensor.py:1227\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "induction = []\n",
    "dict_questions = {}\n",
    "for question, question_token in zip(questions, question_embeds):\n",
    "    length = len(question_token)\n",
    "    \n",
    "    induction_score_store = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    # A function for the average induction score\n",
    "    def induction_score_hook(activation_pattern, hook):\n",
    "        \"\"\"\n",
    "        Computes the average induction score for a given activation pattern.\n",
    "        \n",
    "        Args:\n",
    "            activation_pattern (torch.Tensor): The activation pattern to compute the induction score for.\n",
    "            hook (HookPoint): The hook point that triggered this function.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The average induction score.\n",
    "        \"\"\"\n",
    "        # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
    "        # (This only has entries for tokens with index>=seq_len)\n",
    "        induction_stripe = activation_pattern.diagonal(dim1=-2, dim2=-1, offset=1-length//2)\n",
    "        # Get an average score per head\n",
    "        induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
    "        # Store the result.\n",
    "        induction_score_store[hook.layer(), :] = induction_score\n",
    "\n",
    "    # We make a boolean filter on activation names, that's true only on attention pattern names.\n",
    "    pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "    print(question)\n",
    "    # Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "    model.run_with_hooks(question_token, \n",
    "        return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[(\n",
    "            pattern_hook_names_filter,\n",
    "            induction_score_hook\n",
    "        )]\n",
    "    )\n",
    "    #print(induction_score_store)\n",
    "    # Get the global max value and its flat index\n",
    "    max_value = np.max(induction_score_store)\n",
    "    max_index = np.argmax(induction_score_store)\n",
    "\n",
    "    # Convert flat index to (layer, head) coordinates\n",
    "    #max_index = np.unravel_index(max_index_flat.cpu().numpy(), induction_score_store.shape)\n",
    "    #print(\"Question:\", question)\n",
    "    #print(\"Max value:\", max_value.item())\n",
    "    #print(\"Max index (layer, head):\", max_index)\n",
    "    '''\n",
    "    if (max_index[0], max_index[1]) not in dict_questions:\n",
    "        dict_questions[(max_index[0], max_index[1])] = [question]\n",
    "    else:\n",
    "        dict_questions[(max_index[0], max_index[1])].append(question)\n",
    "    '''\n",
    "    print(max_index[0])\n",
    "    x.append(max_index[0])\n",
    "    y.append(max_index[1])\n",
    "    print(x)\n",
    "    induction.append(induction_score_store)\n",
    " \n",
    "print(x, y)\n",
    "df = pd.DataFrame({\n",
    "    \"Layer\": x,\n",
    "    \"Head\": y,\n",
    "    \"Question\": questions[:len(x)]\n",
    "})\n",
    "#print(dict_questions[(int(x[0]), int(y[0]))])\n",
    "px.scatter(\n",
    "    data_frame=df,\n",
    "    x=\"Layer\",\n",
    "    y=\"Head\",\n",
    "    hover_data=[\"Question\"],\n",
    "    title=\"Induction Scores for Questions\"\n",
    ").show()\n",
    "#px.scatter(x=np.array(x), y=np.array(y), labels={\"x\": \"Layer\", \"y\": \"Head\"},  title=\"Induction Scores for Questions\", hover_data=[\"question\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019595b5",
   "metadata": {},
   "source": [
    "The problem here is that many of the questions have the same max index. Therefore, it might not be the best method to visualize the induction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197ac6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m      3\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=\u001b[32m30\u001b[39m, random_state=\u001b[32m42\u001b[39m, learning_rate=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, early_exaggeration=\u001b[32m5.0\u001b[39m, n_iter=\u001b[32m5000\u001b[39m, init=\u001b[33m\"\u001b[39m\u001b[33mpca\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m induction_embeds = tsne.fit_transform(np.array(induction))\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate=\"auto\", early_exaggeration=5.0, n_iter=5000, init=\"pca\")\n",
    "induction_embeds = tsne.fit_transform(np.array(induction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e04b4380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256]])\n"
     ]
    }
   ],
   "source": [
    "print(model.to_tokens(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04bc06",
   "metadata": {},
   "source": [
    "# Direct Logit Attribution\n",
    "\n",
    "A technique to see what components contrinute the most to an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bc3ba",
   "metadata": {},
   "source": [
    "# Logit Lens\n",
    "\n",
    "A method for seeing how the predictions change throughout a network. We \"pretend\" that a certain layer is the last layer and then see what the prediction would have been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c53a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

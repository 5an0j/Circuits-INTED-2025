{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5ad763",
   "metadata": {},
   "source": [
    "**This is not finished and may contain misconceptions/mistakes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d2aa9",
   "metadata": {},
   "source": [
    "# Circuits in Mechanistic Interpretability\n",
    "\n",
    "## The Fundamentals of Mechanistic Interpretability\n",
    "\n",
    "Mechanistic interpretability is a 'natural science'-inspired approach to Neural Network interpretability, based on zooming in on parts of the network which are sufficiently small as to permit us to make concrete, falsafiable predicitons. The current paradigm is loosely based on three postulates from Olah, et al., 2020 (paraphrased):\n",
    "\n",
    "1. **Linearly representable features:** Features are the fundamental unit of neural networks. They correspond to directions in activation space.\n",
    "\n",
    "2. **Circuits:** Features are connected by weights, forming circuits.\n",
    "\n",
    "3. **Universality:** Analogous features and circuits form across models and tasks.\n",
    "\n",
    "Let's briefly unpack these postulates (based on my very superficial current understanding of developments since 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b0d00",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "A feature is a fuzzy term, with mainly two related uses:\n",
    "\n",
    "1. A property of the input: An interpretable concept in a given input to a model, such as (a curve in an image) or (the language of a text) or (more or less anything that a human could say about the input).\n",
    "\n",
    "2. A fundamental unit of the model: A localized, interpretable concept that the model has learned. In this view, a feature is viewed as an *idealized node* of the network which we expect to light up when the corresponding concept is present in the input.\n",
    "\n",
    "Since we study models *in general*, we will mostly use the second definintion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485d99d",
   "metadata": {},
   "source": [
    "\n",
    "**Linear feature representation**\n",
    "\n",
    "The idea that features correspond to *directions in the activation space of a given layer* is a postulate on *how the model represents meaning* based on the fact that it consists of mostly linear operations. Note that while there are (infinitley) many more directions than dimensions in activation space, we expect the model to minimize interference between features (hence keep them mostly orthogonal, unless the features never co-ocur in the training data). The directions corresponding to features hence form an over-complete basis.\n",
    "\n",
    "Ideally, we would then want to (non-linearly) imbed the activation space of a given layer in a much larger space in such a way that the features represented in that layer point in the basis directions of the large space. This is an idealised form of *disentangeling polysemanticity* and is the motivation behind sparse autoencoders which have empirically been shown to work. I suppose this is strong evidence for the first postulate being directionally correct.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary> Idealised nodes </summary>\n",
    "\n",
    "  In this (idealized) view, the preimage of the basis vectors in the interpretable space represent the features in a given layer - concretely, this gives us *a linear combination of activations in a single layer fixed up to scaling* for each feature. We can then remodel the network graph in terms of the features, making the notion of a feature as an *idealized node* more concrete.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "**In short**, to make the network graph interpretable, we want to *disentagle* the graph by greatly increasing the number of nodes in each layer so that each node corresponds to a separate interpretable feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42462713",
   "metadata": {},
   "source": [
    "#### Circuits\n",
    "\n",
    "This is also a fuzzy term, getting at the interplay between features to implement algorithms in the model. More precisely: If we have identified certain features in the network and disentangled the network graph in terms of these, a circuit is a directed subgraph, or a set of *tightly connected nodes (across layers) implementing an interpretable algorithm*.\n",
    "\n",
    "For example, the features 'wheel', 'car body', 'car window' can form a car-detection circuit. The final node of this circuit is then a feature in its corresponding layer, which activates if cars are present in the input. The 'wheel' feature is itself the final node of a 'wheel detection circuit', which could be the union of a 'right facing wheel'-feature and a 'left facing wheel'-feature. These again are final nodes of a cricuit containing circle-detection, which is made from curve detection, which is a union of directed curve detectors, and so on. This way, complex structure emerges from simple circuits.\n",
    "\n",
    "Scaling (hypothesis)\n",
    "\n",
    "Motifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e593e6",
   "metadata": {},
   "source": [
    "**Universality**\n",
    "\n",
    "Universality is the idea that we can understand something about models in general by studing particular models. The usefulness of Mechanistic interpretability is proportional to the degree to which this holds (Ex: imagine if all animals had *completely unrelated* biology/cell structure. We would then study human cells and domestic animals, mostly. Similarily, mech interp wants to find general circuits which we expect all models to converge on ('optimal, reachable algorithms' for a given problem exist); if not true, we will have to study models in isolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d5878",
   "metadata": {},
   "source": [
    "## Finding circuits\n",
    "\n",
    "Attribution graphs: graph to find tighly connected nodes?\n",
    "\n",
    "Interventions: let us find the causally relevant parts of the attribution graph to reduce to a circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cb632",
   "metadata": {},
   "source": [
    "### Interventions on Jina AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefe6f4",
   "metadata": {},
   "source": [
    "**Non-causal, Encoder Style Transformers**\n",
    "\n",
    "Standard next-token predictors are causal/autoregressive, meaning that the attention patterns are masked. Embedding models are non-causal, and trained by mapping semantically similar inputs close together in the embedding space. Otherwise, the architecture is relatively similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f8f5c9",
   "metadata": {},
   "source": [
    "Given a multiple choice question, we seek to determine which words are most important for the BERT-model's answer selection.\n",
    "\n",
    "Model: RoBERTa adapted for Multiple Choice (non-causal attention). If you want to specialize your model, you can fine-tune it (eg. specifically for physics questions) as described [here](https://huggingface.co/docs/transformers/en/tasks/multiple_choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3c2e2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9b3b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMultipleChoice, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c18a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.logits\n",
    "\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    logits = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)[position]\n",
    "    print(logits.max(1).values.shape)\n",
    "    return logits.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "047bb442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Two metal balls are the same size but one weighs twice as much as the other. The balls are dropped from the roof of a single story building at the same instant of time. The time it takes the balls to reach the ground below will be:\n",
      "Predicted Answer: About half as long for the lighter ball as for the heavier one\n",
      "Ground Truth: About the same for both balls\n",
      "Logits: tensor([[0.4318, 0.4362, 0.4330, 0.4017, 0.4140]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "question = [\"Two metal balls are the same size but one weighs twice as much as the other. The balls \\\n",
    "are dropped from the roof of a single story building at the same instant of time. The time it takes \\\n",
    "the balls to reach the ground below will be:\"]\n",
    "\n",
    "choices = [\"About half as long for the heavier ball as for the lighter one\",\n",
    "           \"About half as long for the lighter ball as for the heavier one\",\n",
    "           \"About the same for both balls\",\n",
    "           \"Considerably less for the heavier ball, but not necessarily half as long\",\n",
    "           \"Considerably less for the lighter ball, but not necessarily half as long\"]\n",
    "\n",
    "ground_truth = \"About the same for both balls\"\n",
    "\n",
    "# Tokenizing: [CLS] question [SEP] [SEP] choice [SEP] for each choice\n",
    "encoding = tokenizer(\n",
    "    question * len(choices),  # Repeat context for each choice\n",
    "    choices,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ") # returns dictionary with input_ids and corresp. token_type_ids and attention_mask (ensuring that padding tokens are not attended to)\n",
    "\n",
    "input_ids = encoding[\"input_ids\"].unsqueeze(0)  # shape: (1, num_choices, seq_length)\n",
    "\n",
    "token_type_ids = encoding[\"token_type_ids\"].unsqueeze(0)  # shape: (1, num_choices, seq_length)\n",
    "attention_mask = encoding[\"attention_mask\"].unsqueeze(0) # attention mask to ignore padding tokens\n",
    "\n",
    "logits = predict(input_ids, attention_mask=attention_mask)\n",
    "choice_idx = torch.argmax(logits, dim=1).item()\n",
    "choice = choices[choice_idx]\n",
    "\n",
    "print('Question:', question[0])\n",
    "print('Predicted Answer:', choice)\n",
    "print('Ground Truth:', ground_truth)\n",
    "\n",
    "print('Logits:', logits) # Too gauge conviction strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ebb9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "def print_tokens(input_ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "cls_token_id = tokenizer.cls_token_id # Start\n",
    "ref_token_id = tokenizer.pad_token_id # Padding\n",
    "sep_token_id = tokenizer.sep_token_id # Separator\n",
    "# print(cls_token_id, ref_token_id, sep_token_id)\n",
    "\n",
    "# The exact setup depends slightly on tokenizer\n",
    "# Reference tokens: [CLS] [PAD] ... [PAD] [SEP] ([SEP]) [PAD] ... [PAD] [SEP] [PAD] ..., for each choice\n",
    "# Type tokens: [0] for question, [1] for choice, for each choice\n",
    "ref_tokens_all = []\n",
    "\n",
    "for i in range(len(choices)):\n",
    "    ref_tokens = [cls_token_id]\n",
    "\n",
    "    for token in input_ids[0,i,1:]:\n",
    "\n",
    "        if token == sep_token_id:\n",
    "            ref_tokens += [token]\n",
    "        else:\n",
    "            ref_tokens += [ref_token_id]\n",
    "\n",
    "    ref_tokens_all.append(ref_tokens)\n",
    "        \n",
    "ref_input_ids = torch.tensor(ref_tokens_all, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "# print_tokens(input_ids[0,0,:])\n",
    "# print(input_ids[:,0,:], ref_input_ids[:,0,:], token_type_ids[:,0,:],)\n",
    "# print(ref_input_ids.shape, token_type_ids.shape, input_ids.shape)\n",
    "\n",
    "# Position ids: [0, 1, 2, ...]\n",
    "seq_length = input_ids.size(0)\n",
    "position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).expand_as(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e27e925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 66]) torch.Size([1, 5, 66]) torch.Size([1, 5, 66]) torch.Size([1, 5, 66])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ref_input_ids = ref_input_ids[:, 2, :]\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# input_ids = input_ids[:, 2, :]\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# token_type_ids = token_type_ids[:, 2, :]\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# attention_mask = attention_mask[:, 2, :]\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(ref_input_ids.shape, token_type_ids.shape, input_ids.shape, attention_mask.shape)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m attributions, delta = \u001b[43mlig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Here only attribution for the chosen input\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:521\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_ids = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.forward_func, \u001b[33m\"\u001b[39m\u001b[33mdevice_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m inputs_layer = \u001b[43m_forward_layer_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m input_layer_list: List[Tuple[Tensor, ...]]\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# if we have one output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:210\u001b[39m, in \u001b[36m_forward_layer_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_layer_eval\u001b[39m(\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\u001b[39;00m\n\u001b[32m    202\u001b[39m     forward_fn: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     grad_enabled: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    209\u001b[39m ) -> Union[Tuple[Tensor, ...], List[Tuple[Tensor, ...]]]:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_forward_layer_eval_with_neuron_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[6]: For 3rd argument expected `Module` but got\u001b[39;49;00m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  `ModuleOrModuleList`.\u001b[39;49;00m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_neuron_selector\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:506\u001b[39m, in \u001b[36m_forward_layer_eval_with_neuron_grads\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[39m\n\u001b[32m    503\u001b[39m grad_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gradient_neuron_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_enabled\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(grad_enabled):\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     saved_layer = \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m device_ids = _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[32m    514\u001b[39m \u001b[38;5;66;03m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:339\u001b[39m, in \u001b[36m_forward_layer_distributed_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[39m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    336\u001b[39m         all_hooks.append(\n\u001b[32m    337\u001b[39m             single_layer.register_forward_hook(hook_wrapper(single_layer))\n\u001b[32m    338\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m output = \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    348\u001b[39m output = cast(Tensor, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\common.py:588\u001b[39m, in \u001b[36m_run_forward\u001b[39m\u001b[34m(forward_func, inputs, target, additional_forward_args)\u001b[39m\n\u001b[32m    585\u001b[39m inputs = _format_inputs(inputs)\n\u001b[32m    586\u001b[39m additional_forward_args = _format_additional_forward_args(additional_forward_args)\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m output = \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch.futures.Future):\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output.then(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x.value(), target))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msquad_pos_forward_func\u001b[39m\u001b[34m(inputs, token_type_ids, position_ids, attention_mask, position)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msquad_pos_forward_func\u001b[39m(inputs, token_type_ids=\u001b[38;5;28;01mNone\u001b[39;00m, position_ids=\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, position=\u001b[32m0\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     logits = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(logits.max(\u001b[32m1\u001b[39m).values.shape)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits.max(\u001b[32m1\u001b[39m).values\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "# ref_input_ids = ref_input_ids[:, 2, :]\n",
    "# input_ids = input_ids[:, 2, :]\n",
    "# token_type_ids = token_type_ids[:, 2, :]\n",
    "# attention_mask = attention_mask[:, 2, :]\n",
    "\n",
    "\n",
    "print(ref_input_ids.shape, token_type_ids.shape, input_ids.shape, attention_mask.shape)\n",
    "\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids, # Here only attribution for the chosen input\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(token_type_ids, position_ids, attention_mask, 2),\n",
    "                                  target=2,\n",
    "                                  return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636244c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5f088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.\n"
     ]
    }
   ],
   "source": [
    "question = [\"A large truck breaks down out on the road and receives a push back into town by a small compact car. \\\n",
    "While the car, still pushing the truck, is speeding up to get up to cruising speed:\"]\n",
    "choices = [\n",
    "\"The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.\",\n",
    "\"The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.\",\n",
    "\"The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.\",\n",
    "\"The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot \\\n",
    "push back against the car. The truck is pushed forward simply because it is in the way of the car.\",\n",
    "\"Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.\"]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    question * len(choices),  # Repeat context for each choice\n",
    "    choices,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Add batch dimension\n",
    "input_ids = encoding[\"input_ids\"].unsqueeze(0)  # shape: (1, num_choices, seq_length)\n",
    "attention_mask = encoding[\"attention_mask\"].unsqueeze(0)\n",
    "\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.logits  # shape: (batch_size, num_choices)\n",
    "\n",
    "# Predicted answer\n",
    "predicted_choice = torch.argmax(logits, dim=1).item()\n",
    "print(\"Predicted:\", choices[predicted_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.\n"
     ]
    }
   ],
   "source": [
    "question = [\"A large truck breaks down out on the road and receives a push back into town by a small compact car. \\\n",
    "After the car reaches the constant cruising speed at which its driver wishes to push the truck:\"]\n",
    "\n",
    "choices = [\n",
    "\"The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.\",\n",
    "\"The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.\",\n",
    "\"The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.\",\n",
    "\"The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot \\\n",
    "push back against the car. The truck is pushed forward simply because it is in the way of the car.\",\n",
    "\"Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.\"\n",
    "]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    question * len(choices),  # Repeat context for each choice\n",
    "    choices,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Add batch dimension\n",
    "input_ids = encoding[\"input_ids\"].unsqueeze(0)  # shape: (1, num_choices, seq_length)\n",
    "attention_mask = encoding[\"attention_mask\"].unsqueeze(0)\n",
    "\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.logits  # shape: (batch_size, num_choices)\n",
    "\n",
    "# Predicted answer\n",
    "predicted_choice = torch.argmax(logits, dim=1).item()\n",
    "print(\"Predicted:\", choices[predicted_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd81f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81a5c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 17]) torch.Size([1, 2, 17]) torch.Size([1, 2, 17])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([50, 2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Attributions tensor and the end_point must match on the first dimension but found attribution: 2 and end_point: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(input_ids.shape, attention_mask.shape, token_type_ids.shape)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Compute attributions for the correct answer (index 0 = Paris)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m attributions, delta = \u001b[43mlig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Target the first choice (Paris)    \u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:588\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    586\u001b[39m     start_point, end_point = baselines, inps\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# computes approximation error based on the completeness axiom\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_convergence_delta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _format_outputs(\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer, \u001b[38;5;28mlist\u001b[39m), output), delta\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _format_outputs(\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer, \u001b[38;5;28mlist\u001b[39m), output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_utils\\attribution.py:310\u001b[39m, in \u001b[36mGradientAttribution.compute_convergence_delta\u001b[39m\u001b[34m(self, attributions, start_point, end_point, target, additional_forward_args)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# verify that the attributions and end_point match on 1st dimension\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attribution, end_point_tnsr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(attributions, end_point):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m end_point_tnsr.shape[\u001b[32m0\u001b[39m] == attribution.shape[\u001b[32m0\u001b[39m], (\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttributions tensor and the end_point must match on the first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m dimension but found attribution: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and end_point: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    313\u001b[39m             attribution.shape[\u001b[32m0\u001b[39m], end_point_tnsr.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    314\u001b[39m         )\n\u001b[32m    315\u001b[39m     )\n\u001b[32m    317\u001b[39m num_samples = end_point[\u001b[32m0\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m    318\u001b[39m _validate_input(end_point, start_point)\n",
      "\u001b[31mAssertionError\u001b[39m: Attributions tensor and the end_point must match on the first dimension but found attribution: 2 and end_point: 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Sample question with two options\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Paris is the capital of France.\", \"Berlin is the capital of France.\"]\n",
    "\n",
    "# Tokenize for multiple choice\n",
    "encoding = tokenizer(\n",
    "    [question] * len(choices),\n",
    "    choices,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Model expects input_ids, attention_mask, token_type_ids in a batch of choices\n",
    "input_ids = encoding[\"input_ids\"]       # shape: [num_choices, seq_len]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "\n",
    "# Prepare baseline \n",
    "with torch.no_grad():\n",
    "    baseline_input_ids = torch.ones_like(input_ids) * tokenizer.pad_token_id  # Use pad token as baseline\n",
    "\n",
    "# Choose the embedding layer\n",
    "embedding_layer = model.bert.embeddings\n",
    "\n",
    "# Forward function: outputs logits for each choice\n",
    "def forward_func(input_ids, attention_mask, token_type_ids):\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "    print(logits.shape)\n",
    "    return logits\n",
    "\n",
    "# Create a wrapper for LayerIntegratedGradients\n",
    "lig = LayerIntegratedGradients(forward_func, embedding_layer)\n",
    "\n",
    "# Preparing shape for LayerIntegratedGradients\n",
    "input_ids = input_ids.unsqueeze(0)  # Add batch dimension\n",
    "attention_mask = attention_mask.unsqueeze(0)  # Add batch dimension \n",
    "token_type_ids = token_type_ids.unsqueeze(0)  # Add batch dimension\n",
    "baseline_input_ids = baseline_input_ids.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(input_ids.shape, attention_mask.shape, token_type_ids.shape)\n",
    "\n",
    "# Compute attributions for the correct answer (index 0 = Paris)\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=baseline_input_ids,\n",
    "    additional_forward_args=(attention_mask, token_type_ids),\n",
    "    target=0,  # Target the first choice (Paris)    \n",
    "    return_convergence_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5bd56f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cafaf260a0c4f14835eab47b20de495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jonas\\.cache\\huggingface\\hub\\models--jonastokoliu--multi_choice_bert-base-uncased_swag_finetune. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ff7a950fe94356a091850134a01055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d40cda8cbb423286556c28aa709f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d66f814036497dbc07b88b458b1dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746de36e209e476a898100f28e522273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1939fc91ed4cd6ae756c5b21dc0fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3350e01960a4b92be88642ae19b1831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.9128],\n",
      "        [6.4859]], grad_fn=<ViewBackward0>)\n",
      "Question:  What is the capital of France?\n",
      "Predicted Answer:  Paris is the capital of France.\n",
      "torch.Size([2, 1, 17]) torch.Size([2, 1, 17]) torch.Size([2, 1, 17])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "Attributions per token (choice 0):\n",
      "tensor(-0.0021)\n"
     ]
    }
   ],
   "source": [
    "# trying instead flat input\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune'\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Sample question with two options\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Paris is the capital of France.\", \"Berlin is the capital of France.\"]\n",
    "\n",
    "# Tokenize for multiple choice\n",
    "encoding = tokenizer(\n",
    "    [question] * len(choices),\n",
    "    choices,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Model expects input_ids, attention_mask, token_type_ids in a batch of choices\n",
    "input_ids = encoding[\"input_ids\"]       # shape: [num_choices, seq_len]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "\n",
    "# Prepare baseline \n",
    "with torch.no_grad():\n",
    "    baseline_input_ids = torch.ones_like(input_ids) * tokenizer.pad_token_id  # Use pad token as baseline\n",
    "\n",
    "# Preparing shape for LayerIntegratedGradients\n",
    "input_ids = input_ids.unsqueeze(1)  # Add batch dimension\n",
    "attention_mask = attention_mask.unsqueeze(1)  # Add batch dimension \n",
    "token_type_ids = token_type_ids.unsqueeze(1)  # Add batch dimension\n",
    "baseline_input_ids = baseline_input_ids.unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "\n",
    "# Compute model prediction\n",
    "scores = predict(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "print(scores)\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', choices[torch.argmax(scores).item()])\n",
    "\n",
    "# Choose the embedding layer\n",
    "embedding_layer = model.bert.embeddings\n",
    "\n",
    "# Forward function: outputs logits for each choice\n",
    "def forward_func(input_ids, attention_mask, token_type_ids):\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "    print(logits.shape)\n",
    "    return logits\n",
    "\n",
    "# Create a wrapper for LayerIntegratedGradients\n",
    "lig = LayerIntegratedGradients(forward_func, embedding_layer)\n",
    "\n",
    "print(input_ids.shape, attention_mask.shape, token_type_ids.shape)\n",
    "\n",
    "# Compute attributions for the correct answer (index 0 = Paris)\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=baseline_input_ids,\n",
    "    additional_forward_args=(attention_mask, token_type_ids),\n",
    "    target=0,  # Target the first choice (Paris)    \n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum across embedding dimensions to get token-level importance\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "\n",
    "print(\"Attributions per token (choice 0):\")\n",
    "print(token_attributions[0][0])  # First sample, first choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55a2feac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Paris is the capital of France.</b></text></td><td><text style=\"padding-right:2em\"><b>0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>Paris is the capital of France.</b></text></td><td><text style=\"padding-right:2em\"><b>-0.06</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Paris is the capital of France.</b></text></td><td><text style=\"padding-right:2em\"><b>0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>Paris is the capital of France.</b></text></td><td><text style=\"padding-right:2em\"><b>-0.06</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "ground_truth = choices[0]  # Assuming first choice is correct\n",
    "\n",
    "choice_idx = torch.argmax(scores).item()\n",
    "indices = input_ids[choice_idx,0].detach().tolist()  # batch=0, choice=choice_idx\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "token_attributions_for_choice = token_attributions[choice_idx]\n",
    "\n",
    "vis = viz.VisualizationDataRecord(\n",
    "    word_attributions=token_attributions_for_choice,\n",
    "    pred_prob=torch.max(torch.softmax(scores[0], dim=0)).item(),\n",
    "    pred_class=choice_idx,\n",
    "    true_class=ground_truth,\n",
    "    attr_class=str(ground_truth),\n",
    "    attr_score=token_attributions_for_choice.sum().item(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=delta[choice_idx].item()\n",
    ")\n",
    "\n",
    "print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "viz.visualize_text([vis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc53778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 17])\n",
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     11\u001b[39m vis = viz.VisualizationDataRecord(\n\u001b[32m     12\u001b[39m                         token_attributions, \n\u001b[32m     13\u001b[39m                         torch.max(torch.softmax(scores[\u001b[32m0\u001b[39m], dim=\u001b[32m0\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m                         all_tokens,\n\u001b[32m     19\u001b[39m                         delta)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[1m\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVisualizations For Start Position\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[0m\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mviz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_utils\\visualization.py:1038\u001b[39m, in \u001b[36mvisualize_text\u001b[39m\u001b[34m(datarecords, legend)\u001b[39m\n\u001b[32m   1018\u001b[39m rows = [\n\u001b[32m   1019\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<tr><th>True Label</th>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<th>Predicted Label</th>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m<th>Word Importance</th>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m ]\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m datarecord \u001b[38;5;129;01min\u001b[39;00m datarecords:\n\u001b[32m   1026\u001b[39m     rows.append(\n\u001b[32m   1027\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m   1028\u001b[39m             [\n\u001b[32m   1029\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m<tr>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1030\u001b[39m                 format_classname(datarecord.true_class),\n\u001b[32m   1031\u001b[39m                 format_classname(\n\u001b[32m   1032\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{1:.2f}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   1033\u001b[39m                         datarecord.pred_class, datarecord.pred_prob\n\u001b[32m   1034\u001b[39m                     )\n\u001b[32m   1035\u001b[39m                 ),\n\u001b[32m   1036\u001b[39m                 format_classname(datarecord.attr_class),\n\u001b[32m   1037\u001b[39m                 format_classname(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0:.2f}\u001b[39;00m\u001b[33m\"\u001b[39m.format(datarecord.attr_score)),\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m                 \u001b[43mformat_word_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdatarecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatarecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_attributions\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m<tr>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1042\u001b[39m             ]\n\u001b[32m   1043\u001b[39m         )\n\u001b[32m   1044\u001b[39m     )\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m legend:\n\u001b[32m   1047\u001b[39m     dom.append(\n\u001b[32m   1048\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m<div style=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mborder-top: 1px solid; margin-top: 5px; \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33m        padding-top: 5px; display: inline-block\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1050\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_utils\\visualization.py:995\u001b[39m, in \u001b[36mformat_word_importances\u001b[39m\u001b[34m(words, importances)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(importances) == \u001b[32m0\u001b[39m:\n\u001b[32m    994\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m<td></td>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words) <= \u001b[38;5;28mlen\u001b[39m(importances)\n\u001b[32m    996\u001b[39m tags = [\u001b[33m\"\u001b[39m\u001b[33m<td>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word, importance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(words, importances[: \u001b[38;5;28mlen\u001b[39m(words)]):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "print(input_ids.shape)\n",
    "choice_idx = 0\n",
    "indices = input_ids[choice_idx,0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "ground_truth = choices[0]  # Assuming the first choice is the ground truth\n",
    "\n",
    "# storing couple samples in an array for visualization purposes\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions, \n",
    "                        torch.max(torch.softmax(scores[0], dim=0)),\n",
    "                        torch.argmax(scores),\n",
    "                        torch.argmax(scores),\n",
    "                        str(ground_truth),\n",
    "                        token_attributions.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta)\n",
    "\n",
    "\n",
    "print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "viz.visualize_text([vis])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

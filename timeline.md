### Project Timeline

## Week 25
- Monday: General introduction 
- Tuesday: Specific introduction
- Wedenesday-Friday: Research
  - Neel Nanda reading list (2024, potentially outdated): https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1
    - Sparse autoencoders
    - Circuits
  - Arena tutorials?
  - Neel Nanda videos
- Weekend: Start coding/implementing

## Week 26



### Relevant articles

Kode (av Neel Nanda): https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp


### Questions to Ane 

-Burde vi bruke TransformerLens-verktøyet i Python? 

### Notes

## Glossary of terms: 

-Patching: Patching means to switch activations from one run of a model (input A) in certain parts of the neural network with the activations from another run of the model (input B) in certain specified locations in the network. 
-Causality means a change or intervention in a part of the network leads to a change in the model’s output or behavior. Attempts to find which parts of a network causes a behavior/output. 
-d_model: dimension of embedding vectors (in input/output). 12288 for GPT-3.  d_head: (reduced) dimension of key/query space in attention head. 128 for GPT-3. In general: d_model/d_head = n_head. 
-Residual stream: 
  



{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144ce5e8",
   "metadata": {},
   "source": [
    "# Q&A example with TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdbc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618b204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27be1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attn_patterns(model, text, layers, compact=True):\n",
    "    str_tokens = model.to_str_tokens(text)\n",
    "    logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "    if compact:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "    \n",
    "    else:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_heads(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c254",
   "metadata": {},
   "source": [
    "We set our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cdb20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1449c1",
   "metadata": {},
   "source": [
    "We import and embed the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c2de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "['0+0=', '1+2=', '2+4=', '3+6=', '4+8=', '5+10=', '6+12=', '7+14=', '8+16=', '9+18=']\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "'''\n",
    "with open(\"Test.json\",\"r\") as infile: \n",
    "    infile.readline()\n",
    "    lines = infile.readlines()\n",
    "    '''\n",
    "\n",
    "with open(\"Test.json\", \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "#data = [json.loads(line.strip(\"\\n\").rstrip(',').rstrip(\"]\")) for line in lines]\n",
    "\n",
    "questions = [line[\"question\"] for line in data]\n",
    "answers = [line[\"rationale\"] for line in data]\n",
    "\n",
    "#Embedding the questions and answers\n",
    "question_embeds = model.to_tokens(questions)\n",
    "answer_embeds = model.to_tokens(answers)\n",
    "\n",
    "str_questions = model.to_str_tokens(questions)\n",
    "str_answers = model.to_str_tokens(answers)\n",
    "\n",
    "print(len(question_embeds), len(answer_embeds))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "829baee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-160e598c-811f\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-160e598c-811f\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"0\", \"+\", \"0\", \"=\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0], [0.9891597628593445, 0.010840212926268578, 0.0, 0.0, 0.0], [0.8966480493545532, 0.0427163690328598, 0.06063550338149071, 0.0, 0.0], [0.8851548433303833, 0.025741858407855034, 0.0726809948682785, 0.016422390937805176, 0.0], [0.5714359283447266, 0.09132212400436401, 0.19835905730724335, 0.07240286469459534, 0.06648008525371552]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0028671380132436752, 0.9971328973770142, 0.0, 0.0, 0.0], [0.0006681804079562426, 0.0006948129157535732, 0.9986370205879211, 0.0, 0.0], [0.0010404394706711173, 0.5419923067092896, 0.0012282029492780566, 0.4557391405105591, 0.0], [0.0008445467101410031, 0.0006854899111203849, 0.03564046695828438, 0.00021290591394063085, 0.9626166224479675]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9730475544929504, 0.026952486485242844, 0.0, 0.0, 0.0], [0.8931781649589539, 0.061432916671037674, 0.04538885876536369, 0.0, 0.0], [0.7272925972938538, 0.06123005598783493, 0.17107561230659485, 0.040401674807071686, 0.0], [0.7210594415664673, 0.07204141467809677, 0.07926400005817413, 0.04949994757771492, 0.07813514024019241]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.18119114637374878, 0.8188088536262512, 0.0, 0.0, 0.0], [0.03745585307478905, 0.01766420342028141, 0.9448798894882202, 0.0, 0.0], [0.009712415747344494, 0.22267566621303558, 0.01947314478456974, 0.7481387853622437, 0.0], [0.0294822808355093, 0.023622211068868637, 0.046667344868183136, 0.09546057134866714, 0.8047676086425781]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8976199626922607, 0.10237998515367508, 0.0, 0.0, 0.0], [0.5964090824127197, 0.08323649317026138, 0.32035449147224426, 0.0, 0.0], [0.31344544887542725, 0.20004408061504364, 0.2166450172662735, 0.269865483045578, 0.0], [0.49432826042175293, 0.06928491592407227, 0.1609697937965393, 0.07716358453035355, 0.19825342297554016]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.08274372667074203, 0.9172562956809998, 0.0, 0.0, 0.0], [0.12487166374921799, 0.003526201704517007, 0.8716021776199341, 0.0, 0.0], [0.01102330069988966, 0.574510931968689, 0.00028829489019699395, 0.41417741775512695, 0.0], [0.012569474056363106, 0.011247579008340836, 0.005851686932146549, 0.0048193782567977905, 0.9655119180679321]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9638966917991638, 0.03610330447554588, 0.0, 0.0, 0.0], [0.8301544785499573, 0.0636177733540535, 0.10622770339250565, 0.0, 0.0], [0.5439756512641907, 0.09135548770427704, 0.29729515314102173, 0.06737374514341354, 0.0], [0.4876915514469147, 0.09923647344112396, 0.24315625429153442, 0.07081545889377594, 0.0991002693772316]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9709039330482483, 0.029096119105815887, 0.0, 0.0, 0.0], [0.4836089313030243, 0.3254626989364624, 0.1909283995628357, 0.0, 0.0], [0.40509575605392456, 0.11874889582395554, 0.28481367230415344, 0.19134163856506348, 0.0], [0.30637040734291077, 0.09529449045658112, 0.17412970960140228, 0.27601006627082825, 0.14819535613059998]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9422704577445984, 0.057729512453079224, 0.0, 0.0, 0.0], [0.8342065215110779, 0.04270046576857567, 0.12309300154447556, 0.0, 0.0], [0.6130539774894714, 0.09210546314716339, 0.22341088950634003, 0.07142971456050873, 0.0], [0.2795199453830719, 0.06695786863565445, 0.4311180114746094, 0.04898301884531975, 0.17342112958431244]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8820385932922363, 0.1179613545536995, 0.0, 0.0, 0.0], [0.7555484175682068, 0.1569860875606537, 0.08746553957462311, 0.0, 0.0], [0.6106290817260742, 0.13170142471790314, 0.1528821885585785, 0.10478729009628296, 0.0], [0.5562772154808044, 0.13424058258533478, 0.12196172028779984, 0.1104963943362236, 0.07702414691448212]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7978128790855408, 0.202187180519104, 0.0, 0.0, 0.0], [0.6486339569091797, 0.11512016505002975, 0.23624585568904877, 0.0, 0.0], [0.3924897611141205, 0.3342163860797882, 0.07724018394947052, 0.19605369865894318, 0.0], [0.46732157468795776, 0.14305149018764496, 0.10477343946695328, 0.07879626750946045, 0.20605729520320892]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.766453206539154, 0.23354680836200714, 0.0, 0.0, 0.0], [0.7728310227394104, 0.18068861961364746, 0.04648034647107124, 0.0, 0.0], [0.6621089577674866, 0.1401902586221695, 0.050517208874225616, 0.1471835970878601, 0.0], [0.5833333134651184, 0.14076516032218933, 0.061092063784599304, 0.15323396027088165, 0.061575502157211304]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x2b822224e60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ef8654c2-10fd\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ef8654c2-10fd\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"1\", \"+\", \"2\", \"=\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0], [0.9872710108757019, 0.012728998437523842, 0.0, 0.0, 0.0], [0.9101009368896484, 0.02835373394191265, 0.06154525279998779, 0.0, 0.0], [0.829233705997467, 0.03240637108683586, 0.12042691558599472, 0.017933016642928123, 0.0], [0.5596994757652283, 0.09791040420532227, 0.1942850649356842, 0.08299046754837036, 0.06511468440294266]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.003795072203502059, 0.9962049126625061, 0.0, 0.0, 0.0], [0.0006679092184640467, 0.0011003856780007482, 0.9982317090034485, 0.0, 0.0], [0.0010826325742527843, 0.013598272576928139, 0.015629148110747337, 0.9696899056434631, 0.0], [0.0008440457750111818, 0.001226973021402955, 0.03561932593584061, 0.0002640008751768619, 0.9620456695556641]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9445667862892151, 0.05543322488665581, 0.0, 0.0, 0.0], [0.8745049834251404, 0.08105504512786865, 0.04443993791937828, 0.0, 0.0], [0.5988194942474365, 0.17447686195373535, 0.09442896395921707, 0.13227476179599762, 0.0], [0.6942626237869263, 0.08551467955112457, 0.07631830126047134, 0.06867299973964691, 0.0752313956618309]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.2637639343738556, 0.7362360954284668, 0.0, 0.0, 0.0], [0.03745085000991821, 0.017795423045754433, 0.9447537064552307, 0.0, 0.0], [0.018399780616164207, 0.0670437142252922, 0.07592573761940002, 0.838630735874176, 0.0], [0.03187906742095947, 0.018793845549225807, 0.05046120285987854, 0.02867397479712963, 0.8701918721199036]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9062865972518921, 0.09371338784694672, 0.0, 0.0, 0.0], [0.587710976600647, 0.09660667181015015, 0.31568241119384766, 0.0, 0.0], [0.3755447268486023, 0.17222115397453308, 0.19940818846225739, 0.25282591581344604, 0.0], [0.5077248811721802, 0.07143720984458923, 0.16533218324184418, 0.05187951773405075, 0.20362623035907745]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.22788019478321075, 0.7721198201179504, 0.0, 0.0, 0.0], [0.12504805624485016, 0.002118498319759965, 0.8728333711624146, 0.0, 0.0], [0.10868213325738907, 0.01135842502117157, 0.0014058664673939347, 0.878553569316864, 0.0], [0.012761140242218971, 0.0007352137472480536, 0.005940916948020458, 0.000328096270095557, 0.9802346229553223]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9654070138931274, 0.03459296002984047, 0.0, 0.0, 0.0], [0.830137312412262, 0.06363720446825027, 0.10622550547122955, 0.0, 0.0], [0.5820505023002625, 0.11243720352649689, 0.21838906407356262, 0.08712323009967804, 0.0], [0.5263121128082275, 0.06775940954685211, 0.2624119222164154, 0.036568496376276016, 0.10694807022809982]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9809218049049377, 0.019078122451901436, 0.0, 0.0, 0.0], [0.4804436266422272, 0.3298775851726532, 0.18967874348163605, 0.0, 0.0], [0.304563969373703, 0.15698330104351044, 0.3820182681083679, 0.15643438696861267, 0.0], [0.3039604723453522, 0.09384484589099884, 0.1727599948644638, 0.28240513801574707, 0.14702963829040527]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8457146286964417, 0.15428532660007477, 0.0, 0.0, 0.0], [0.8102120757102966, 0.07023544609546661, 0.11955245584249496, 0.0, 0.0], [0.3850944936275482, 0.26235726475715637, 0.17991076409816742, 0.172637477517128, 0.0], [0.2427377849817276, 0.1395605057477951, 0.3743869960308075, 0.09271416813135147, 0.15060055255889893]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8588211536407471, 0.14117877185344696, 0.0, 0.0, 0.0], [0.7464104890823364, 0.16718192398548126, 0.08640769124031067, 0.0, 0.0], [0.5860289931297302, 0.1644473671913147, 0.12605907022953033, 0.12346448749303818, 0.0], [0.5270518660545349, 0.16005991399288177, 0.11555416882038116, 0.12435651570558548, 0.07297749817371368]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7837377190589905, 0.21626225113868713, 0.0, 0.0, 0.0], [0.6191799640655518, 0.15530194342136383, 0.2255181074142456, 0.0, 0.0], [0.4880222678184509, 0.1611311137676239, 0.10218155384063721, 0.24866507947444916, 0.0], [0.44818630814552307, 0.1676366627216339, 0.10048331320285797, 0.08607374876737595, 0.19761992990970612]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7410699725151062, 0.2589300572872162, 0.0, 0.0, 0.0], [0.7646116018295288, 0.18940238654613495, 0.045986004173755646, 0.0, 0.0], [0.6684932708740234, 0.16126954555511475, 0.057166486978530884, 0.11307068914175034, 0.0], [0.5873878598213196, 0.16800878942012787, 0.061516694724559784, 0.12108318507671356, 0.062003493309020996]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x2b8239a3ec0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_attn_patterns(model, questions[0], layers=[0])\n",
    "vis_attn_patterns(model, questions[1], layers=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9d17c",
   "metadata": {},
   "source": [
    "Trying activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc528f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean answer logit difference: 3.1147\n",
      "Corrupted answer logit difference: -4.0909\n"
     ]
    }
   ],
   "source": [
    "clean_prompt = \"What is the capital of France?\"\n",
    "corrupted_prompt = \"What is the capital of England?\"\n",
    "\n",
    "clean_answer = \"Paris\"\n",
    "corrupted_answer = \"London\"\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "corrupted_logits = model(corrupted_prompt)\n",
    "\n",
    "clean_index = model.to_single_token(clean_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "print(f\"Clean answer logit difference: {clean_diff:.4f}\")\n",
    "\n",
    "corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "print(f\"Corrupted answer logit difference: {corrupted_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383d40d",
   "metadata": {},
   "source": [
    "Then we want to patch the clean prompt onto the corrupted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54862200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(resid_pre, hook, position, clean_cache):\n",
    "    clean_activation = clean_cache[hook.name]\n",
    "    resid_pre[:, position, :] = clean_activation[:, position, :]\n",
    "    return resid_pre\n",
    "\n",
    "def model_data(model, prompt):\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    return tokens, logits, cache\n",
    "\n",
    "#num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "#patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "\n",
    "def activation_patching(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer, store_corrupted_cache=False):\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "    if store_corrupted_cache:\n",
    "        corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_prompt, remove_batch_dim=True)\n",
    "    else:\n",
    "        corrupted_logits = model(corrupted_prompt)\n",
    "\n",
    "    clean_index = model.to_single_token(clean_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "    clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "    corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "\n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "    num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "   \n",
    "    patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "    for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(num_positions):\n",
    "            # We use a temporary hook with functool.partial to patch at each position\n",
    "            temp_hook = partial(activation_patching_hook, position=position, clean_cache=clean_cache)\n",
    "            # We then run the model with hooks as usual\n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, \n",
    "                                                  fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), temp_hook)])\n",
    "            # We then calculate the logit difference\n",
    "            patched_diff = (patched_logits[0, -1, clean_index] - patched_logits[0, -1, corrupted_index]).detach()\n",
    "            # We then store the result in the patching_result tensor, normalizing it\n",
    "            patching_result[layer, position] = (patched_diff - corrupted_diff) / (clean_diff - corrupted_diff)\n",
    "\n",
    "    if store_corrupted_cache:\n",
    "        return patching_result, patched_logits, corrupted_cache\n",
    "    else:\n",
    "        return patching_result, patched_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53bd1c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "Looking for;  blocks.0.hook_activation\n",
      "Number of positions: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6981a2cc1b48459e726b1456d365c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Position 0\n",
      "Layer 0, Position 1\n",
      "Layer 0, Position 2\n",
      "Layer 0, Position 3\n",
      "Layer 0, Position 4\n",
      "Layer 0, Position 5\n",
      "Layer 0, Position 6\n",
      "Layer 0, Position 7\n",
      "Layer 1, Position 0\n",
      "Layer 1, Position 1\n",
      "Layer 1, Position 2\n",
      "Layer 1, Position 3\n",
      "Layer 1, Position 4\n",
      "Layer 1, Position 5\n",
      "Layer 1, Position 6\n",
      "Layer 1, Position 7\n",
      "Layer 2, Position 0\n",
      "Layer 2, Position 1\n",
      "Layer 2, Position 2\n",
      "Layer 2, Position 3\n",
      "Layer 2, Position 4\n",
      "Layer 2, Position 5\n",
      "Layer 2, Position 6\n",
      "Layer 2, Position 7\n",
      "Layer 3, Position 0\n",
      "Layer 3, Position 1\n",
      "Layer 3, Position 2\n",
      "Layer 3, Position 3\n",
      "Layer 3, Position 4\n",
      "Layer 3, Position 5\n",
      "Layer 3, Position 6\n",
      "Layer 3, Position 7\n",
      "Layer 4, Position 0\n",
      "Layer 4, Position 1\n",
      "Layer 4, Position 2\n",
      "Layer 4, Position 3\n",
      "Layer 4, Position 4\n",
      "Layer 4, Position 5\n",
      "Layer 4, Position 6\n",
      "Layer 4, Position 7\n",
      "Layer 5, Position 0\n",
      "Layer 5, Position 1\n",
      "Layer 5, Position 2\n",
      "Layer 5, Position 3\n",
      "Layer 5, Position 4\n",
      "Layer 5, Position 5\n",
      "Layer 5, Position 6\n",
      "Layer 5, Position 7\n",
      "Layer 6, Position 0\n",
      "Layer 6, Position 1\n",
      "Layer 6, Position 2\n",
      "Layer 6, Position 3\n",
      "Layer 6, Position 4\n",
      "Layer 6, Position 5\n",
      "Layer 6, Position 6\n",
      "Layer 6, Position 7\n",
      "Layer 7, Position 0\n",
      "Layer 7, Position 1\n",
      "Layer 7, Position 2\n",
      "Layer 7, Position 3\n",
      "Layer 7, Position 4\n",
      "Layer 7, Position 5\n",
      "Layer 7, Position 6\n",
      "Layer 7, Position 7\n",
      "Layer 8, Position 0\n",
      "Layer 8, Position 1\n",
      "Layer 8, Position 2\n",
      "Layer 8, Position 3\n",
      "Layer 8, Position 4\n",
      "Layer 8, Position 5\n",
      "Layer 8, Position 6\n",
      "Layer 8, Position 7\n",
      "Layer 9, Position 0\n",
      "Layer 9, Position 1\n",
      "Layer 9, Position 2\n",
      "Layer 9, Position 3\n",
      "Layer 9, Position 4\n",
      "Layer 9, Position 5\n",
      "Layer 9, Position 6\n",
      "Layer 9, Position 7\n",
      "Layer 10, Position 0\n",
      "Layer 10, Position 1\n",
      "Layer 10, Position 2\n",
      "Layer 10, Position 3\n",
      "Layer 10, Position 4\n",
      "Layer 10, Position 5\n",
      "Layer 10, Position 6\n",
      "Layer 10, Position 7\n",
      "Layer 11, Position 0\n",
      "Layer 11, Position 1\n",
      "Layer 11, Position 2\n",
      "Layer 11, Position 3\n",
      "Layer 11, Position 4\n",
      "Layer 11, Position 5\n",
      "Layer 11, Position 6\n",
      "Layer 11, Position 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(clean_cache.keys()))\n",
    "print(\"Looking for; \", utils.get_act_name(\"activation\", 0))\n",
    "patching_result = activation_patching(model, clean_prompt, model.to_tokens(corrupted_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74147f",
   "metadata": {},
   "source": [
    "We then visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c46c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patching_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m token_labels = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model.to_str_tokens(clean_prompt))]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m imshow(patching_result, x=token_labels, xaxis=\u001b[33m\"\u001b[39m\u001b[33mPosition\u001b[39m\u001b[33m\"\u001b[39m, yaxis=\u001b[33m\"\u001b[39m\u001b[33mLayer\u001b[39m\u001b[33m\"\u001b[39m, title=\u001b[33m\"\u001b[39m\u001b[33mPatching Result\u001b[39m\u001b[33m\"\u001b[39m,)\n",
      "\u001b[31mNameError\u001b[39m: name 'patching_result' is not defined"
     ]
    }
   ],
   "source": [
    "token_labels = [f'{token}_{index}' for index, token in enumerate(model.to_str_tokens(clean_prompt))]\n",
    "imshow(patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Patching Result\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75333f",
   "metadata": {},
   "source": [
    "When patching the clean results onto the corrupted results, the change is first very localized, but is then brought to the end in the last few layers. \n",
    "We now want to try with a more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89cd6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_physics_prompt = \"The action of adding numbers is called\"\n",
    "corrupted_physics_prompt = \"The action of multiplying numbers is called\"\n",
    "clean_answer = \" addition\"\n",
    "corrupted_answer = \" multiplication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87dc8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e85536c0d4bcc9a7d98a0a06dbf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'The', ' action', ' of', ' multiplying', ' numbers', ' is', ' called']\n",
      "Tokenized answer: [' multiplication']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.75</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.70</span><span style=\"font-weight: bold\">% Token: | multiplication|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m12.75\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.70\u001b[0m\u001b[1m% Token: | multiplication|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.49 Prob:  9.86% Token: | the|\n",
      "Top 1th token. Logit: 12.85 Prob:  5.19% Token: | \"|\n",
      "Top 2th token. Logit: 12.75 Prob:  4.70% Token: | multiplication|\n",
      "Top 3th token. Logit: 12.15 Prob:  2.57% Token: | division|\n",
      "Top 4th token. Logit: 11.89 Prob:  1.99% Token: | differentiation|\n",
      "Top 5th token. Logit: 11.75 Prob:  1.73% Token: | multipl|\n",
      "Top 6th token. Logit: 11.67 Prob:  1.60% Token: | trig|\n",
      "Top 7th token. Logit: 11.59 Prob:  1.46% Token: | '|\n",
      "Top 8th token. Logit: 11.50 Prob:  1.35% Token: | a|\n",
      "Top 9th token. Logit: 11.41 Prob:  1.23% Token: | fraction|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' multiplication'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' multiplication'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate(clean_physics_prompt, max_new_tokens=10, temperature=0.0, top_p=1.0, do_sample=False)\n",
    "utils.test_prompt(corrupted_physics_prompt, corrupted_answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28764a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "027c66ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90bd22e7d0b41d289ca7ec0792b348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "<|endoftext|>_0",
          "The_1",
          " action_2",
          " of_3",
          " adding_4",
          " numbers_5",
          " is_6",
          " called_7"
         ],
         "xaxis": "x",
         "yaxis": "y",
         "z": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAgD8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3FqCP8KWxbqHTKy7lry4vAAAAAAAAAAAAAAAAAAAAAAkeYU/BdaNOYZDartyyiy9AAAAAAAAAAAAAAAAAAAAAG6Zhj/CzJk917bIPMHgl70AAAAAAAAAAAAAAAAAAAAA8Wp+PxNmHT6YkkY8xhF0vQAAAAAAAAAAAAAAAAAAAADei3A/NI9cPpSV+7vxIhC9AAAAAAAAAAAAAAAAAAAAADuiYT/TPmw+qFn1uyBz9bwAAAAAAAAAAAAAAAAAAAAA+DJRPx0+fT5Xzmy8m/UKvQAAAAAAAAAAAAAAAAAAAAAURFU/WOFNPkAdNjstHBg8AAAAAAAAAAAAAAAAAAAAAI8RTj/l4Vo+Q1PKPJY8sDwAAAAAAAAAAAAAAAAAAAAA9GBAPwEgZT5I4N48LV8NPQAAAAAAAAAAAAAAAAAAAAAHDUI/UwxuPpmJET0VsC49AAAAAAAAAAAAAAAAAAAAADYrOD/R5oY+HyG6PGYMjz0AAAAAAAAAAAAAAAAAAAAAcoMvPzFNiz5dHRE9I0ySPQAAAAAAAAAAAAAAAAAAAABNMi0/jNpbPr3qKT22yu09AAAAAAAAAAAAAAAAAAAAANPTJz/ZqlI+DlHJPHohGT4AAAAAAAAAAAAAAAAAAAAAmmMEP24xZD6ev1Q8dzE9PgAAAAAAAAAAAAAAAAAAAADiUNg+/JBaPmkhRjxs94E+AAAAAAAAAAAAAAAAAAAAAMgbqD5X0zM+RwTNOs4tUz4AAAAAAAAAAAAAAAAAAAAAhXFnPkmKtT3jOZi6GHfOPgAAAAAAAAAAAAAAAAAAAAAagYo+bfeRPSGAI7ow2cE+AAAAAAAAAAAAAAAAAAAAAKhiSD5DS5A9v6QhuSoX3D4AAAAAAAAAAAAAAAAAAAAAhiQSPQi4MD3pKRM7nuY7PwAAAAAAAAAAAAAAAAAAAACcAcA9Yv0bPKrlHzs4a08/",
          "dtype": "f4",
          "shape": "24, 8"
         }
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Patching Result"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Position"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patching_result_physics = activation_patching(model, clean_physics_prompt, corrupted_physics_prompt, clean_answer, corrupted_answer, store_corrupted_cache=True)\n",
    "#print(\"Patching result shape:\", patching_result_physics[1].shape)\n",
    "token_labels = [f'{token}_{index}' for index, token in enumerate(model.to_str_tokens(clean_physics_prompt))]\n",
    "imshow(patching_result_physics[0], x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Patching Result\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c83eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache keys: dict_keys(['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])\n",
      "Attention pattern shape: torch.Size([16, 8, 8])\n",
      "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "min: 0.0 max: 1.0 mean: 0.125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5c88a6b3-2c88\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5c88a6b3-2c88\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"The\", \" action\", \" of\", \" adding\", \" numbers\", \" is\", \" called\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8908061981201172, 0.10919377207756042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7488934397697449, 0.09257612377405167, 0.15853042900562286, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5527200102806091, 0.10671655088663101, 0.29400867223739624, 0.04655482992529869, 0.0, 0.0, 0.0, 0.0], [0.4731975793838501, 0.07838192582130432, 0.11403077095746994, 0.03875517472624779, 0.2956346571445465, 0.0, 0.0, 0.0], [0.34078943729400635, 0.07139888405799866, 0.12402920424938202, 0.027166366577148438, 0.3706888258457184, 0.06592723727226257, 0.0, 0.0], [0.2568972110748291, 0.037482984364032745, 0.11873263865709305, 0.0322856530547142, 0.42247816920280457, 0.057431649416685104, 0.07469170540571213, 0.0], [0.2439350038766861, 0.03971197456121445, 0.09094070643186569, 0.02063569612801075, 0.29468461871147156, 0.045223601162433624, 0.13521559536457062, 0.12965284287929535]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49648579955101013, 0.5035141706466675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3826310932636261, 0.49265575408935547, 0.12471321225166321, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19287008047103882, 0.2674787640571594, 0.22131049633026123, 0.3183407187461853, 0.0, 0.0, 0.0, 0.0], [0.23734985291957855, 0.3131917417049408, 0.09532269090414047, 0.23775365948677063, 0.11638191342353821, 0.0, 0.0, 0.0], [0.23183931410312653, 0.28794270753860474, 0.06841807812452316, 0.24690790474414825, 0.0844256803393364, 0.08046624064445496, 0.0, 0.0], [0.16295982897281647, 0.14763763546943665, 0.10750638693571091, 0.1890457272529602, 0.08455568552017212, 0.13806535303592682, 0.17022937536239624, 0.0], [0.16367767751216888, 0.17839676141738892, 0.06405951827764511, 0.1959240883588791, 0.06785625964403152, 0.07882250100374222, 0.12222496420145035, 0.12903815507888794]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5499619245529175, 0.4500381052494049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5572860836982727, 0.4241708219051361, 0.0185431856662035, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09327536076307297, 0.03965114802122116, 0.0033341096714138985, 0.8637394309043884, 0.0, 0.0, 0.0, 0.0], [0.21471883356571198, 0.15189169347286224, 0.009956355206668377, 0.612041711807251, 0.01139138825237751, 0.0, 0.0, 0.0], [0.2752366065979004, 0.20558924973011017, 0.009892068803310394, 0.47574591636657715, 0.014920064248144627, 0.01861611194908619, 0.0, 0.0], [0.09288520365953445, 0.05348174646496773, 0.003065716475248337, 0.3389788269996643, 0.004466141574084759, 0.004446736071258783, 0.5026755928993225, 0.0], [0.10565552115440369, 0.10318785905838013, 0.006032145116478205, 0.3014555275440216, 0.007688215468078852, 0.009504595771431923, 0.44892552495002747, 0.01755053736269474]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8110514879226685, 0.18894855678081512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.621932327747345, 0.31239065527915955, 0.06567702442407608, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6949179172515869, 0.1424742490053177, 0.03328398987650871, 0.1293238252401352, 0.0, 0.0, 0.0, 0.0], [0.5809587240219116, 0.16922162473201752, 0.027332773432135582, 0.19652429223060608, 0.025962654501199722, 0.0, 0.0, 0.0], [0.4074665606021881, 0.21667057275772095, 0.034599147737026215, 0.28670617938041687, 0.02936839684844017, 0.025189079344272614, 0.0, 0.0], [0.5832232236862183, 0.10605286806821823, 0.021481461822986603, 0.13163290917873383, 0.015609189867973328, 0.015064651146531105, 0.12693561613559723, 0.0], [0.5543068051338196, 0.10428761690855026, 0.017093824222683907, 0.13458651304244995, 0.016496870666742325, 0.015536980703473091, 0.12351492047309875, 0.03417643532156944]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8722108006477356, 0.12778916954994202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3259766101837158, 0.07394306361675262, 0.6000803112983704, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17851515114307404, 0.0890803188085556, 0.2518180012702942, 0.48058658838272095, 0.0, 0.0, 0.0, 0.0], [0.19247041642665863, 0.05631206929683685, 0.1829727739095688, 0.476529061794281, 0.09171559661626816, 0.0, 0.0, 0.0], [0.16315990686416626, 0.04176773875951767, 0.34221333265304565, 0.2491386979818344, 0.10581281036138535, 0.09790743142366409, 0.0, 0.0], [0.08957821130752563, 0.05772102624177933, 0.22641783952713013, 0.2868742346763611, 0.11699061095714569, 0.0635460615158081, 0.15887200832366943, 0.0], [0.08956947177648544, 0.04497635364532471, 0.23383654654026031, 0.25545814633369446, 0.08535981178283691, 0.06287846714258194, 0.10517449676990509, 0.12274673581123352]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9400989413261414, 0.05990113317966461, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8211066126823425, 0.08481179177761078, 0.09408163279294968, 0.0, 0.0, 0.0, 0.0, 0.0], [0.576495885848999, 0.2180592268705368, 0.1399880051612854, 0.06545690447092056, 0.0, 0.0, 0.0, 0.0], [0.695472776889801, 0.06555866450071335, 0.1537761688232422, 0.03560206666588783, 0.04959037899971008, 0.0, 0.0, 0.0], [0.6122575998306274, 0.04879744350910187, 0.20171698927879333, 0.01176081970334053, 0.07170888781547546, 0.05375829339027405, 0.0, 0.0], [0.3713464140892029, 0.12889721989631653, 0.1475113034248352, 0.05163402110338211, 0.11319603025913239, 0.15658801794052124, 0.030826983973383904, 0.0], [0.4938068389892578, 0.10257838666439056, 0.1291726529598236, 0.02667570486664772, 0.08277340233325958, 0.11838676780462265, 0.011637425981462002, 0.034968774765729904]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5153334736824036, 0.48466652631759644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7277762293815613, 0.17773443460464478, 0.09448932856321335, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6204738020896912, 0.34140193462371826, 0.03238543123006821, 0.005738734267652035, 0.0, 0.0, 0.0, 0.0], [0.4689798653125763, 0.23054486513137817, 0.09315269440412521, 0.08778934925794601, 0.11953318864107132, 0.0, 0.0, 0.0], [0.46411558985710144, 0.10525748133659363, 0.10445167124271393, 0.11601465195417404, 0.09893779456615448, 0.11122279614210129, 0.0, 0.0], [0.3483860492706299, 0.47716572880744934, 0.041159771382808685, 0.007542312145233154, 0.06106170266866684, 0.048931632190942764, 0.015752799808979034, 0.0], [0.4999969005584717, 0.23254360258579254, 0.05642453208565712, 0.04083625599741936, 0.06395937502384186, 0.04858674481511116, 0.015375735238194466, 0.04227689653635025]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7875546216964722, 0.2124454379081726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6985920071601868, 0.23708517849445343, 0.06432285904884338, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7751134634017944, 0.08879856765270233, 0.03343692794442177, 0.10265101492404938, 0.0, 0.0, 0.0, 0.0], [0.5749664306640625, 0.15825822949409485, 0.047981198877096176, 0.1539348065853119, 0.0648593083024025, 0.0, 0.0, 0.0], [0.5192074775695801, 0.15767695009708405, 0.05403154343366623, 0.14143997430801392, 0.06980367749929428, 0.05784033238887787, 0.0, 0.0], [0.553517758846283, 0.09862464666366577, 0.03991466760635376, 0.09739654511213303, 0.05058111995458603, 0.04938185214996338, 0.11058341711759567, 0.0], [0.5997029542922974, 0.0783306285738945, 0.02688070386648178, 0.08110373467206955, 0.03543694317340851, 0.03264598175883293, 0.08837106078863144, 0.05752792954444885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5559275150299072, 0.4440724849700928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5020520091056824, 0.3347516655921936, 0.1631963700056076, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2631402313709259, 0.1510179340839386, 0.1570226550102234, 0.4288192391395569, 0.0, 0.0, 0.0, 0.0], [0.21635931730270386, 0.2380710393190384, 0.1072242334485054, 0.26244863867759705, 0.1758967787027359, 0.0, 0.0, 0.0], [0.23823243379592896, 0.14342224597930908, 0.07418720424175262, 0.3356230854988098, 0.09965615719556808, 0.10887882858514786, 0.0, 0.0], [0.08942774683237076, 0.09483572840690613, 0.09474087506532669, 0.21871639788150787, 0.06703317165374756, 0.05652355030179024, 0.37872254848480225, 0.0], [0.16287541389465332, 0.14410202205181122, 0.08103086799383163, 0.1736167073249817, 0.06784798204898834, 0.03523683920502663, 0.20902015268802643, 0.12627004086971283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9556117057800293, 0.044388268142938614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8824322819709778, 0.06494761258363724, 0.05262008681893349, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5289629101753235, 0.23288115859031677, 0.08341290056705475, 0.154743030667305, 0.0, 0.0, 0.0, 0.0], [0.7283562421798706, 0.0763198658823967, 0.09662087261676788, 0.05455225706100464, 0.044150736182928085, 0.0, 0.0, 0.0], [0.6971502304077148, 0.04060911014676094, 0.1219663918018341, 0.02008719928562641, 0.07770012319087982, 0.04248705506324768, 0.0, 0.0], [0.4605134129524231, 0.1070990189909935, 0.0672563835978508, 0.09632489085197449, 0.09250470250844955, 0.06466221064329147, 0.11163939535617828, 0.0], [0.5116550922393799, 0.058286748826503754, 0.1094117984175682, 0.039723191410303116, 0.09601335972547531, 0.09307696670293808, 0.0340423509478569, 0.05779053643345833]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9424622058868408, 0.05753784254193306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8957748413085938, 0.045318443328142166, 0.05890665575861931, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49737071990966797, 0.27173087000846863, 0.16503697633743286, 0.06586137413978577, 0.0, 0.0, 0.0, 0.0], [0.49038469791412354, 0.18635913729667664, 0.13104724884033203, 0.046171993017196655, 0.14603693783283234, 0.0, 0.0, 0.0], [0.5898847579956055, 0.06566531211137772, 0.18509316444396973, 0.010940325446426868, 0.11199451237916946, 0.036421988159418106, 0.0, 0.0], [0.2668997645378113, 0.19334407150745392, 0.1349547654390335, 0.037933651357889175, 0.2535625696182251, 0.07172782719135284, 0.041577402502298355, 0.0], [0.474330335855484, 0.09350273758172989, 0.13422918319702148, 0.02968323789536953, 0.1418696939945221, 0.054912958294153214, 0.019764494150877, 0.051707297563552856]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4422158896923065, 0.5577840209007263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.302198588848114, 0.5314541459083557, 0.16634725034236908, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18059194087982178, 0.1881832629442215, 0.30277109146118164, 0.3284536600112915, 0.0, 0.0, 0.0, 0.0], [0.16039727628231049, 0.38873016834259033, 0.1137324869632721, 0.2183031141757965, 0.11883699148893356, 0.0, 0.0, 0.0], [0.16995379328727722, 0.3348008096218109, 0.09975822269916534, 0.1964741200208664, 0.0951322540640831, 0.10388077795505524, 0.0, 0.0], [0.12978504598140717, 0.1712699979543686, 0.13108226656913757, 0.14936323463916779, 0.10999973863363266, 0.1316450834274292, 0.1768546849489212, 0.0], [0.08198712021112442, 0.1652248054742813, 0.07211419939994812, 0.1745789349079132, 0.08105044066905975, 0.08654526621103287, 0.19411517679691315, 0.14438405632972717]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7951699495315552, 0.2048300802707672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7690131664276123, 0.16853488981723785, 0.06245189905166626, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6489514112472534, 0.2295161783695221, 0.061656445264816284, 0.059875939041376114, 0.0, 0.0, 0.0, 0.0], [0.5603657364845276, 0.1681664139032364, 0.07242589443922043, 0.05580994859337807, 0.14323198795318604, 0.0, 0.0, 0.0], [0.5134336948394775, 0.15964625775814056, 0.07599590718746185, 0.07576733827590942, 0.1534004509449005, 0.021756375208497047, 0.0, 0.0], [0.6214274168014526, 0.12032684683799744, 0.03593801334500313, 0.014758625067770481, 0.11938977241516113, 0.006654806435108185, 0.08150442689657211, 0.0], [0.4776436388492584, 0.14701004326343536, 0.040456850081682205, 0.03406889736652374, 0.09344235807657242, 0.011912773363292217, 0.08573724329471588, 0.10972817987203598]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8117632269859314, 0.1882367879152298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6272330284118652, 0.23586148023605347, 0.1369055062532425, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6167694926261902, 0.15472058951854706, 0.09822410345077515, 0.1302858144044876, 0.0, 0.0, 0.0, 0.0], [0.44405028223991394, 0.1911592334508896, 0.107844777405262, 0.16373571753501892, 0.09321002662181854, 0.0, 0.0, 0.0], [0.43546193838119507, 0.16819731891155243, 0.09068924933671951, 0.15097278356552124, 0.08567669987678528, 0.06900198757648468, 0.0, 0.0], [0.4528263509273529, 0.11789862811565399, 0.0790751725435257, 0.10672765970230103, 0.06775210797786713, 0.05920965597033501, 0.11651045083999634, 0.0], [0.43868499994277954, 0.11521147936582565, 0.06542196124792099, 0.09925501048564911, 0.05553722381591797, 0.03987722098827362, 0.09038183838129044, 0.09563025832176208]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9800365567207336, 0.019963469356298447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9819896221160889, 0.011318891309201717, 0.006691556423902512, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9325822591781616, 0.03841092064976692, 0.016685830429196358, 0.01232087891548872, 0.0, 0.0, 0.0, 0.0], [0.8959062695503235, 0.03776904195547104, 0.013134865090250969, 0.01261922623962164, 0.04057056084275246, 0.0, 0.0, 0.0], [0.8946720361709595, 0.029434017837047577, 0.00999655295163393, 0.009215018711984158, 0.03838447481393814, 0.018297888338565826, 0.0, 0.0], [0.8635208606719971, 0.030788559466600418, 0.016335131600499153, 0.005855129100382328, 0.034426916390657425, 0.02900351770222187, 0.020069917663931847, 0.0], [0.8666788339614868, 0.02968038059771061, 0.01450237724930048, 0.007240944541990757, 0.031918223947286606, 0.023600593209266663, 0.01560974307358265, 0.01076893974095583]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6386268138885498, 0.3613731563091278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5126052498817444, 0.33837395906448364, 0.1490207314491272, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5277552008628845, 0.2114867866039276, 0.11076873540878296, 0.14998924732208252, 0.0, 0.0, 0.0, 0.0], [0.493940144777298, 0.2052289843559265, 0.09224750101566315, 0.13663148880004883, 0.0719519555568695, 0.0, 0.0, 0.0], [0.5400253534317017, 0.17407847940921783, 0.06597213447093964, 0.11179240792989731, 0.053860168904066086, 0.054271481931209564, 0.0, 0.0], [0.3991376757621765, 0.15804502367973328, 0.08250337839126587, 0.12343066185712814, 0.055111486464738846, 0.05726122111082077, 0.1245105117559433, 0.0], [0.419831246137619, 0.15665486454963684, 0.06555961072444916, 0.10120981931686401, 0.05130321532487869, 0.04972606152296066, 0.11025803536176682, 0.04545705020427704]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x2b8233757c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Want to visualize the attention patterns in the first layerprint(\"Cache keys:\", patching_result_physics[-1].keys())\n",
    "print(\"Cache keys:\", patching_result_physics[-1].keys())\n",
    "print(\"Attention pattern shape:\", patching_result_physics[-1][\"pattern\", 0].shape)\n",
    "print(patching_result_physics[-1])\n",
    "attn = patching_result_physics[-1][\"pattern\", 0]\n",
    "print(\"min:\", attn.min().item(), \"max:\", attn.max().item(), \"mean:\", attn.mean().item())\n",
    "\n",
    "vis_attn_patterns(model, clean_physics_prompt, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6795e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' remainder': 0.1201\n",
      "' same': 0.1110\n",
      "' sum': 0.0781\n",
      "' result': 0.0380\n",
      "' number': 0.0373\n",
      "' square': 0.0240\n",
      "' whole': 0.0235\n",
      "' next': 0.0197\n",
      "' product': 0.0188\n",
      "' fraction': 0.0173\n"
     ]
    }
   ],
   "source": [
    "# What about the probability distribution now?\n",
    "\n",
    "patching_logits = patching_result_physics[1][0, -1, :]\n",
    "# Get the logits for the last position\n",
    "logits = patching_result_physics[1][0, -1, :]  # shape: (vocab_size,)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = logits.softmax(dim=-1)\n",
    "\n",
    "# Get the top 10 token indices and their probabilities\n",
    "top_probs, top_indices = probs.topk(10)\n",
    "\n",
    "# Decode the tokens to strings\n",
    "top_tokens = [model.to_string([idx.item()]) for idx in top_indices]\n",
    "\n",
    "# Print the results\n",
    "for token, prob in zip(top_tokens, top_probs):\n",
    "    print(f\"{token!r}: {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d409d45",
   "metadata": {},
   "source": [
    "This seems to work!!!\n",
    "\n",
    "But can we make a function that does not require the questions to be the same length? And where the answers can be multiple words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dca7933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run with hooks (this is where we write to the `induction_score_store` tensor`)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model.run_with_hooks(question_token, \n\u001b[32m     35\u001b[39m     return_type=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# For efficiency, we don't need to calculate the logits\u001b[39;00m\n\u001b[32m     36\u001b[39m     fwd_hooks=[(\n\u001b[32m     37\u001b[39m         pattern_hook_names_filter,\n\u001b[32m     38\u001b[39m         induction_score_hook\n\u001b[32m     39\u001b[39m     )]\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#print(induction_score_store)\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get the global max value and its flat index\u001b[39;00m\n\u001b[32m     43\u001b[39m max_value = np.max(induction_score_store)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hooked_model.forward(*model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = block(\n\u001b[32m    613\u001b[39m         residual,\n\u001b[32m    614\u001b[39m         \u001b[38;5;66;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;00m\n\u001b[32m    615\u001b[39m         \u001b[38;5;66;03m# block\u001b[39;00m\n\u001b[32m    616\u001b[39m         past_kv_cache_entry=past_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    617\u001b[39m         shortformer_pos_embed=shortformer_pos_embed,\n\u001b[32m    618\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    619\u001b[39m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    161\u001b[39m         query_input=\u001b[38;5;28mself\u001b[39m.ln1(query_input)\n\u001b[32m    162\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    163\u001b[39m         key_input=\u001b[38;5;28mself\u001b[39m.ln1(key_input)\n\u001b[32m    164\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    165\u001b[39m         value_input=\u001b[38;5;28mself\u001b[39m.ln1(value_input),\n\u001b[32m    166\u001b[39m         past_kv_cache_entry=past_kv_cache_entry,\n\u001b[32m    167\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\abstract_attention.py:261\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    259\u001b[39m pattern = F.softmax(attn_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    260\u001b[39m pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m pattern = \u001b[38;5;28mself\u001b[39m.hook_pattern(pattern)  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n\u001b[32m    263\u001b[39m pattern = pattern.to(v.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1818\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1816\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1821\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:109\u001b[39m, in \u001b[36mHookPoint.add_hook.<locals>.full_hook\u001b[39m\u001b[34m(module, module_input, module_output)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mdir\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mbwd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[32m    108\u001b[39m     module_output = module_output[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hook(module_output, hook=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36minduction_score_hook\u001b[39m\u001b[34m(activation_pattern, hook)\u001b[39m\n\u001b[32m     26\u001b[39m induction_score = einops.reduce(induction_stripe, \u001b[33m\"\u001b[39m\u001b[33mbatch head_index position -> head_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Store the result.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m induction_score_store[hook.layer(), :] = induction_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\_tensor.py:1227\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "induction = []\n",
    "dict_questions = {}\n",
    "for question, question_token in zip(questions, question_embeds):\n",
    "    length = len(question_token)\n",
    "    \n",
    "    induction_score_store = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    # A function for the average induction score\n",
    "    def induction_score_hook(activation_pattern, hook):\n",
    "        \"\"\"\n",
    "        Computes the average induction score for a given activation pattern.\n",
    "        \n",
    "        Args:\n",
    "            activation_pattern (torch.Tensor): The activation pattern to compute the induction score for.\n",
    "            hook (HookPoint): The hook point that triggered this function.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The average induction score.\n",
    "        \"\"\"\n",
    "        # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
    "        # (This only has entries for tokens with index>=seq_len)\n",
    "        induction_stripe = activation_pattern.diagonal(dim1=-2, dim2=-1, offset=1-length//2)\n",
    "        # Get an average score per head\n",
    "        induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
    "        # Store the result.\n",
    "        induction_score_store[hook.layer(), :] = induction_score\n",
    "\n",
    "    # We make a boolean filter on activation names, that's true only on attention pattern names.\n",
    "    pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "    print(question)\n",
    "    # Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "    model.run_with_hooks(question_token, \n",
    "        return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[(\n",
    "            pattern_hook_names_filter,\n",
    "            induction_score_hook\n",
    "        )]\n",
    "    )\n",
    "    #print(induction_score_store)\n",
    "    # Get the global max value and its flat index\n",
    "    max_value = np.max(induction_score_store)\n",
    "    max_index = np.argmax(induction_score_store)\n",
    "\n",
    "    # Convert flat index to (layer, head) coordinates\n",
    "    #max_index = np.unravel_index(max_index_flat.cpu().numpy(), induction_score_store.shape)\n",
    "    #print(\"Question:\", question)\n",
    "    #print(\"Max value:\", max_value.item())\n",
    "    #print(\"Max index (layer, head):\", max_index)\n",
    "    '''\n",
    "    if (max_index[0], max_index[1]) not in dict_questions:\n",
    "        dict_questions[(max_index[0], max_index[1])] = [question]\n",
    "    else:\n",
    "        dict_questions[(max_index[0], max_index[1])].append(question)\n",
    "    '''\n",
    "    print(max_index[0])\n",
    "    x.append(max_index[0])\n",
    "    y.append(max_index[1])\n",
    "    print(x)\n",
    "    induction.append(induction_score_store)\n",
    " \n",
    "print(x, y)\n",
    "df = pd.DataFrame({\n",
    "    \"Layer\": x,\n",
    "    \"Head\": y,\n",
    "    \"Question\": questions[:len(x)]\n",
    "})\n",
    "#print(dict_questions[(int(x[0]), int(y[0]))])\n",
    "px.scatter(\n",
    "    data_frame=df,\n",
    "    x=\"Layer\",\n",
    "    y=\"Head\",\n",
    "    hover_data=[\"Question\"],\n",
    "    title=\"Induction Scores for Questions\"\n",
    ").show()\n",
    "#px.scatter(x=np.array(x), y=np.array(y), labels={\"x\": \"Layer\", \"y\": \"Head\"},  title=\"Induction Scores for Questions\", hover_data=[\"question\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019595b5",
   "metadata": {},
   "source": [
    "The problem here is that many of the questions have the same max index. Therefore, it might not be the best method to visualize the inuction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197ac6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m      3\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=\u001b[32m30\u001b[39m, random_state=\u001b[32m42\u001b[39m, learning_rate=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, early_exaggeration=\u001b[32m5.0\u001b[39m, n_iter=\u001b[32m5000\u001b[39m, init=\u001b[33m\"\u001b[39m\u001b[33mpca\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m induction_embeds = tsne.fit_transform(np.array(induction))\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate=\"auto\", early_exaggeration=5.0, n_iter=5000, init=\"pca\")\n",
    "induction_embeds = tsne.fit_transform(np.array(induction))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144ce5e8",
   "metadata": {},
   "source": [
    "# Q&A example with TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdbc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618b204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27be1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attn_patterns(model, text, layers, compact=True):\n",
    "    str_tokens = model.to_str_tokens(text)\n",
    "    logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "    if compact:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "    \n",
    "    else:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_heads(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c254",
   "metadata": {},
   "source": [
    "We set our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdb20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1449c1",
   "metadata": {},
   "source": [
    "We import and embed the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c2de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "['0+0=', '1+2=', '2+4=', '3+6=', '4+8=', '5+10=', '6+12=', '7+14=', '8+16=', '9+18=']\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "'''\n",
    "with open(\"Test.json\",\"r\") as infile: \n",
    "    infile.readline()\n",
    "    lines = infile.readlines()\n",
    "    '''\n",
    "\n",
    "with open(\"Test.json\", \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "#data = [json.loads(line.strip(\"\\n\").rstrip(',').rstrip(\"]\")) for line in lines]\n",
    "\n",
    "questions = [line[\"question\"] for line in data]\n",
    "answers = [line[\"rationale\"] for line in data]\n",
    "\n",
    "#Embedding the questions and answers\n",
    "question_embeds = model.to_tokens(questions)\n",
    "answer_embeds = model.to_tokens(answers)\n",
    "\n",
    "str_questions = model.to_str_tokens(questions)\n",
    "str_answers = model.to_str_tokens(answers)\n",
    "\n",
    "print(len(question_embeds), len(answer_embeds))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829baee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d5f6dd60-1730\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d5f6dd60-1730\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"0\", \"+\", \"0\", \"=\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0], [0.9679885506629944, 0.032011520117521286, 0.0, 0.0, 0.0], [0.9242812991142273, 0.04485804960131645, 0.03086068294942379, 0.0, 0.0], [0.9142372608184814, 0.03334973752498627, 0.02892901934683323, 0.023483993485569954, 0.0], [0.8983451724052429, 0.023011548444628716, 0.017407618463039398, 0.013762950897216797, 0.047472696751356125]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.5575425624847412, 0.4424574375152588, 0.0, 0.0, 0.0], [0.43405967950820923, 0.3435291051864624, 0.22241118550300598, 0.0, 0.0], [0.26991522312164307, 0.24699647724628448, 0.23221957683563232, 0.2508687674999237, 0.0], [0.19817976653575897, 0.19469134509563446, 0.17008976638317108, 0.19748732447624207, 0.23955181241035461]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7374756336212158, 0.26252439618110657, 0.0, 0.0, 0.0], [0.6801552772521973, 0.200013667345047, 0.11983105540275574, 0.0, 0.0], [0.5294799208641052, 0.17609652876853943, 0.10863009840250015, 0.1857934296131134, 0.0], [0.45543861389160156, 0.15119433403015137, 0.08256997168064117, 0.15762761235237122, 0.1531694531440735]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8184526562690735, 0.18154728412628174, 0.0, 0.0, 0.0], [0.7201865315437317, 0.14093716442584991, 0.13887625932693481, 0.0, 0.0], [0.6287482976913452, 0.1478918194770813, 0.12165702134370804, 0.10170282423496246, 0.0], [0.6375045776367188, 0.10604211688041687, 0.08989257365465164, 0.07828791439533234, 0.08827277272939682]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9486026763916016, 0.051397379487752914, 0.0, 0.0, 0.0], [0.8189542293548584, 0.07578674703836441, 0.1052590012550354, 0.0, 0.0], [0.8033092021942139, 0.08670332282781601, 0.06329502910375595, 0.04669241979718208, 0.0], [0.7310044765472412, 0.05034106597304344, 0.05289936438202858, 0.025051692500710487, 0.14070333540439606]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9687373638153076, 0.03126261755824089, 0.0, 0.0, 0.0], [0.7733528017997742, 0.14079029858112335, 0.0858568623661995, 0.0, 0.0], [0.7362372279167175, 0.09316854178905487, 0.1361006796360016, 0.03449363261461258, 0.0], [0.5743207335472107, 0.10125430673360825, 0.09989191591739655, 0.04796849191188812, 0.17656458914279938]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9644237756729126, 0.035576220601797104, 0.0, 0.0, 0.0], [0.9060759544372559, 0.025965513661503792, 0.06795859336853027, 0.0, 0.0], [0.8188088536262512, 0.041883643716573715, 0.1139979362487793, 0.025309622287750244, 0.0], [0.7125449776649475, 0.05401250347495079, 0.15687033534049988, 0.03636031225323677, 0.04021187499165535]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8311226963996887, 0.16887731850147247, 0.0, 0.0, 0.0], [0.7214011549949646, 0.15281236171722412, 0.1257864236831665, 0.0, 0.0], [0.6923720836639404, 0.11789779365062714, 0.09342531114816666, 0.09630478173494339, 0.0], [0.5636921525001526, 0.12508530914783478, 0.10188793390989304, 0.10211201012134552, 0.10722259432077408]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.4691430628299713, 0.5308569669723511, 0.0, 0.0, 0.0], [0.44394955039024353, 0.32868507504463196, 0.2273653745651245, 0.0, 0.0], [0.22157056629657745, 0.3038328289985657, 0.17494072020053864, 0.299655944108963, 0.0], [0.29178908467292786, 0.2292303442955017, 0.15643657743930817, 0.2168990969657898, 0.10564488172531128]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9494742751121521, 0.0505257323384285, 0.0, 0.0, 0.0], [0.8214703798294067, 0.12878963351249695, 0.049740009009838104, 0.0, 0.0], [0.7425975799560547, 0.12477888911962509, 0.0627165287733078, 0.06990706920623779, 0.0], [0.6491406559944153, 0.09337957948446274, 0.09634716063737869, 0.06497418880462646, 0.09615842998027802]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9141035079956055, 0.08589649945497513, 0.0, 0.0, 0.0], [0.6405526995658875, 0.2401433140039444, 0.11930397152900696, 0.0, 0.0], [0.5427948832511902, 0.1959298700094223, 0.14416436851024628, 0.11711094528436661, 0.0], [0.395060271024704, 0.24366699159145355, 0.13121289014816284, 0.18668924272060394, 0.043370574712753296]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.4950873553752899, 0.5049126744270325, 0.0, 0.0, 0.0], [0.3592243790626526, 0.31089314818382263, 0.3298824727535248, 0.0, 0.0], [0.2094457894563675, 0.26733314990997314, 0.2847387194633484, 0.2384822815656662, 0.0], [0.14989854395389557, 0.2025127410888672, 0.2407248169183731, 0.18144698441028595, 0.22541692852973938]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9787673950195312, 0.021232588216662407, 0.0, 0.0, 0.0], [0.9306658506393433, 0.021140605211257935, 0.04819349944591522, 0.0, 0.0], [0.8990907073020935, 0.029185596853494644, 0.05559246987104416, 0.016131196171045303, 0.0], [0.868726372718811, 0.013166549615561962, 0.06545431911945343, 0.005755038466304541, 0.046897634863853455]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8606029152870178, 0.13939709961414337, 0.0, 0.0, 0.0], [0.6246770024299622, 0.1533670276403427, 0.22195598483085632, 0.0, 0.0], [0.5591806173324585, 0.13532227277755737, 0.19816133379936218, 0.10733586549758911, 0.0], [0.42957809567451477, 0.12460332363843918, 0.17471151053905487, 0.09823152422904968, 0.17287547886371613]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.927640438079834, 0.07235953211784363, 0.0, 0.0, 0.0], [0.7966118454933167, 0.12487640231847763, 0.0785117968916893, 0.0, 0.0], [0.7762333750724792, 0.09571566432714462, 0.0480230413377285, 0.08002793043851852, 0.0], [0.705571711063385, 0.08759500831365585, 0.07961777597665787, 0.07727191597223282, 0.04994355887174606]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7750127911567688, 0.2249872088432312, 0.0, 0.0, 0.0], [0.6136274337768555, 0.19068314135074615, 0.19568945467472076, 0.0, 0.0], [0.6201993227005005, 0.13456887006759644, 0.13172101974487305, 0.11351076513528824, 0.0], [0.4713764488697052, 0.12501142919063568, 0.13058751821517944, 0.10657698661088943, 0.16644757986068726]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x292b1784920>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7cc49882-5cb5\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7cc49882-5cb5\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"1\", \"+\", \"2\", \"=\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0], [0.9684523344039917, 0.031547706574201584, 0.0, 0.0, 0.0], [0.9282559156417847, 0.04075062274932861, 0.030993390828371048, 0.0, 0.0], [0.9112078547477722, 0.03795705735683441, 0.03324998915195465, 0.017585057765245438, 0.0], [0.9053216576576233, 0.02172788605093956, 0.017542803660035133, 0.007566225714981556, 0.04784136638045311]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.5394425392150879, 0.46055740118026733, 0.0, 0.0, 0.0], [0.42234981060028076, 0.36123910546302795, 0.2164110690355301, 0.0, 0.0], [0.26339590549468994, 0.25971511006355286, 0.1757339984178543, 0.3011550009250641, 0.0], [0.19888339936733246, 0.19847607612609863, 0.17069366574287415, 0.19154448807239532, 0.24040234088897705]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.43506473302841187, 0.5649353265762329, 0.0, 0.0, 0.0], [0.41506293416023254, 0.5118104219436646, 0.07312658429145813, 0.0, 0.0], [0.23960593342781067, 0.3211289048194885, 0.05015759542584419, 0.3891076445579529, 0.0], [0.22685347497463226, 0.3135043978691101, 0.0411280132830143, 0.34222060441970825, 0.07629353553056717]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7427689433097839, 0.25723111629486084, 0.0, 0.0, 0.0], [0.67123943567276, 0.19932293891906738, 0.1294376105070114, 0.0, 0.0], [0.5741448998451233, 0.20033544301986694, 0.09888695180416107, 0.12663277983665466, 0.0], [0.5751430988311768, 0.15831409394741058, 0.08109917491674423, 0.10580575466156006, 0.07963782548904419]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9332805275917053, 0.06671952456235886, 0.0, 0.0, 0.0], [0.7712058424949646, 0.12967215478420258, 0.09912196546792984, 0.0, 0.0], [0.7425117492675781, 0.14106188714504242, 0.049918126314878464, 0.0665082260966301, 0.0], [0.6954461932182312, 0.0866435095667839, 0.05032617598772049, 0.0337250716984272, 0.13385909795761108]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9711823463439941, 0.028817687183618546, 0.0, 0.0, 0.0], [0.7912516593933105, 0.1209043562412262, 0.08784397691488266, 0.0, 0.0], [0.7066357731819153, 0.1061134785413742, 0.14813682436943054, 0.03911396116018295, 0.0], [0.589984655380249, 0.08648785203695297, 0.10261634737253189, 0.03953102231025696, 0.18138018250465393]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9441772699356079, 0.055822715163230896, 0.0, 0.0, 0.0], [0.8929442167282104, 0.04008220508694649, 0.06697367131710052, 0.0, 0.0], [0.7432310581207275, 0.07365415245294571, 0.13546176254749298, 0.04765298217535019, 0.0], [0.6845705509185791, 0.07754633575677872, 0.15071162581443787, 0.04853829741477966, 0.038633160293102264]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7939220070838928, 0.20607797801494598, 0.0, 0.0, 0.0], [0.694524884223938, 0.18437491357326508, 0.12110016494989395, 0.0, 0.0], [0.7116798758506775, 0.11523319780826569, 0.07509160041809082, 0.09799525141716003, 0.0], [0.5286424160003662, 0.15110552310943604, 0.09555266052484512, 0.12414368242025375, 0.10055562108755112]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.43231305480003357, 0.5676868557929993, 0.0, 0.0, 0.0], [0.44542282819747925, 0.3264572322368622, 0.228119894862175, 0.0, 0.0], [0.23455457389354706, 0.3051563501358032, 0.15833109617233276, 0.30195796489715576, 0.0], [0.2907950282096863, 0.22667071223258972, 0.155903622508049, 0.22134561836719513, 0.10528497397899628]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9608451724052429, 0.039154838770627975, 0.0, 0.0, 0.0], [0.8570511341094971, 0.09105449914932251, 0.0518944226205349, 0.0, 0.0], [0.7010040283203125, 0.13800661265850067, 0.09065692126750946, 0.07033237814903259, 0.0], [0.6990773677825928, 0.06117596849799156, 0.10375890135765076, 0.03243213891983032, 0.10355564951896667]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9438822269439697, 0.056117795407772064, 0.0, 0.0, 0.0], [0.6692742705345154, 0.20607233047485352, 0.1246534138917923, 0.0, 0.0], [0.5636351704597473, 0.17819711565971375, 0.16260264813899994, 0.09556505084037781, 0.0], [0.39953750371932983, 0.25611117482185364, 0.13269993662834167, 0.16778920590877533, 0.04386209696531296]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.4922903776168823, 0.5077096223831177, 0.0, 0.0, 0.0], [0.33913105726242065, 0.34943848848342896, 0.3114303946495056, 0.0, 0.0], [0.2161860316991806, 0.2552684545516968, 0.28065869212150574, 0.24788682162761688, 0.0], [0.13625368475914001, 0.22347904741764069, 0.21881228685379028, 0.21655714511871338, 0.20489783585071564]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.9716506600379944, 0.02834935486316681, 0.0, 0.0, 0.0], [0.9265936613082886, 0.025423718616366386, 0.04798262193799019, 0.0, 0.0], [0.8741143345832825, 0.04475601390004158, 0.06022845581173897, 0.020901110023260117, 0.0], [0.8661559820175171, 0.015274256467819214, 0.06526065617799759, 0.006550201680511236, 0.046758875250816345]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.8352201581001282, 0.16477984189987183, 0.0, 0.0, 0.0], [0.6007806658744812, 0.18575404584407806, 0.21346530318260193, 0.0, 0.0], [0.530565619468689, 0.15800240635871887, 0.1831417828798294, 0.12829017639160156, 0.0], [0.4058133065700531, 0.1493576020002365, 0.16504624485969543, 0.11647112667560577, 0.16331179440021515]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.948870062828064, 0.05112992227077484, 0.0, 0.0, 0.0], [0.8525082468986511, 0.06347101926803589, 0.08402079343795776, 0.0, 0.0], [0.8354392647743225, 0.06950818747282028, 0.050354428589344025, 0.044698067009449005, 0.0], [0.7938336730003357, 0.03956783190369606, 0.08957739174365997, 0.02082992158830166, 0.05619113892316818]], [[1.0, 0.0, 0.0, 0.0, 0.0], [0.7183544039726257, 0.28164559602737427, 0.0, 0.0, 0.0], [0.5778692364692688, 0.23784488439559937, 0.1842859536409378, 0.0, 0.0], [0.5846219062805176, 0.16938376426696777, 0.11686703562736511, 0.12912727892398834, 0.0], [0.444057434797287, 0.1550976037979126, 0.12301921099424362, 0.12102483212947845, 0.15680097043514252]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x292b1761a60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_attn_patterns(model, questions[0], layers=[0])\n",
    "vis_attn_patterns(model, questions[1], layers=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9d17c",
   "metadata": {},
   "source": [
    "Trying activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc528f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean answer logit difference: 3.1147\n",
      "Corrupted answer logit difference: -4.0909\n"
     ]
    }
   ],
   "source": [
    "clean_prompt = \"What is the capital of France?\"\n",
    "corrupted_prompt = \"What is the capital of England?\"\n",
    "\n",
    "clean_answer = \"Paris\"\n",
    "corrupted_answer = \"London\"\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "corrupted_logits = model(corrupted_prompt)\n",
    "\n",
    "clean_index = model.to_single_token(clean_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "print(f\"Clean answer logit difference: {clean_diff:.4f}\")\n",
    "\n",
    "corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "print(f\"Corrupted answer logit difference: {corrupted_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383d40d",
   "metadata": {},
   "source": [
    "Then we want to patch the clean prompt onto the corrupted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54862200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(resid_pre, hook, position, clean_cache):\n",
    "    clean_activation = clean_cache[hook.name]\n",
    "    resid_pre[:, position, :] = clean_activation[:, position, :]\n",
    "    return resid_pre\n",
    "\n",
    "def model_data(model, prompt):\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    return tokens, logits, cache\n",
    "\n",
    "#num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "#patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "\n",
    "def activation_patching(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer):\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "    corrupted_logits = model(corrupted_prompt)\n",
    "\n",
    "    clean_index = model.to_single_token(clean_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "    clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "    corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "\n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "    num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "   \n",
    "    patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "    for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(num_positions):\n",
    "            # We use a temporary hook with functool.partial to patch at each position\n",
    "            temp_hook = partial(activation_patching_hook, position=position, clean_cache=clean_cache)\n",
    "            # We then run the model with hooks as usual\n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, \n",
    "                                                  fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), temp_hook)])\n",
    "            # We then calculate the logit difference\n",
    "            patched_diff = (patched_logits[0, -1, clean_index] - patched_logits[0, -1, corrupted_index]).detach()\n",
    "            # We then store the result in the patching_result tensor, normalizing it\n",
    "            patching_result[layer, position] = (patched_diff - corrupted_diff) / (clean_diff - corrupted_diff)\n",
    "\n",
    "    return patching_result, patched_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53bd1c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "Looking for;  blocks.0.hook_activation\n",
      "Number of positions: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6981a2cc1b48459e726b1456d365c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Position 0\n",
      "Layer 0, Position 1\n",
      "Layer 0, Position 2\n",
      "Layer 0, Position 3\n",
      "Layer 0, Position 4\n",
      "Layer 0, Position 5\n",
      "Layer 0, Position 6\n",
      "Layer 0, Position 7\n",
      "Layer 1, Position 0\n",
      "Layer 1, Position 1\n",
      "Layer 1, Position 2\n",
      "Layer 1, Position 3\n",
      "Layer 1, Position 4\n",
      "Layer 1, Position 5\n",
      "Layer 1, Position 6\n",
      "Layer 1, Position 7\n",
      "Layer 2, Position 0\n",
      "Layer 2, Position 1\n",
      "Layer 2, Position 2\n",
      "Layer 2, Position 3\n",
      "Layer 2, Position 4\n",
      "Layer 2, Position 5\n",
      "Layer 2, Position 6\n",
      "Layer 2, Position 7\n",
      "Layer 3, Position 0\n",
      "Layer 3, Position 1\n",
      "Layer 3, Position 2\n",
      "Layer 3, Position 3\n",
      "Layer 3, Position 4\n",
      "Layer 3, Position 5\n",
      "Layer 3, Position 6\n",
      "Layer 3, Position 7\n",
      "Layer 4, Position 0\n",
      "Layer 4, Position 1\n",
      "Layer 4, Position 2\n",
      "Layer 4, Position 3\n",
      "Layer 4, Position 4\n",
      "Layer 4, Position 5\n",
      "Layer 4, Position 6\n",
      "Layer 4, Position 7\n",
      "Layer 5, Position 0\n",
      "Layer 5, Position 1\n",
      "Layer 5, Position 2\n",
      "Layer 5, Position 3\n",
      "Layer 5, Position 4\n",
      "Layer 5, Position 5\n",
      "Layer 5, Position 6\n",
      "Layer 5, Position 7\n",
      "Layer 6, Position 0\n",
      "Layer 6, Position 1\n",
      "Layer 6, Position 2\n",
      "Layer 6, Position 3\n",
      "Layer 6, Position 4\n",
      "Layer 6, Position 5\n",
      "Layer 6, Position 6\n",
      "Layer 6, Position 7\n",
      "Layer 7, Position 0\n",
      "Layer 7, Position 1\n",
      "Layer 7, Position 2\n",
      "Layer 7, Position 3\n",
      "Layer 7, Position 4\n",
      "Layer 7, Position 5\n",
      "Layer 7, Position 6\n",
      "Layer 7, Position 7\n",
      "Layer 8, Position 0\n",
      "Layer 8, Position 1\n",
      "Layer 8, Position 2\n",
      "Layer 8, Position 3\n",
      "Layer 8, Position 4\n",
      "Layer 8, Position 5\n",
      "Layer 8, Position 6\n",
      "Layer 8, Position 7\n",
      "Layer 9, Position 0\n",
      "Layer 9, Position 1\n",
      "Layer 9, Position 2\n",
      "Layer 9, Position 3\n",
      "Layer 9, Position 4\n",
      "Layer 9, Position 5\n",
      "Layer 9, Position 6\n",
      "Layer 9, Position 7\n",
      "Layer 10, Position 0\n",
      "Layer 10, Position 1\n",
      "Layer 10, Position 2\n",
      "Layer 10, Position 3\n",
      "Layer 10, Position 4\n",
      "Layer 10, Position 5\n",
      "Layer 10, Position 6\n",
      "Layer 10, Position 7\n",
      "Layer 11, Position 0\n",
      "Layer 11, Position 1\n",
      "Layer 11, Position 2\n",
      "Layer 11, Position 3\n",
      "Layer 11, Position 4\n",
      "Layer 11, Position 5\n",
      "Layer 11, Position 6\n",
      "Layer 11, Position 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(clean_cache.keys()))\n",
    "print(\"Looking for; \", utils.get_act_name(\"activation\", 0))\n",
    "patching_result = activation_patching(model, clean_prompt, model.to_tokens(corrupted_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74147f",
   "metadata": {},
   "source": [
    "We then visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c46c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patching_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m token_labels = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model.to_str_tokens(clean_prompt))]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m imshow(patching_result, x=token_labels, xaxis=\u001b[33m\"\u001b[39m\u001b[33mPosition\u001b[39m\u001b[33m\"\u001b[39m, yaxis=\u001b[33m\"\u001b[39m\u001b[33mLayer\u001b[39m\u001b[33m\"\u001b[39m, title=\u001b[33m\"\u001b[39m\u001b[33mPatching Result\u001b[39m\u001b[33m\"\u001b[39m,)\n",
      "\u001b[31mNameError\u001b[39m: name 'patching_result' is not defined"
     ]
    }
   ],
   "source": [
    "token_labels = [f'{token}_{index}' for index, token in enumerate(model.to_str_tokens(clean_prompt))]\n",
    "imshow(patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Patching Result\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75333f",
   "metadata": {},
   "source": [
    "When patching the clean results onto the corrupted results, the change is first very localized, but is then brought to the end in the last few layers. \n",
    "We now want to try with a more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89cd6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_physics_prompt = \"When you drop a ball, it falls to the ground because of\"\n",
    "corrupted_physics_prompt = \"When you throw a ball, it falls to the ground because of\"\n",
    "clean_answer = \" gravity\"\n",
    "corrupted_answer = \" air\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87dc8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9594f01354d42ebbf3f022564bbddd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'When you drop a ball, it falls to the ground because of gravity. When you drop a ball, it falls'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(clean_physics_prompt, max_new_tokens=10, temperature=0.0, top_p=1.0, do_sample=False)\n",
    "#utils.test_prompt(clean_physics_prompt, clean_answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28764a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027c66ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519fd271063c4db48649fc4a870c7363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "<|endoftext|>_0",
          "When_1",
          " you_2",
          " drop_3",
          " a_4",
          " ball_5",
          ",_6",
          " it_7",
          " falls_8",
          " to_9",
          " the_10",
          " ground_11",
          " because_12",
          " of_13"
         ],
         "xaxis": "x",
         "yaxis": "y",
         "z": {
          "bdata": "AAAAAAAAAAAAAAAAAACAPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABdyTw/JN8/O9d8Vj7GnUU7RWYMvErh4TtV+u46VMIEPLYTtrzTV5e87J4FPAAAAAAAAAAAAAAAAAWKKz9s7p88f5hrPojRNTxjnN+8AZoXPOE/gLx6ODY8bvoCvKEi97snTVA8AAAAAAAAAAAAAAAAglkGP3e4w7xDfdg+IQ+CPL3X9byWe0+9fnuhvLzlgzyZFU29R6aFvPliqjsAAAAAAAAAAAAAAACnXoE+/PO2PRGpUD/Q81m8tBk1vYZ2abq2PyO9UaS/Os7mFLrtCaS8p67MPAAAAAAAAAAAAAAAAMN9ib3OPrw9KDdmP7gExjybITA9m7+CO5cIiLsnr/05qswRPamf3LxMKkw8AAAAAAAAAAAAAAAArSZ8vmnzVj4x/2w/o1gdvOBhND01tRI+a+MEPc1gI7znJBI77Bo+vZFJ/jwAAAAAAAAAAAAAAADK8pK9Q/qlPaRfSj8ZVE299Erku5w8gz49Hb08aYCPu6Ml6Tx5N269a+MEPQAAAAAAAAAAAAAAADCogbzCl5Q9X+CUPwI6FL7Pbei8tISgPng+tT09f+q8UARDPepW/byAOGi+AAAAAAAAAAAAAAAAgl5WPj0aZD6BZ1Q/nbkDvdC9mb0cxNQ+jr84Pf3+0bwKwpQ8HWvLPMyejL4AAAAAAAAAAAAAAABAKjU+yrlGPpwBQD9+nh09rVbXvGTPBj+ojXo9JopXPLs+QD3qGXa8vrC+vgAAAAAAAAAAAAAAAJJk0Tx9uwE+LU4XP4fuZj4yksq941AAP4Fdwbut6zg8hwMiPhCaRz6y1q++AAAAAAAAAAAAAAAAcWn1u7r3/z2myCQ/dotBPu52bL1hL+Q+l3wXvHrpzLzA7Em8vB1uPna/Cr4AAAAAAAAAAAAAAAC9kf28evCTPUI3+j6VJk4+i44vvZoVsz4YQME7o6eGOwDqYj0D3Zw+yf4pPgAAAAAAAAAAAAAAAKIhlbtELwQ+bG8bPxGxLD6CbyO8O49vPrmde7x7b768Fa6fvRQYdj51DMo+AAAAAAAAAAAAAAAAbAnAPWtFsj2rJ/g+1vx8Pr5TFLzCapI+TU6QO0Nsi7y3dIG9VRcGPiNlyz4AAAAAAAAAAAAAAABGPMk96FuaPD5Iez7e9Lg+bpm3PVuZiD6Otke9zO31vAQ9Bj1rmR4+nPSTPgAAAAAAAAAAAAAAAFW95z2YUtQ7aq2RPuAWuT66X5I9y0uCPqEraL3qBjK9w3UtPZTrVz6VlVo+AAAAAAAAAAAAAAAA1yQvPjIpVjvevw0+lSPCPqrNcz1bTJY+xv9yvRKzib2bJAk+Iz/DPaD9UD4AAAAAAAAAAAAAAABWIzY+qiXOPBqbDT2UfMs+cuWTvCwSPz6GdYe9XPqGvaSi6T36N4U9W7W9PgAAAAAAAAAAAAAAAPGVRj66r108yq38PLc25T6n9m69MBMgvfRKZL05Z4q98K68PPQuL73amTk/AAAAAAAAAAAAAAAAGOA9Plwf+jsQWD297yTdPge0h72p5Qe+5997vZp/ib1Thw6+zms+vawUhz8AAAAAAAAAAAAAAABAOwI+DYnhu7/Rdr3iJoo9fSqOvRG7sr1dkie9j+1PvSJQXb3etpy7886lPwAAAAAAAAAAAAAAADIwnT17mQG8w82HvAS5Prsl6lq9XrdNvGJ1D73OYk29nRuxvNhwP70/xZs/",
          "dtype": "f4",
          "shape": "24, 14"
         }
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Patching Result"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Position"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patching_result_physics = activation_patching(model, clean_physics_prompt, corrupted_physics_prompt, clean_answer, corrupted_answer)\n",
    "token_labels = [f'{token}_{index}' for index, token in enumerate(model.to_str_tokens(clean_physics_prompt))]\n",
    "imshow(patching_result_physics[0], x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Patching Result\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6795e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' gravity': 0.6126\n",
      "' the': 0.1128\n",
      "' its': 0.0351\n",
      "' friction': 0.0283\n",
      "' a': 0.0258\n",
      "' inertia': 0.0244\n",
      "' two': 0.0168\n",
      "' momentum': 0.0128\n",
      "' gravitational': 0.0125\n",
      "' your': 0.0073\n"
     ]
    }
   ],
   "source": [
    "# What about the probability distribution now?\n",
    "patching_logits = patching_result_physics[1][0, -1, :]\n",
    "# Get the logits for the last position\n",
    "logits = patching_result_physics[1][0, -1, :]  # shape: (vocab_size,)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = logits.softmax(dim=-1)\n",
    "\n",
    "# Get the top 10 token indices and their probabilities\n",
    "top_probs, top_indices = probs.topk(10)\n",
    "\n",
    "# Decode the tokens to strings\n",
    "top_tokens = [model.to_string([idx.item()]) for idx in top_indices]\n",
    "\n",
    "# Print the results\n",
    "for token, prob in zip(top_tokens, top_probs):\n",
    "    print(f\"{token!r}: {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d409d45",
   "metadata": {},
   "source": [
    "This seems to work!!!\n",
    "\n",
    "But can we make a function that does not require the questions to be the same length? And where the answers can be multiple words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dca7933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run with hooks (this is where we write to the `induction_score_store` tensor`)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model.run_with_hooks(question_token, \n\u001b[32m     35\u001b[39m     return_type=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# For efficiency, we don't need to calculate the logits\u001b[39;00m\n\u001b[32m     36\u001b[39m     fwd_hooks=[(\n\u001b[32m     37\u001b[39m         pattern_hook_names_filter,\n\u001b[32m     38\u001b[39m         induction_score_hook\n\u001b[32m     39\u001b[39m     )]\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#print(induction_score_store)\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get the global max value and its flat index\u001b[39;00m\n\u001b[32m     43\u001b[39m max_value = np.max(induction_score_store)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hooked_model.forward(*model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = block(\n\u001b[32m    613\u001b[39m         residual,\n\u001b[32m    614\u001b[39m         \u001b[38;5;66;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;00m\n\u001b[32m    615\u001b[39m         \u001b[38;5;66;03m# block\u001b[39;00m\n\u001b[32m    616\u001b[39m         past_kv_cache_entry=past_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    617\u001b[39m         shortformer_pos_embed=shortformer_pos_embed,\n\u001b[32m    618\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    619\u001b[39m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    161\u001b[39m         query_input=\u001b[38;5;28mself\u001b[39m.ln1(query_input)\n\u001b[32m    162\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    163\u001b[39m         key_input=\u001b[38;5;28mself\u001b[39m.ln1(key_input)\n\u001b[32m    164\u001b[39m         + (\u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[32m    165\u001b[39m         value_input=\u001b[38;5;28mself\u001b[39m.ln1(value_input),\n\u001b[32m    166\u001b[39m         past_kv_cache_entry=past_kv_cache_entry,\n\u001b[32m    167\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\components\\abstract_attention.py:261\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    259\u001b[39m pattern = F.softmax(attn_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    260\u001b[39m pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m pattern = \u001b[38;5;28mself\u001b[39m.hook_pattern(pattern)  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n\u001b[32m    263\u001b[39m pattern = pattern.to(v.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1818\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1816\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1821\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformer_lens\\hook_points.py:109\u001b[39m, in \u001b[36mHookPoint.add_hook.<locals>.full_hook\u001b[39m\u001b[34m(module, module_input, module_output)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mdir\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mbwd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[32m    108\u001b[39m     module_output = module_output[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hook(module_output, hook=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36minduction_score_hook\u001b[39m\u001b[34m(activation_pattern, hook)\u001b[39m\n\u001b[32m     26\u001b[39m induction_score = einops.reduce(induction_stripe, \u001b[33m\"\u001b[39m\u001b[33mbatch head_index position -> head_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Store the result.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m induction_score_store[hook.layer(), :] = induction_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Klara\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\_tensor.py:1227\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "induction = []\n",
    "dict_questions = {}\n",
    "for question, question_token in zip(questions, question_embeds):\n",
    "    length = len(question_token)\n",
    "    \n",
    "    induction_score_store = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "\n",
    "    # A function for the average induction score\n",
    "    def induction_score_hook(activation_pattern, hook):\n",
    "        \"\"\"\n",
    "        Computes the average induction score for a given activation pattern.\n",
    "        \n",
    "        Args:\n",
    "            activation_pattern (torch.Tensor): The activation pattern to compute the induction score for.\n",
    "            hook (HookPoint): The hook point that triggered this function.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The average induction score.\n",
    "        \"\"\"\n",
    "        # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
    "        # (This only has entries for tokens with index>=seq_len)\n",
    "        induction_stripe = activation_pattern.diagonal(dim1=-2, dim2=-1, offset=1-length//2)\n",
    "        # Get an average score per head\n",
    "        induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
    "        # Store the result.\n",
    "        induction_score_store[hook.layer(), :] = induction_score\n",
    "\n",
    "    # We make a boolean filter on activation names, that's true only on attention pattern names.\n",
    "    pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "    print(question)\n",
    "    # Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "    model.run_with_hooks(question_token, \n",
    "        return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[(\n",
    "            pattern_hook_names_filter,\n",
    "            induction_score_hook\n",
    "        )]\n",
    "    )\n",
    "    #print(induction_score_store)\n",
    "    # Get the global max value and its flat index\n",
    "    max_value = np.max(induction_score_store)\n",
    "    max_index = np.argmax(induction_score_store)\n",
    "\n",
    "    # Convert flat index to (layer, head) coordinates\n",
    "    #max_index = np.unravel_index(max_index_flat.cpu().numpy(), induction_score_store.shape)\n",
    "    #print(\"Question:\", question)\n",
    "    #print(\"Max value:\", max_value.item())\n",
    "    #print(\"Max index (layer, head):\", max_index)\n",
    "    '''\n",
    "    if (max_index[0], max_index[1]) not in dict_questions:\n",
    "        dict_questions[(max_index[0], max_index[1])] = [question]\n",
    "    else:\n",
    "        dict_questions[(max_index[0], max_index[1])].append(question)\n",
    "    '''\n",
    "    print(max_index[0])\n",
    "    x.append(max_index[0])\n",
    "    y.append(max_index[1])\n",
    "    print(x)\n",
    "    induction.append(induction_score_store)\n",
    " \n",
    "print(x, y)\n",
    "df = pd.DataFrame({\n",
    "    \"Layer\": x,\n",
    "    \"Head\": y,\n",
    "    \"Question\": questions[:len(x)]\n",
    "})\n",
    "#print(dict_questions[(int(x[0]), int(y[0]))])\n",
    "px.scatter(\n",
    "    data_frame=df,\n",
    "    x=\"Layer\",\n",
    "    y=\"Head\",\n",
    "    hover_data=[\"Question\"],\n",
    "    title=\"Induction Scores for Questions\"\n",
    ").show()\n",
    "#px.scatter(x=np.array(x), y=np.array(y), labels={\"x\": \"Layer\", \"y\": \"Head\"},  title=\"Induction Scores for Questions\", hover_data=[\"question\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019595b5",
   "metadata": {},
   "source": [
    "The problem here is that many of the questions have the same max index. Therefore, it might not be the best method to visualize the inuction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197ac6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m      3\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=\u001b[32m30\u001b[39m, random_state=\u001b[32m42\u001b[39m, learning_rate=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, early_exaggeration=\u001b[32m5.0\u001b[39m, n_iter=\u001b[32m5000\u001b[39m, init=\u001b[33m\"\u001b[39m\u001b[33mpca\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m induction_embeds = tsne.fit_transform(np.array(induction))\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate=\"auto\", early_exaggeration=5.0, n_iter=5000, init=\"pca\")\n",
    "induction_embeds = tsne.fit_transform(np.array(induction))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1902ae6",
   "metadata": {},
   "source": [
    "We define the following:\n",
    "- $G$, the computational graph of the model, that is, a set of nodes $n_G$ (heads in attention an MLP fx) and a set of the connections, ie computations, between them.\n",
    "- $D$, the dataset. This contains all data we want to test the hypothesis on, ie. the relevant sentences.\n",
    "- $h$, the hypothesis. This is a tuple $(G, I, c)$, where $I$ is the hypothesized important graph and $c:I\\to G$ is an injective correspondence function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffeb3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b097ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, layer: int, head: int):\n",
    "        self.layer = layer\n",
    "        self.head = head\n",
    "\n",
    "    def parents(self):\n",
    "        parent_list = [(i, j) for i, j in zip(range(self.layer), range(self.head))]\n",
    "        return [Node(*parent) for parent in parent_list]\n",
    "    \n",
    "        \n",
    "    def value_on(self, x: str) -> torch.tensor:\n",
    "        logits, _ = self.model.run_with_cache(x)\n",
    "        return logits[self.layer, self.head]\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.layer, self.head\n",
    "    \n",
    "    def value_from_inputs(self, inputs):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90697b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    def __init__(self, G: list[tuple[int]], I: list[tuple[int]], c: callable):\n",
    "        self.G = G\n",
    "        self.I = I \n",
    "        self.c = c\n",
    "\n",
    "\n",
    "    def set_domain(self, D: list[str]):\n",
    "        self.D = D\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def c_image(self, set: list[Node]) -> list[Node]:\n",
    "        if isinstance(set, Node):\n",
    "            return self.c(set)\n",
    "    \n",
    "        image = list(map(self.c, set))\n",
    "        return image\n",
    "    \n",
    "    def c_preimage(self, set: Union[list[Node], Node]) -> list[Node]:\n",
    "        preimage = []\n",
    "        for node in self.I:\n",
    "            if isinstance(set, Node):\n",
    "                if self.c(node) == set:\n",
    "                    preimage.append(node)\n",
    "            else:\n",
    "                if self.c(node) in set:\n",
    "                    preimage.append(node)\n",
    "        return preimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507fdf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO What if no matches? approx?\n",
    "def sample_agreeing_x(D: list[str], n_I: Node, ref_x: str) -> str:\n",
    "    '''Returns random sample input that agrees on the specified node.'''\n",
    "    D_agree = [x for x in D if n_I.value_on(ref_x) == n_I.value_on(x)]\n",
    "    return random.choice(D_agree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74660771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6952043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In a hypothesis class?\n",
    "def hypothesis_correspondence(n_I: Node) -> Node:\n",
    "    '''\n",
    "    Return the corresponding node in the main graph.\n",
    "\n",
    "    Param:\n",
    "    n_I (int): A node in the interpretation graph I\n",
    "\n",
    "    Returns:\n",
    "    n_G (int): The correspondin node in the main graph G, according to the hypothesis\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_scrub(h: Hypothesis, D: list[str], n_I: Node, ref_x: str, model):\n",
    "    '''\n",
    "    Return the output after a scrub changing all activation of unimportant nodes.\n",
    "\n",
    "    Param:\n",
    "    c (callable): The hypothesis correspondence\n",
    "    D (list[list]): A list of all possible inputs in the domain.\n",
    "    n_I (tuple[int]): The node in question\n",
    "    ref_x (list): The reference input\n",
    "    '''\n",
    "    h.set_domain(D)\n",
    "    h.set_model(model)\n",
    "\n",
    "    # The corresponding main node\n",
    "    n_G = h.c(n_I)\n",
    "\n",
    "    if n_G in ref_x:\n",
    "        return ref_x\n",
    "    \n",
    "    inputs_G = torch.zeros(model.cfg.n_layers, model.cfg.heads)\n",
    "    \n",
    "    # We pick a ranom sample to use as the exchange values for the unimportant nodes\n",
    "    random_x = random.choice(D)\n",
    "\n",
    "    # Get scrubbed activations for the inputs to n_G\n",
    "    for parent_G in n_G.parents():\n",
    "        # \"important\" parents\n",
    "        if parent_G in h.c_image(n_I.parents):\n",
    "            parent_I = h.c_preimage(parent_G)\n",
    "\n",
    "            # Sample new input that agrees on the interpretation node\n",
    "            new_x = sample_agreeing_x(D, parent_I, ref_x)\n",
    "\n",
    "            # Get the scrubbed activations\n",
    "            inputs_G[*parent_G(), :] = run_scrub(h, D, parent_I, new_x)\n",
    "        \n",
    "        # \"unimportant\" parents\n",
    "        else:\n",
    "            # get activations on the random input value chosen\n",
    "            inputs_G[*parent_G(), :] = parent_G.value_on(random_x)\n",
    "    \n",
    "    # run n_G given the computed input activations\n",
    "    return n_G.value_from_inputs(inputs_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d65c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b3ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
      "torch.Size([1, 2, 50257])\n"
     ]
    }
   ],
   "source": [
    "text = \"hello\"\n",
    "logits, cache = model.run_with_cache(text)\n",
    "print(cache)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchange_activations_attn(out, hook, inputs, node):\n",
    "    for layer in range(model.cfg.n_layer):\n",
    "        if not node:\n",
    "            for position in range(model.cfg.n_heads):\n",
    "                out[layer, position, :] = inputs[layer, position]\n",
    "        else:\n",
    "            for position in range(node.head):\n",
    "                out[layer, position, :] = inputs[layer, position]\n",
    "            return \n",
    "\n",
    "def run_with_changes(model, text, inputs, end_node=False):\n",
    "    tokens = model.to_tokens(text)\n",
    "    temp_hook = partial(exchange_activations_attn, inputs, end_node)\n",
    "    logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "This is based on the Captum BERT-interpretation demo notebook, adapted for multiple choice models, and hopefully more instructive by indicating clearly the expected tensor shapes for the catum methods. Overall, Captum is neither wonderful nor horrible to work with, but I suspect transformer lens / more specific libraries could make for a more enjoyable coding experience.\n",
    "\n",
    "The applications listed are suggestions for further research, and none have been completed satisfactorily.\n",
    "\n",
    "### Application I: Finding Physical Misunderstandings on the FCI of BERT-Style Models Fine-Tuned for Multiple Choice\n",
    "\n",
    "We use Integrated Gradients from Captum to compute word attributions on the FCI for a BERT-model fine-tuned for answering mulitple choice questions. Specifically, we want to see whether the emphasised words indicate that the model exhibits the same common misconceptions as humans. In the example question below, even if the model gives a common wrong answer (eg. that the pushing car exerts a larger force, or that the heavier object exerts a larger force) we can perhaps use attribution to see precisely which words throw the model off, and then modify the prompt to determine by causal experiments what the model's misconception is. There is detailed qualitative research on student misconceptions on the FCI which could aid us greatly in finding likely misconceptions.\n",
    "\n",
    "Alas, I cannot compute the gradients for large models with long inputs on my laptop, and our MpC-models are not trained for long physics questions, so the models used here are not sufficiently good to get at all interesting results with these. Therefore, we use a basic test question instead. Still, the code will hopefully be directly applicable to any appropriate model (potentially self fine-tuned for the purpose) with minimal adjustments, and we show as an example how to (very slighlty) modify it for a slighly different architechture (RoBERTa).\n",
    "\n",
    "---\n",
    "\n",
    "Example FCI-question: *A large truck breaks down out on the road and receives a push back into town by a small compact car. After the car reaches the constant cruising speed at which its driver wishes to push the truck:*\n",
    "\n",
    "0) *The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.*\n",
    "1) *The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.*\n",
    "2) *The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.*\n",
    "3) *The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot push back against the car. The truck is pushed forward simply because it is in the way of the car.*\n",
    "4) *Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b113b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question (replace with some FCI-question)\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Berlin\", \"Madrid\", \"Paris\",] # \"Rome\", \"India\"]\n",
    "ground_truth_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients, LayerConductance\n",
    "from captum.attr import visualization as viz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug here)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer (replace with a model with some rudimentary physics understanding)\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff1a9a",
   "metadata": {},
   "source": [
    "First, we create a custom forward function, taking as arguments\n",
    "\n",
    "> 1. the preembedded input tokens, \n",
    "> 2. the attention masks (since every choice may have a different length, we use padding tokens which should be masked) and\n",
    "> 3. token type ids (which signify where the question ends and the choice begins), all of shape [num_choices, seq_len], \n",
    "\n",
    "and returning\n",
    " \n",
    "> 1. the numbers we want to attribute with respect to (usually the logits). \n",
    "\n",
    "For multiple choice, we return a tensor [batch_size, num_choices], and we will specify the index to attribute (that will be the model's predicted index) in the final attribution call to reduce this tensor to a single scalar number. Note that we use the softmaxed output instead of the logits, since we want to know why one choice is *preferred to the others* (with logits, every index could increase simultaneously, so it is not a good choice for differentiating the choices).\n",
    "\n",
    "Effectively, this is just telling Captum what number we should attribute with respect to, and we will later showcase how one could use this to attempt input attribution for an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_MpC(inputs_embeds, attention_mask, token_type_ids=None): \n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. \n",
    "    \n",
    "    Captum expects a 2D tensor ([batch_size=num_choices, seq_len]),\n",
    "    while mulitple choice/classification models expect [batch size, num_choices, seq_len],\n",
    "    so we unsqueeze the first index to add batch_size=1. \n",
    "    We return the normalized logits for each choice.\n",
    "    Some models (eg. RoBERTa) do not use token_type_ids, hence the optional parameter.\"\"\"\n",
    "\n",
    "    # Reshaping for multiple choice compatibility\n",
    "    inputs_embeds = inputs_embeds.unsqueeze(0) # If attributing the embedding layer, this will be different, see below.\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logit shape from forward pass:\", logits.shape)\n",
    "    return logits.softmax(dim=-1) # normalizing to differentiate choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1181e",
   "metadata": {},
   "source": [
    "We now create a convenience function for tokenizing and preembedding the inputs (question + choices) and making the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Question: What is the capital of France?\n",
      "Predicted Answer: 2) Paris\n",
      "Probabilities: tensor([[0.0797, 0.1972, 0.7231]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get input_ids, inputs_embeds, attention_masks, token_type_ids for Captum attribution methods \n",
    "# and make model prediciton to get probabilities and choice_idx\n",
    "\n",
    "def get_encoding_and_predict(question, choices, tokenizer, embed_layer=model.bert.embeddings, type_tokens=True, print_output=True):\n",
    "    \"\"\"Tokenizes and preembeds the question and choices and makes prediction, returning input_ids, attention_masks, and token_type_ids, \n",
    "    logits and choice index. token_type_ids is None if token_types is False. The input ids are shaped as [num_choices, seq_len].\"\"\"\n",
    "    \n",
    "    # Tokenize for multiple choice and get input ids, attention masks, and token type ids\n",
    "    encoding = tokenizer(\n",
    "        [question] * len(choices),\n",
    "        choices,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]               # shape: [num_choices, seq_len]\n",
    "    attention_masks = encoding[\"attention_mask\"]    # -\"-\n",
    "    if type_tokens:\n",
    "        token_type_ids = encoding[\"token_type_ids\"] # -\"-\n",
    "    else:\n",
    "        token_type_ids = None\n",
    "\n",
    "    # Run input_ids through preembed layer\n",
    "    inputs_embeds = embed_layer(input_ids, token_type_ids=token_type_ids) # position_ids are generated by default\n",
    "\n",
    "    # Compute model prediction and get choice index\n",
    "    probs = forward_func_MpC(inputs_embeds, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "    choice_idx = torch.argmax(probs).item()\n",
    "\n",
    "    if print_output:\n",
    "        print('Question:', question)\n",
    "        print('Predicted Answer:', f'{choice_idx})', choices[choice_idx])\n",
    "    if cfg.debug: print('Probabilities:', probs) # To gauge model confidence\n",
    "\n",
    "    # Get baseline with previous function\n",
    "    return input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx\n",
    "\n",
    "\n",
    "# Quick computation\n",
    "input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx = get_encoding_and_predict(question, choices, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9c7ed",
   "metadata": {},
   "source": [
    "Integrated Gradients calculates the attribution by differentiation, varying the input between the a baseline and the original input. For MpC, one very natural baseline based on the input ids of the chosen index is generated generally by the following function (the format is indicated in the inline comment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3aeaccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline inputs embeds for Integrated Gradients\n",
    "def get_baseline(tokenizer, input_ids, choice_idx, embed_layer=model.bert.embeddings):\n",
    "    ref_token_id = tokenizer.pad_token_id   # padding\n",
    "    sep_token_id = tokenizer.sep_token_id   # sepatator\n",
    "    cls_token_id = tokenizer.cls_token_id   # start of sequence\n",
    "\n",
    "    # Create baseline input: [CLS] [PAD] ... [PAD] [SEP] [PAD] ... [PAD] [SEP] [PAD] ... like choice_ids, expanded as input_ids\n",
    "    ref_tokens = [cls_token_id]\n",
    "    for token in input_ids[choice_idx, 1:]:\n",
    "        if token == sep_token_id: # We keep only the separators, else we use padding\n",
    "            ref_tokens += [sep_token_id]\n",
    "        else:\n",
    "            ref_tokens += [ref_token_id]\n",
    "    \n",
    "    ref_input_ids = torch.tensor(ref_tokens, dtype=torch.long).expand_as(input_ids)\n",
    "    ref_token_type_ids = torch.zeros_like(ref_input_ids)\n",
    "\n",
    "    return embed_layer(ref_input_ids, token_type_ids=ref_token_type_ids) # preembed for multi-layer attribution\n",
    "\n",
    "ref_inputs_embeds = get_baseline(tokenizer, input_ids, choice_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c7106",
   "metadata": {},
   "source": [
    "We now make a convenience function for getting the token attributions, summing over the internal model dimension to get an overal attribution for each position (token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "671fbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_attributions(forward_func, choice_idx, inputs_embeds, ref_inputs_embeds, attention_masks, token_type_ids=None, n_steps=50):\n",
    "\n",
    "    if cfg.debug: print(inputs_embeds.shape)\n",
    "    \n",
    "    # LayerIntegratedGradients for attribution\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    # Compute attributions for chosen index \n",
    "    # (Captum wants [num_choices, seq_len] and gives attr shape [choices, seq_len, layer_output_dim])\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        baselines=ref_inputs_embeds,\n",
    "        additional_forward_args=(attention_masks, token_type_ids),\n",
    "        target=choice_idx,              # Target the chosen answer, takes idx [0,target] of forward_func output\n",
    "        n_steps=n_steps,                # Number of steps for approximation\n",
    "        return_convergence_delta=True   # Error estimate\n",
    "    )\n",
    "    # Captum attributes every input wrt the target idx. We only care about the choice_idx attributions, \n",
    "    # but we have to pass in all the mulitple choice alternatives in order to get the correct softmax output.\n",
    "    # This significantly increases compute time!\n",
    "\n",
    "    # Sum across layer_output_dim to get token-level importance¨\n",
    "    print(attributions.shape)\n",
    "    choice_token_attributions = attributions[choice_idx].sum(dim=-1)  # shape: [num_choices, seq_len]\n",
    "    choice_token_attributions = choice_token_attributions / torch.norm(choice_token_attributions)  # Normalize\n",
    "\n",
    "    if cfg.debug: print('Attributions per token at choice_idx:', choice_token_attributions)\n",
    "    \n",
    "    return choice_token_attributions, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db902418",
   "metadata": {},
   "source": [
    "Here comes the main computation block, which can take several minutes with no GPU depending on the model, input length and number of steps. With our toy example, it should take seconds even on a laptop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7028056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 768])\n",
      "Logit shape from forward pass: torch.Size([1, 150])\n",
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "torch.Size([3, 11, 768])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000, -0.2769,  0.0954,  0.0860,  0.1008, -0.2202,  0.2358, -0.6626,\n",
      "         0.0000,  0.5882, -0.0874], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Main computation block, can take several minutes depending on the model, input length and number of steps.\n",
    "# If cfg.debug is True, it will print intermediate results to indicate progress.\n",
    "\n",
    "token_attributions, delta = get_token_attributions(\n",
    "    forward_func_MpC, choice_idx, inputs_embeds, ref_inputs_embeds, attention_masks, token_type_ids\n",
    ")\n",
    "\n",
    "# Convert input_ids to readable tokens for the choice_idx\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d43274",
   "metadata": {},
   "source": [
    "At last, we use Captum to make a neat visualisation of the attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "702aba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.14</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,         # word attributions\n",
    "                        torch.max(probs),           # prediction probability\n",
    "                        torch.argmax(probs),        # predicted class\n",
    "                        ground_truth_idx,           # ground truth class\n",
    "                        str(choice_idx),            # attributing to this class\n",
    "                        token_attributions.sum(),   # summed attribution score\n",
    "                        tokens,                     # tokens for the question and choice\n",
    "                        delta,                      # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # Save return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7eae6",
   "metadata": {},
   "source": [
    "Indeed, the words \"France\", \"capital\" and \"Paris\" seems to be the main positive contributors, though the attribution score is very low. The model credence is also very poor at 65%, so a better model should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33fb68",
   "metadata": {},
   "source": [
    "#### Adapting for another model\n",
    "\n",
    "This short section adds nothing new, but collects the above in a convenience function and shows how to apply it to a slightly bigger, and much better, RoBERTa model. The only required modification is that RoBERTa does not use type_token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "759937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing new here, just importing the RoBERTa model. We use the question and choices from before.\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"LIAMF-USP/roberta-large-finetuned-race\"\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb992cf",
   "metadata": {},
   "source": [
    "All-in-one convenience wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(forward_func, layer, question=question, choices=choices, ground_truth_idx=ground_truth_idx, tokenizer=tokenizer, type_tokens=False, n_steps=50):\n",
    "    \"\"\"A quick function to do everything we have implemented so far: tokenize, make prediction, compute attributions and visualize.\"\"\"\n",
    "    # Tokenize for multiple choice and get input ids, attention masks, WITHOUT token type ids (if token_types=False), \n",
    "    # and make prediction to get logits and choice index\n",
    "    input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx = get_encoding_and_predict(question, choices, tokenizer, layer, type_tokens=type_tokens)\n",
    "    ref_inputs_embeds = get_baseline(tokenizer, input_ids, choice_idx, layer)\n",
    "\n",
    "    # Compute attributition at choice idx\n",
    "    token_attributions, delta = get_token_attributions(\n",
    "        forward_func, choice_idx, inputs_embeds, ref_inputs_embeds, attention_masks, token_type_ids, n_steps=n_steps, \n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "    vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,         # word attributions\n",
    "                        torch.max(probs),           # prediction probability\n",
    "                        torch.argmax(probs),        # predicted class\n",
    "                        ground_truth_idx,           # ground truth class\n",
    "                        str(choice_idx),            # attributing to this class\n",
    "                        token_attributions.sum(),   # summed attribution score\n",
    "                        tokens,                     # tokens for the question and choice\n",
    "                        delta,                      # convergence delta\n",
    "    )\n",
    "\n",
    "    viz.visualize_text([vis])\n",
    "\n",
    "    return choice_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc406a",
   "metadata": {},
   "source": [
    "Main computation block, which could take several minutes on a laptop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0678691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 5])\n",
      "Question: What is the capital of France?\n",
      "Predicted Answer: 2) Paris\n",
      "Probabilities: tensor([[1.0868e-02, 1.9246e-03, 9.8029e-01, 6.7416e-03, 1.7915e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5, 13, 1024])\n",
      "Logit shape from forward pass: torch.Size([1, 250])\n",
      "Logit shape from forward pass: torch.Size([1, 5])\n",
      "Logit shape from forward pass: torch.Size([1, 5])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000, -0.3038,  0.1999,  0.0105, -0.0012, -0.0873,  0.2603,  0.6403,\n",
      "        -0.1718, -0.2130, -0.4560,  0.3154,  0.0000], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.19</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġthe                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġcapital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġof                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ĠFrance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Paris                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #pad                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attributions for RoBERTa\n",
    "layer = model.roberta.embeddings\n",
    "do_everything(forward_func_MpC, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d86ad",
   "metadata": {},
   "source": [
    "Notice that the model credence is way better, at over 98%, than the previous model. Alas, it is still not great on the FCI, and preliminary testing indicates that it does not really make sense to talk about misconceptions here since the credences are so low (that is, the answers are almost random; I am not convinced that this model has developed any internal understanding of physics). This is not surprising, since the FCI questions are very different from the training data, and since the model is so small as to run on a laptop CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfffc79",
   "metadata": {},
   "source": [
    "**Note:** The results do not really make sense, as I get negative attr scores with bert. This indicates that the baseline is more likely than the correct result; either my BERT is really very bad, or something is off. With the Roberta, another strange thing ocurs; paris gives a negative contribution, instead the separator contributes. Look it over later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac226ab",
   "metadata": {},
   "source": [
    "### Attribution for embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5716736",
   "metadata": {},
   "source": [
    "To determine which words in a text chunk are most important for the placement of that chunk in embedding space, we must fist choose a metric on the embedding space and some reference vector. Concretely, we do the following:\n",
    "\n",
    "> Given a text chunk, we run it thorugh the model to get the corresponding embedding vector. Then, we want to see which words in the chunk we should tune to get the largest change in embedding position; that is, we attribute with respect to the cosine similarity between the true output and the perturbed output.\n",
    "\n",
    "Ideally, one might want to use a separate vector as a reference, but there is no canonical choice. If the dataset gives a natural clustering, such as in a a MpC/QA-dataset, one could experiment with using the question as the reference vector. \n",
    "\n",
    "The following code gives the general procedure, but I am not sure how to test whether it is a sensible/useful approach. This is a particular example of how embedding models may cause difficulty in interpretability, since the output is non-interpretable. There are also other potential issues: i) In high dimensional spaces, most randomly chosen vectors are essentially orthogonal, so unless the perturbations are very small, we might just get orthogonality for all our perturbations, giving basically no information at all. ii) If the linearity hypothesis is not entirely true, the model may weight different directions in embedding space differently, so that a small change along an axis which is very densely populated with different semantic concepts could be more meaningful than a large movement in a sparse direction. I have not investigated either issue at all, as I suspect they will be model and input specific, and I do not have a particular application in mind here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea479d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer # note that the class structures of Autos are slightly different\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\" # embedding model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - from sentence transformers,\n",
    "# essentially averaging over all positions to get the actual singe embedding for the whole chunk\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Use preembedded\n",
    "def embed_tokens(input_ids, attention_mask=None): # Uses global model\n",
    "    model_output = model(input_ids, attention_mask) # attn mask only needed for padding tokens on acausal models, and we have a single batch\n",
    "    # Perform pooling and normalize\n",
    "    embedding = mean_pooling(model_output, attention_mask)\n",
    "    return F.normalize(embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Compute baseline embedding\n",
    "text = ['This is a test sentence.']\n",
    "target_idx = 0 # only one input\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt') # Tokenize\n",
    "\n",
    "# Model input\n",
    "input_ids = encoding['input_ids'] # Get input ids, shape: [batch_size, seq_len]\n",
    "attention_masks = encoding['attention_mask'] # Get attention masks (for each batch, same shape as input_ids)\n",
    "inputs_embeds = \n",
    "\n",
    "# Baseline embedding for IG\n",
    "baseline_token_id = tokenizer.pad_token_id \n",
    "baseline_ids = torch.full_like(input_ids, baseline_token_id)\n",
    "\n",
    "# Reference embedding for distance computation\n",
    "reference_embeddings = embed_tokens(input_ids, attention_masks) # shape: [batch_size, embedding_dim]\n",
    "\n",
    "if cfg.debug: print(input_ids.shape, attention_masks.shape, baseline_ids.shape, reference_embeddings.shape, ) # Debug shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9efae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_cos_sim(input_ids, attention_mask=None, reference_embeddings=reference_embeddings):\n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients for cosine similarity to reference embeddings.\"\"\"\n",
    "    \n",
    "    embeddings = embed_tokens(input_ids, attention_mask)\n",
    "    sim = F.cosine_similarity(reference_embeddings, embeddings, dim=1) # Compute cosine distance with baseline embedding\n",
    "\n",
    "    if cfg.debug: print(\"Similarity:\", sim.shape)\n",
    "    return sim # tensor of shape [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccdc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: torch.Size([1])\n",
      "Original distance: 1.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m lig = IntegratedGradients(forward_func_cos_sim)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m attributions, delta = \u001b[43mlig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of steps for approximation\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:289\u001b[39m, in \u001b[36mIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[39m\n\u001b[32m    277\u001b[39m     attributions = _batch_attribution(\n\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m         num_examples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m         method=method,\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[32m    299\u001b[39m     start_point, end_point = baselines, inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:368\u001b[39m, in \u001b[36mIntegratedGradients._attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[39m\n\u001b[32m    365\u001b[39m expanded_target = _expand_target(target, n_steps)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[32m    377\u001b[39m scaled_grads = [\n\u001b[32m    378\u001b[39m     grad.contiguous().view(n_steps, -\u001b[32m1\u001b[39m)\n\u001b[32m    379\u001b[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001b[32m1\u001b[39m).to(grad.device)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[32m    381\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:128\u001b[39m, in \u001b[36mcompute_gradients\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03marbitrary forward function.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m \u001b[33;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     outputs = \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    132\u001b[39m     outputs = cast(Tensor, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\common.py:588\u001b[39m, in \u001b[36m_run_forward\u001b[39m\u001b[34m(forward_func, inputs, target, additional_forward_args)\u001b[39m\n\u001b[32m    585\u001b[39m inputs = _format_inputs(inputs)\n\u001b[32m    586\u001b[39m additional_forward_args = _format_additional_forward_args(additional_forward_args)\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m output = \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch.futures.Future):\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output.then(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x.value(), target))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mforward_func_cos_sim\u001b[39m\u001b[34m(input_ids, attention_mask, reference_embeddings)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_func_cos_sim\u001b[39m(input_ids, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, reference_embeddings=reference_embeddings):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom forward pass for Captum Integrated Gradients for cosine similarity to reference embeddings.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     embeddings = \u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     sim = F.cosine_similarity(reference_embeddings, embeddings, dim=\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Compute cosine distance with baseline embedding\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cfg.debug: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSimilarity:\u001b[39m\u001b[33m\"\u001b[39m, sim.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36membed_tokens\u001b[39m\u001b[34m(input_ids, attention_mask)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_tokens\u001b[39m(input_ids, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# Uses global model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     model_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# attn mask only needed for padding tokens on acausal models, and we have a single batch\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Perform pooling and normalize\u001b[39;00m\n\u001b[32m     11\u001b[39m     embedding = mean_pooling(model_output, attention_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:952\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    950\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    961\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:178\u001b[39m, in \u001b[36mBertEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    175\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.position_ids.device)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m token_type_embeddings = \u001b[38;5;28mself\u001b[39m.token_type_embeddings(token_type_ids)\n\u001b[32m    181\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Compute model prediction\n",
    "start_dist = forward_func_cos_sim(input_ids, attention_masks, reference_embeddings)\n",
    "print(\"Original distance:\", start_dist.item()) # Print original distance\n",
    "\n",
    "layer = model.embeddings # Get the embedding layer\n",
    "# get preembeds\n",
    "\n",
    "ig = IntegratedGradients(forward_func_cos_sim)\n",
    "\n",
    "# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=baseline_ids,\n",
    "    additional_forward_args=(attention_masks, reference_embeddings, ), \n",
    "    n_steps=50,  # Number of steps for approximation\n",
    "    return_convergence_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b963398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[target_idx])\n",
    "\n",
    "# Step 2: Aggregate attribution scores (across embedding dimension)\n",
    "# Shape of attributions: [1, seq_len, emb_dim]\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [seq_len]\n",
    "\n",
    "# Step 3: Print token + attribution\n",
    "print(\"Token-wise attributions:\")\n",
    "for token, score in zip(tokens, token_attributions):\n",
    "    print(f\"{token:>12} : {score.item():.4f}\")\n",
    "\n",
    "\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,        # token-wise attributions\n",
    "                        0, # prediction probability (not relevant for distance attribution)\n",
    "                        0, # predicted class            -\"-\n",
    "                        0, # ground truth class         -\"-\n",
    "                        0, # attributing to this class  -\"-\n",
    "                        token_attributions.sum(),  # summed attribution score (NA)\n",
    "                        tokens,                    # tokens for the question and choice\n",
    "                        delta,                     # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # get return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84999749",
   "metadata": {},
   "source": [
    "### Layer-wise attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90998b7",
   "metadata": {},
   "source": [
    "Often, we may want to use input attribution to get a rough idea of where a feature may be located in a model. For instance, the fact that Paris is the capitol of France may be stored in some MLP-layer of the model. Once the layer is identified, the specific neurons may ideally be localized with intervention methods (see part 1). This section makes an attempt at implementing such a rough fact-layer search on an encoder for multiple choice, though the results are not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9b9e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93adda",
   "metadata": {},
   "source": [
    "Note the negative overall attribution score; this indicates that the initial embedding layer did not prefer Paris to the other capitals, and that only later layers corrected for this. That aligns with the general view that facts are mostly contained in MLP-layers. In this way we can try to use LIG to indicate what layer has stored the fact \"Paris is the capital of France\" (alas, there are 24 (!) layers in this model, so we can only do this for a toy model - even here, all layers will by random chance prefer some alternative, so without tracing the full information path thorough the model it is nontrivial to do this in practice). \n",
    "\n",
    "The naive approach to find the imporant MLP-layers is given below (not nearly enough steps to get very accurate attributions; the delta-s should be examined if making a through analysis). We see that certain layers (Mainly MLP output-layer 0) give a strong push towards the correct answer, while the later layers are hardly involved or pushes in the opposite direction, and one could try to find a Paris node/feature in the eary layers of the model with patching.\n",
    "\n",
    "However, there is a much better way to do this with LayerConductance in Captum, which will follow below.\n",
    "\n",
    "<details>\n",
    "  <summary> Obvious application [click to expand]  </summary>\n",
    "\n",
    "Take a toy BERT model which can answer very easy MpCs (like capitals, or easier) and find the \"Paris in France\" **layer**, then use patching from Frederik to find specific **nodes**. I will not pursue this unless I have time in the end, since it is a straightforward (though not very likely to be successful) application of the tools in this notebook and the patching-notebook.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92e4518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(model.roberta.encoder.layer)):\n",
    "#     print(i)\n",
    "#     layer = model.roberta.encoder.layer[i].output # or should i use the hidden mlp layer .intermediate? Can use .dense to do pre-(layerNorm/activation)\n",
    "#     do_everything(forward_func_softmax, layer, n_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a2cda",
   "metadata": {},
   "source": [
    "### Muli-layer attribution\n",
    "As a final step with IG-attribution from captum, let's look at how to attribute the whole model rather than only a layer. The only difference is that we provide embeddings instead of ids to the IG attribution method (which is also an option with LIG, but ids are easier), since the ids are discrete/not diffable. So we need to tokenize *and* word/position/type-embed before inputting, and get the attribution for the layers that are not the embedding layers (which are essentially look-up-tables anyway/not the most interesting part of a transformer).\n",
    "\n",
    "This is really the most interesting for the application described above, while the layer attribution is most useful for coarse feature search before intervening on specific nodes in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0143be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From here on down, I have not been too careful / reviewed yet - there might be errors (but the code runs).\n",
    "\n",
    "from captum.attr import LayerConductance\n",
    "\n",
    "# For plotting heatmaps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Returning to smaller BERT-model\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# Absurd way to get choice idx for testing only\n",
    "# layer = model.bert.embeddings\n",
    "# choice_idx = do_everything(forward_func_softmax, layer, model=model, tokenizer=tokenizer, token_types=True)\n",
    "\n",
    "# Forward func taking embeds\n",
    "def forward_func_softmax_embed(input_emb, attention_mask=None):\n",
    "    \n",
    "    input_emb = input_emb.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=input_emb,\n",
    "        attention_mask=attention_mask,\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits.softmax(dim=1)\n",
    "\n",
    "# From demo, to get input_embs and ref_input_embs (running ids through emb layer)\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 768]) torch.Size([3, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "ref_token_type_ids = torch.zeros_like(token_type_ids)  # Reference token type ids, all zeros for BERT\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=device)  # Position ids for the input sequence\n",
    "ref_position_ids = torch.zeros_like(position_ids)  # Reference position ids, all zeros for BERT\n",
    "\n",
    "input_embs, ref_input_embs = construct_whole_bert_embeddings(\n",
    "    input_ids, ref_input_ids, token_type_ids=token_type_ids, \n",
    "    ref_token_type_ids=ref_token_type_ids, position_ids=position_ids, ref_position_ids=ref_position_ids\n",
    ") # maybe zero vector is better baseline?\n",
    "\n",
    "if cfg.debug: print(input_embs.shape, ref_input_embs.shape)\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients # Not tested yet\n",
    "\n",
    "# IntegratedGradients\n",
    "def get_token_attributions_for_model(layer, forward_func, choice_idx, input_embs, ref_input_embs, attention_mask, token_type_ids=None, n_steps=50):\n",
    "\n",
    "    if cfg.debug: print(input_embs.shape)\n",
    "    \n",
    "    # LayerIntegratedGradients for attribution\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    # Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, layer_output_dim])\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=input_embs,\n",
    "        baselines=ref_input_embs,\n",
    "        additional_forward_args=(attention_mask, token_type_ids),\n",
    "        target=choice_idx,  # Target the chosen answer, uses [0,target]\n",
    "        n_steps=n_steps,  # Number of steps for approximation\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    # Sum across embedding dimensions to get token-level importance\n",
    "    token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "    token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "\n",
    "    if cfg.debug: \n",
    "        print('Token attributions:', token_attributions.shape)\n",
    "        print('Attributions per token at choice_idx:', token_attributions[choice_idx])\n",
    "    \n",
    "    return token_attributions, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ce8d1",
   "metadata": {},
   "source": [
    "\n",
    "We can also do LayerConductance to see attr-scores for each token across layers - this is for locating the feature to a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7dad3e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 1: -1.2461051386781037\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 2: -1.2421367736533284\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 3: -1.315762855578214\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 4: -1.0917498202761635\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 5: -1.1474649207666516\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 6: -1.0865842448547482\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 7: -1.0264790678629652\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 8: -1.032434618100524\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 9: -1.0169967371039093\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 10: -1.081912085879594\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 11: -1.2508521545678377\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 12: -0.9224346876144409\n"
     ]
    }
   ],
   "source": [
    "layer_attrs = [] # Empty list to store attributions for each layer\n",
    "\n",
    "for i in range(model.config.num_hidden_layers): # attrubuting tokenwise for each layer\n",
    "    lc = LayerConductance(forward_func_softmax_embed, model.bert.encoder.layer[i])\n",
    "    layer_attributions = lc.attribute(inputs=input_embs, baselines=ref_input_embs, additional_forward_args=(attention_masks,), \\\n",
    "                                      target=choice_idx, n_steps=50, ) # reduced steps to speed up, sacrificing acc, though the trend holds up\n",
    "    layer_attrs.append(summarize_attributions(layer_attributions).cpu().detach().tolist())\n",
    "    print(f\"Layer {i+1}:\", np.sum(layer_attrs[i][choice_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73e1ba",
   "metadata": {},
   "source": [
    "These results make no sense, crap. In fact, they seem to be inverted by the plot below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "36170947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3, 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if cfg.debug: print(np.array(layer_attrs).shape)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "xticklabels=tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "yticklabels=list(range(1, model.config.num_hidden_layers+1))\n",
    "ax = sns.heatmap(np.array(layer_attrs)[:,choice_idx,:], xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2) #, annot=True\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e86339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27af172f",
   "metadata": {},
   "source": [
    "It is also possible to look closely at specific tokens and how they are processed by each layer, see the very end of the [first Captum BERT demo](https://captum.ai/tutorials/Bert_SQUAD_Interpret) (This should be pretty much similar for our case when treating only the choice idx and using only one attribution rather than start and end). Attn interp can be seen in the attention notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

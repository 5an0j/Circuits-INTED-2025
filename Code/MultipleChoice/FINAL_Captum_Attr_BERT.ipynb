{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61132036",
   "metadata": {},
   "source": [
    "# Attribution with Captum on Encoders for Multiple Choice or Embedding\n",
    "\n",
    "<!-- This is based on the Captum BERT-interpretation demo notebook, adapted for multiple choice models, and hopefully more instructive by indicating clearly the expected tensor shapes for the catum methods. Overall, Captum is neither wonderful nor horrible to work with, but I suspect transformer lens / more specific libraries could make for a more enjoyable coding experience. -->\n",
    "\n",
    "The applications listed are suggestions for further research, and none have been completed: The notebook instead seeks to provide a toolkit, and the applications serve as motivations.\n",
    "\n",
    "Overall, this has sadly been time consuming and not at all successfull, and I suspect that while the listed applications are highly interesting and seem promising, the simple attribution method I employ is too weak, so that a satisfactory solution to either application will require a dramatically more systematic/sophisticated approach. The main problem is that all results are very unstable: Even the simplest toy results are highly sensitive to seemingly arbitrary choices like question phrasing (such as adding spaces, exchanging synonyms or reordering words). One could hope that this would be somewhat improved with a good model, but I am skeptical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients, LayerConductance\n",
    "from captum.attr import visualization as viz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug here)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "## Application I: Finding Physical Misunderstandings on the FCI of BERT-Style Models Fine-Tuned for Multiple Choice\n",
    "\n",
    "We use Integrated Gradients from Captum to compute word attributions on the FCI for a BERT-model fine-tuned for answering mulitple choice questions. Specifically, we want to see whether the emphasised words indicate that the model exhibits the same common misconceptions as humans. In the example question below, even if the model gives a common wrong answer (eg. that the pushing car exerts a larger force, or that the heavier object exerts a larger force) we can perhaps use attribution to see precisely which words throw the model off, and then modify the prompt to determine by causal experiments what the model's misconception is. There is detailed qualitative research on student misconceptions on the FCI which could aid us greatly in finding likely misconceptions.\n",
    "\n",
    "Alas, I cannot compute the gradients for large models with long inputs on my laptop, and our MpC-models are not trained for long physics questions, so the models used here are not sufficiently good to get at all interesting results with these. Therefore, we cannot complete this application, and instead demonstrate only the methods on a basic test question. \n",
    "\n",
    "The following code will hopefully be directly applicable to any appropriate model (potentially self fine-tuned for the purpose) with minimal adjustments, and we show as an example how to (very slighlty) modify it for a different architechture (RoBERTa).\n",
    "\n",
    "---\n",
    "\n",
    "Example FCI-question: *A large truck breaks down out on the road and receives a push back into town by a small compact car. After the car reaches the constant cruising speed at which its driver wishes to push the truck:*\n",
    "\n",
    "0) *The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.*\n",
    "1) *The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.*\n",
    "2) *The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.*\n",
    "3) *The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot push back against the car. The truck is pushed forward simply because it is in the way of the car.*\n",
    "4) *Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b113b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question (replace with some FCI-question)\n",
    "question = \"The capital of France is...\" # we try to format the question like the model traning data\n",
    "choices = [\"Berlin\", \"Madrid\", \"Paris\", ] # \" Rome\", \" India\"]\n",
    "ground_truth_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer (replace with a model with some rudimentary physics understanding)\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff1a9a",
   "metadata": {},
   "source": [
    "First, we create a custom forward function, taking as arguments\n",
    "\n",
    "> 1. the preembedded input tokens, \n",
    "> 2. the attention masks (since every choice may have a different length, we use padding tokens which should be masked) and\n",
    "> 3. token type ids (which signify where the question ends and the choice begins), all of shape [num_choices, seq_len], \n",
    "\n",
    "and returning\n",
    " \n",
    "> 1. the numbers we want to attribute with respect to (usually the logits). \n",
    "\n",
    "For multiple choice, we return a tensor [batch_size, num_choices], and we will specify the index to attribute (that will be the model's predicted index) in the final attribution call to reduce this tensor to a single scalar number. \n",
    "\n",
    "<details>\n",
    "<summary>Discussion on logits versus softmaxed output for MpC</summary>\n",
    "\n",
    "However, it might not be reasonable to use the logits for MpC: If using logits, only the choice index is relevant (there is no coupling between choices), and since every choice to a multiple choice question will often be designed to be somewhat feasible, this could make it so that every choice gets a positive overall attribution score. Said more plainly; it is possible for all logits to increase simultaneously, even though the model credence cannot increase for all choices simultaneously. However, softmax is highly non-linear, and the coupling may cause instability. Additionally, adding coupling means that we need to feed the model every choice, while logits would allow for faster computation since we would only need the choice we want to attribute with respect to. I have tested both, and alas, the results do seem to depend highly on the choice here.\n",
    "\n",
    "For our imagined application (detecting misconceptions) we are mostly interested in how the question phrasing changes the credence for the given model choice, and so it should be fine to use logits. However, I have kept the code unoptimized such that one could just as well use a softmax/normalized input, if one would prefer. For real applications using logits, reduce the input, baseline, token types and attention masks by indexing with the choice_index before feeding into ig.attribute.\n",
    "\n",
    "</details> \n",
    "\n",
    "Effectively, this is just telling Captum what number we should attribute with respect to, and we will later showcase how one could use this to attempt input attribution for an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_MpC(inputs_embeds, attention_mask, token_type_ids=None): \n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. \n",
    "    \n",
    "    Captum expects a 2D tensor ([batch_size=num_choices, seq_len]),\n",
    "    while mulitple choice/classification models expect [batch size, num_choices, seq_len],\n",
    "    so we unsqueeze the first index to add batch_size=1. \n",
    "    We return the normalized logits for each choice.\n",
    "    Some models (eg. RoBERTa) do not use token_type_ids, hence the optional parameter.\"\"\"\n",
    "\n",
    "    # Reshaping for multiple choice compatibility\n",
    "    inputs_embeds = inputs_embeds.unsqueeze(0) # If attributing the embedding layer, this will be different, see below.\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logit shape from forward pass:\", logits.shape)\n",
    "    return logits.softmax(dim=-1) # normalizing to differentiate choices (this is maybe not a good idea, it causes to much coupling / instability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1181e",
   "metadata": {},
   "source": [
    "We now create a convenience function for tokenizing and preembedding the inputs (question + choices) and making the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Question: The capital of France is...\n",
      "Predicted Answer: 2) Paris\n",
      "Probabilities: tensor([[0.0098, 0.1466, 0.8436]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get input_ids, inputs_embeds, attention_masks, token_type_ids for Captum attribution methods \n",
    "# and make model prediciton to get probabilities and choice_idx\n",
    "\n",
    "def get_encoding_and_predict(question, choices, tokenizer, embed_layer=model.bert.embeddings, type_tokens=True, print_output=True):\n",
    "    \"\"\"Tokenizes and preembeds the question and choices and makes prediction, returning input_ids, attention_masks, and token_type_ids, \n",
    "    logits and choice index. token_type_ids is None if token_types is False. The input ids are shaped as [num_choices, seq_len].\"\"\"\n",
    "    \n",
    "    # Tokenize for multiple choice and get input ids, attention masks, and token type ids\n",
    "    encoding = tokenizer(\n",
    "        [question] * len(choices),\n",
    "        choices,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]               # shape: [num_choices, seq_len]\n",
    "    attention_masks = encoding[\"attention_mask\"]    # -\"-\n",
    "    if type_tokens:\n",
    "        token_type_ids = encoding[\"token_type_ids\"] # -\"-\n",
    "    else:\n",
    "        token_type_ids = None\n",
    "\n",
    "    # Run input_ids through preembed layer\n",
    "    inputs_embeds = embed_layer(input_ids, token_type_ids=token_type_ids) # position_ids are generated by default\n",
    "\n",
    "    # Compute model prediction and get choice index\n",
    "    probs = forward_func_MpC(inputs_embeds, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "    choice_idx = torch.argmax(probs).item()\n",
    "\n",
    "    if print_output:\n",
    "        print('Question:', question)\n",
    "        print('Predicted Answer:', f'{choice_idx})', choices[choice_idx])\n",
    "    if cfg.debug: print('Probabilities:', probs) # To gauge model confidence\n",
    "\n",
    "    # Get baseline with previous function\n",
    "    return input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx\n",
    "\n",
    "\n",
    "# Quick computation\n",
    "input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx = get_encoding_and_predict(question, choices, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9c7ed",
   "metadata": {},
   "source": [
    "Integrated Gradients calculates the attribution by differentiation, varying the input between a baseline and the original input. The choice of baseline constitutes our second major choice, though this seemingly has a smaller effect on the results. If using logits, we would only care about the choice index, and the construction indicated in the inline comment is most natural. However, when including coupling, one might either i) change every choice to the same baseline and move towards the true input, ii) change only the choice index and leave the unchosen choices the same in the baseline as in the true input or iii) replace the choice index with some other choice in the baseline. There are probably other options, but these are the ones I tested (briefly).\n",
    "\n",
    "We chose option i), which seems the naive/most natual approach, but I find that the attribution score is confusing and unstable. The best way to address this might be to keep fixed words which may be irrelevant, so that we only replace the token for [France] with [PAD] and [paris] with [PAD]. This manual work would have the advantage of increased control, but also makes the results more dependent on subjective choices from the researcher (with this toy question it is pretty obvious which words are meaningful, but maybe not on the FCI. Also, empirically, small choices seem to have a potentially large effect on the results...)\n",
    "\n",
    "Alterntaivelt, one could do the attribution wrt the question and wrt the choice separately, that is, keep the question xor choice fixed in the baseline, as is made possible by inline comments. Otherwise, the attribution will be influenced both by the changing question and the changing answer. I would assume this to be fine, since the subset of semantic space where the model has strong credences should intuitively be very small..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "3aeaccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline inputs embeds for Integrated Gradients\n",
    "def get_baseline(tokenizer, input_ids, choice_idx, embed_layer=model.bert.embeddings, ids=False):\n",
    "    ref_token_id = tokenizer.pad_token_id   # padding\n",
    "    sep_token_id = tokenizer.sep_token_id   # sepatator\n",
    "    cls_token_id = tokenizer.cls_token_id   # start of sequence\n",
    "\n",
    "    # Create baseline input: [CLS] [PAD] ... [PAD] [SEP] [PAD] ... [PAD] [SEP] [PAD] ... like choice_ids, expanded as input_ids\n",
    "    ref_tokens = [cls_token_id]\n",
    "    # question = True # switch to keep question the same in the baseline\n",
    "    for token in input_ids[choice_idx, 1:]:\n",
    "        if token == sep_token_id: # We keep only the separators, else we use padding\n",
    "            ref_tokens += [sep_token_id] # Pythonic .append(sep_token_id)\n",
    "    #        question = False # separator indicates end of question\n",
    "    #    elif question: ref_tokens += [token]\n",
    "        else:\n",
    "            ref_tokens += [ref_token_id]\n",
    "    \n",
    "    ref_input_ids = torch.tensor(ref_tokens, dtype=torch.long).expand_as(input_ids)\n",
    "    if ids: return ref_input_ids\n",
    "    \n",
    "    ref_token_type_ids = torch.zeros_like(ref_input_ids)\n",
    "    return embed_layer(ref_input_ids, token_type_ids=ref_token_type_ids) # preembed for multi-layer attribution\n",
    "\n",
    "# Quickly compute\n",
    "ref_inputs_embeds = get_baseline(tokenizer, input_ids, choice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "fb8fc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a different baseline\n",
    "# Baseline inputs embeds for Integrated Gradients\n",
    "def get_baseline(tokenizer, input_ids, choice_idx, embed_layer=model.bert.embeddings, ids=False):\n",
    "    ref_token_id = tokenizer.pad_token_id   # padding\n",
    "    sep_token_id = tokenizer.sep_token_id   # sepatator\n",
    "    cls_token_id = tokenizer.cls_token_id   # start of sequence\n",
    "\n",
    "    # Create baseline input: [CLS] [PAD] ... [PAD] [SEP] [PAD] ... [PAD] [SEP] [PAD] ... like choice_ids, expanded as input_ids\n",
    "    ref_tokens = [cls_token_id]\n",
    "    for token in input_ids[choice_idx, 1:]:\n",
    "        if token == sep_token_id: # We keep only the separators, else we use padding\n",
    "            ref_tokens += [sep_token_id]\n",
    "        else:\n",
    "            ref_tokens += [ref_token_id]\n",
    "    \n",
    "    \n",
    "    ref_input_ids = input_ids.clone() # Copy input_ids to avoid modifying the original\n",
    "    ref_input_ids[choice_idx] = torch.tensor(ref_tokens, dtype=torch.long) # Replace the choice input with the baseline\n",
    "    if ids: return ref_input_ids\n",
    "\n",
    "    ref_token_type_ids = torch.zeros_like(ref_input_ids)\n",
    "    return embed_layer(ref_input_ids, token_type_ids=ref_token_type_ids) # preembed for multi-layer attribution\n",
    "\n",
    "# Quickly compute\n",
    "# ref_inputs_embeds = get_baseline(tokenizer, input_ids, choice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "d98ac9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)\n",
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "tensor([[0.0098, 0.1466, 0.8436]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ref_logits = forward_func_MpC(ref_inputs_embeds, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "print(ref_logits)\n",
    "logits = forward_func_MpC(inputs_embeds, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c7106",
   "metadata": {},
   "source": [
    "We now make a convenience function for getting the token attributions, summing over the internal model dimension to get an overal attribution for each position (token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "671fbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_attributions(forward_func, choice_idx, inputs_embeds, ref_inputs_embeds, attention_masks, token_type_ids=None, n_steps=50, attr_layer=None):\n",
    "\n",
    "    if cfg.debug: print(inputs_embeds.shape)\n",
    "    \n",
    "    # LayerIntegratedGradients for attribution\n",
    "    if attr_layer is None: ig = IntegratedGradients(forward_func)\n",
    "    else: ig = LayerIntegratedGradients(forward_func, attr_layer) # Note that inputs_embeds may be input_ids if doing layer attribution\n",
    "\n",
    "    # Compute attributions for chosen index \n",
    "    # (Captum wants [num_choices, seq_len] and gives attr shape [choices, seq_len, layer_output_dim])\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        baselines=ref_inputs_embeds,\n",
    "        additional_forward_args=(attention_masks, token_type_ids),\n",
    "        target=choice_idx,              # Target the chosen answer, takes idx [0,target] of forward_func output\n",
    "        n_steps=n_steps,                # Number of steps for approximation\n",
    "        return_convergence_delta=True   # Error estimate\n",
    "    )\n",
    "    # Captum attributes every input wrt the target idx. We only care about the choice_idx attributions, \n",
    "    # but we have to pass in all the mulitple choice alternatives in order to get the correct softmax output.\n",
    "    # This significantly increases compute time! So it is yet another reason to just use the logits, and then only pass in the\n",
    "    # true index. But I am not sure if that really makes sense, since differentiation is important in MpC...\n",
    "    if cfg.debug: print(attributions.shape)\n",
    "\n",
    "    # Sum across layer_output_dim to get token-level importance\n",
    "    choice_token_attributions = attributions[choice_idx].sum(dim=-1)  # shape: [num_choices, seq_len]\n",
    "    choice_token_attributions = choice_token_attributions / torch.norm(choice_token_attributions)  # Normalize\n",
    "\n",
    "    if cfg.debug: print('Attributions per token at choice_idx:', choice_token_attributions)\n",
    "    \n",
    "    return choice_token_attributions, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db902418",
   "metadata": {},
   "source": [
    "Here comes the main computation block, which can take several minutes with no GPU depending on the model, input length and number of steps. With our toy example, it should take seconds even on a laptop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 768])\n",
      "Logit shape from forward pass: torch.Size([1, 450])\n",
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "torch.Size([3, 12, 768])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000,  0.1608,  0.0638,  0.0459,  0.3812,  0.3064, -0.4257, -0.5432,\n",
      "        -0.1402,  0.0000,  0.4805, -0.0442], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Main computation block, can take several minutes depending on the model, input length and number of steps.\n",
    "# If cfg.debug is True, it will print intermediate results to indicate progress.\n",
    "# choice_idx = 2 # Change this here if you want to attribute with choices different from the one the model chose\n",
    "\n",
    "token_attributions, delta = get_token_attributions(\n",
    "    forward_func_MpC, choice_idx, inputs_embeds, ref_inputs_embeds, attention_masks, token_type_ids, n_steps=50\n",
    ")\n",
    "\n",
    "# Convert input_ids to readable tokens for the choice_idx\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d43274",
   "metadata": {},
   "source": [
    "At last, we use Captum to make a neat visualisation of the attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "702aba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.29</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,         # word attributions\n",
    "                        torch.max(probs),           # prediction probability\n",
    "                        torch.argmax(probs),        # predicted class\n",
    "                        ground_truth_idx,           # ground truth class\n",
    "                        choice_idx,                 # attributing to this class\n",
    "                        token_attributions.sum(),   # summed attribution score\n",
    "                        tokens,                     # tokens for the question and choice\n",
    "                        delta,                      # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # Save return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7eae6",
   "metadata": {},
   "source": [
    "Indeed, the words \"France\" and \"Paris\" seems to be the main positive contributors, though the attribution score is very low (in fact, when changing the question format, it sometimes becomes negative, which is strange. It could be because of some error in the code, or indicate that the choice of baseline is not good,). The model credence is also not amazing at about 85%, so a better model should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "ef0dcd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2696, 0.7304]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quickly verifying that the baseline is not preferred to the actual input, only for testing\n",
    "test_inp = torch.stack([inputs_embeds[choice_idx], ref_inputs_embeds[choice_idx]], dim=0)\n",
    "forward_func_MpC(test_inp, torch.ones((2,11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33fb68",
   "metadata": {},
   "source": [
    "#### Adapting for another model\n",
    "\n",
    "This short section adds nothing new, but collects the above in a convenience function and shows how to apply it to a slightly bigger, and better, RoBERTa model. The only required modification is that RoBERTa does not use type_token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "759937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing new here, just importing the RoBERTa model. We use the question and choices from before, just reformatted to a RACE-like format\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "\n",
    "roberta_question = \"What is the capitol of France?\" # we try to format the question like the model traning data\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"LIAMF-USP/roberta-large-finetuned-race\"\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f51d2",
   "metadata": {},
   "source": [
    "Let's also test logits rather than softmax, since it is a lot faster, and this model is bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "df330165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_MpC_logit(inputs_embeds, attention_mask, token_type_ids=None): \n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. Here with logits, so only pass the relevant choice input (shape [1, seq_len]).\n",
    "    Some models (eg. RoBERTa) do not use token_type_ids, hence the optional parameter.\"\"\"\n",
    "\n",
    "    # Reshaping for multiple choice compatibility\n",
    "    inputs_embeds = inputs_embeds.unsqueeze(0) # If attributing the embedding layer, this will be different, see below.\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logit = model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logit shape from forward pass:\", logit.shape)\n",
    "    return logit # normalizing to differentiate choices (this is maybe not a good idea, it causes to much coupling / instability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb992cf",
   "metadata": {},
   "source": [
    "All-in-one convenience wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "563bb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(forward_func, tokenizer, embed_layer, question=question, choices=choices, ground_truth_idx=ground_truth_idx, type_tokens=False, n_steps=50, ids=False, attr_layer=None):\n",
    "    \"\"\"A quick function to do everything we have implemented so far: tokenize, make prediction, compute attributions and visualize.\"\"\"\n",
    "    # Tokenize for multiple choice and get input ids, attention masks, WITHOUT token type ids (if token_types=False), \n",
    "    # and make prediction to get logits and choice index\n",
    "    input_ids, inputs_embeds, attention_masks, token_type_ids, probs, choice_idx = get_encoding_and_predict(question, choices, tokenizer, embed_layer, type_tokens=type_tokens, print_output=cfg.debug)\n",
    "    ref_inputs_embeds = get_baseline(tokenizer, input_ids, choice_idx, embed_layer, ids=ids)\n",
    "    \n",
    "    if type_tokens: token_type_ids = token_type_ids[choice_idx].unsqueeze(0)\n",
    "    if ids: inputs_embeds = input_ids # the ids-bool sends embeds to ids if True; this is not important\n",
    "\n",
    "    # Compute attributition at choice idx\n",
    "    token_attributions, delta = get_token_attributions(\n",
    "        forward_func, 0, inputs_embeds[choice_idx].unsqueeze(0), ref_inputs_embeds[choice_idx].unsqueeze(0), attention_masks[choice_idx].unsqueeze(0), token_type_ids, n_steps=n_steps, attr_layer=attr_layer\n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "    vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,         # word attributions\n",
    "                        torch.max(probs),           # prediction probability\n",
    "                        torch.argmax(probs),        # predicted class\n",
    "                        ground_truth_idx,           # ground truth class\n",
    "                        choice_idx,            # attributing to this class\n",
    "                        token_attributions.sum(),   # summed attribution score\n",
    "                        tokens,                     # tokens for the question and choice\n",
    "                        delta,                      # convergence delta\n",
    "    )\n",
    "\n",
    "    viz.visualize_text([vis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc406a",
   "metadata": {},
   "source": [
    "Main computation block, which could take several minutes on a laptop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "0678691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Question: What is the capitol of France?\n",
      "Predicted Answer: 2) Paris\n",
      "Probabilities: tensor([[8.4838e-03, 9.4364e-04, 9.9057e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 14, 1024])\n",
      "Logit shape from forward pass: torch.Size([1, 50])\n",
      "Logit shape from forward pass: torch.Size([1, 1])\n",
      "Logit shape from forward pass: torch.Size([1, 1])\n",
      "torch.Size([1, 14, 1024])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000,  0.1721,  0.3265,  0.0897,  0.3923,  0.2591, -0.1325, -0.0565,\n",
      "         0.4022, -0.3390, -0.3900,  0.0660,  0.4252,  0.0000],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1.22</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġthe                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġcap                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> itol                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġof                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ĠFrance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Paris                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #pad                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute attributions for RoBERTa\n",
    "embed_layer = model.roberta.embeddings\n",
    "do_everything(forward_func_MpC_logit, tokenizer, embed_layer, question=roberta_question) # note: the attribution goes negative around n=20, here it is unstable: bad result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d86ad",
   "metadata": {},
   "source": [
    "Notice that the model credence is way better (99%) than the previous model's. Alas, it is still not great on the FCI, and preliminary testing indicates that it does not really make sense to talk about misconceptions here since the credences are so low (that is, the answers are almost random; I am not convinced that this model has developed any internal understanding of physics). This is not surprising, since the FCI questions are very different from the training data, and since the model is so small as to run on a laptop CPU.\n",
    "\n",
    "Also, this attribution really does not make much sense, or at the very least, is not at all illuminating. I guess the attribution score is simply not a good score for multiple choice, since it has nothing to do with the actual model prediction with respect to the other choices, but only wrt the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfffc79",
   "metadata": {},
   "source": [
    "**Note:** The results do not really make sense, as I get negative attr scores with bert. This indicates that the baseline is more likely than the correct result; either my BERT is really very bad, or something is off. With the Roberta, another strange thing ocurs; paris gives a negative contribution, instead the separator contributes. Look it over later.\n",
    "\n",
    "I guess this supports the idea to hold filler words fixed in the baseline. It could be that the words \"what is the capitol of france\" are worse than just \"[pad] ... [pad] of france\" for giving the wanted answer. But this is really strange and not great. This is in line with the original paper definition as \"absence of a feature\".\n",
    "\n",
    "The AS goes negative when n_steps increases, but positive for low values, indicating that there are \"better\" configs close to the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac226ab",
   "metadata": {},
   "source": [
    "### Application II: Comparing the Embedding Spaces of Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5716736",
   "metadata": {},
   "source": [
    "For this, we need attribution for embedding models. To determine which words in a text chunk are most important for the placement of that chunk in embedding space, we must fist choose a metric on the embedding space and some reference vector. **I think this method is potentially useful for comparing the embedding spaces of different models: In particular, would the same words in a sentence be the most important for the sentence placement for several models? If we fine-tune one model for increased resolution on physics related concepts, could we use attribution to see increased sensitivity to physics related words?**\n",
    "\n",
    "Concretely, we do the following:\n",
    "\n",
    "> Given a text chunk, we run it thorugh the model to get the corresponding embedding vector. Then, we want to see which words in the chunk we should tune to get the largest change in embedding position; that is, we attribute with respect to the cosine similarity between the true output and the perturbed output.\n",
    "\n",
    "Ideally, one might want to use a separate vector as a reference, but there is no canonical choice. If the dataset gives a natural pairing, such as in a MpC/QA-dataset, one could experiment with using the question as the reference vector. \n",
    "\n",
    "The following code gives the general procedure, but I am not sure how to test whether it is a sensible/useful approach. This is a particular example of how embedding models may cause difficulty in interpretability, since the output is non-interpretable. There are also other potential issues: i) In high dimensional spaces, most randomly chosen vectors are essentially orthogonal, so unless the perturbations are very small, we might just get orthogonality for all our perturbations, giving basically no information at all. ii) In principle, the model may weight different directions in embedding space differently, so that a small change along an axis which is very densely populated with different semantic concepts could be more meaningful than a large movement in a sparse direction. The first seems to be disproven by my results, but I have not investigated the second, as I suspect it will be model and input specific; the application in bold above could shed light on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ea479d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer # note that the class structures of Autos are slightly different when accessing embedding layers etc.\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\" # embedding model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71730d0c",
   "metadata": {},
   "source": [
    "Since we use transformers, we have to do the mean pooling from last hidden state to embedding vector ourselves. We essentially borrow the code from sentence_transformers, though we also manually run the first preembed layer in order to get differentiable numbers for captum (instead of the discrete input_ids). \n",
    "\n",
    "(In the follwing, we will for clarity use variable name suffices _embeds to denote prembeddings and _embeddings to denote mean-pooled model outputs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - from sentence transformers, nothing new here;\n",
    "# essentially averaging over all positions to get the actual singe embedding for the whole chunk\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Uses preembedded input for IG\n",
    "def embed_tokens(inputs_embeds, attention_mask=None): # Uses global model\n",
    "    model_output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask) # attn mask only needed for padding tokens on acausal models, and we have a single batch\n",
    "    # Perform pooling and normalize\n",
    "    embedding = mean_pooling(model_output, attention_mask)\n",
    "    return F.normalize(embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc4724",
   "metadata": {},
   "source": [
    "We now create the preembedded input, preembedded baseline for IG, and the reference embeddings which will be used in the custom forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "229f2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 384])\n",
      "torch.Size([1, 8, 384]) torch.Size([1, 8]) torch.Size([1, 8, 384]) torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Compute baseline embedding\n",
    "text = ['This is a test sentence.']\n",
    "target_idx = 0 # only one input\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt') # Tokenize\n",
    "\n",
    "embed_layer = model.embeddings # get embed layer\n",
    "\n",
    "# Model input\n",
    "input_ids = encoding['input_ids'] # Get input ids, shape: [batch_size, seq_len]\n",
    "attention_masks = encoding['attention_mask'] # Get attention masks (for each batch, same shape as input_ids)\n",
    "inputs_embeds = embed_layer(input_ids)\n",
    "\n",
    "if cfg.debug: print(inputs_embeds.shape)\n",
    "\n",
    "# Baseline embedding for IG\n",
    "baseline_token_id = tokenizer.pad_token_id \n",
    "baseline_ids = torch.full_like(input_ids, baseline_token_id)\n",
    "baseline_embeds = embed_layer(baseline_ids)\n",
    "\n",
    "# Reference embedding for distance computation\n",
    "reference_embeddings = embed_tokens(inputs_embeds, attention_masks) # shape: [batch_size, embedding_dim]\n",
    "\n",
    "if cfg.debug: print(inputs_embeds.shape, attention_masks.shape, baseline_embeds.shape, reference_embeddings.shape, ) # Debug shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a36a95",
   "metadata": {},
   "source": [
    "Custom forward function as described earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c9efae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_cos_sim(inputs_embeds, attention_mask=None, reference_embeddings=reference_embeddings):\n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients for cosine similarity to reference embeddings.\"\"\"\n",
    "    \n",
    "    embeddings = embed_tokens(inputs_embeds, attention_mask)\n",
    "    sim = F.cosine_similarity(reference_embeddings, embeddings, dim=1) # Compute cosine similarity to baseline embedding\n",
    "\n",
    "    if cfg.debug: print(\"Similarity:\", sim.shape, sim)\n",
    "    return sim # tensor of shape [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380e85",
   "metadata": {},
   "source": [
    "We then do the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "81ccdc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: torch.Size([1]) tensor([1.0000], grad_fn=<SumBackward1>)\n",
      "Original similarity: 1.0000001192092896\n",
      "Similarity: torch.Size([50]) tensor([0.0680, 0.0681, 0.0684, 0.0688, 0.0694, 0.0701, 0.0709, 0.0718, 0.0729,\n",
      "        0.0740, 0.0752, 0.0764, 0.0778, 0.0794, 0.0814, 0.0839, 0.0873, 0.0922,\n",
      "        0.1005, 0.1165, 0.1387, 0.1696, 0.2188, 0.2881, 0.3692, 0.4593, 0.5581,\n",
      "        0.6665, 0.7760, 0.8620, 0.9150, 0.9454, 0.9638, 0.9756, 0.9834, 0.9887,\n",
      "        0.9924, 0.9949, 0.9966, 0.9977, 0.9985, 0.9990, 0.9994, 0.9996, 0.9998,\n",
      "        0.9999, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "Similarity: torch.Size([1]) tensor([0.0679])\n",
      "Similarity: torch.Size([1]) tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Compute model prediction\n",
    "start_dist = forward_func_cos_sim(inputs_embeds, attention_masks, reference_embeddings)\n",
    "print(\"Original similarity:\", start_dist.item()) # Print original distance, should be 1 with our choice of reference embeddings\n",
    "\n",
    "ig = IntegratedGradients(forward_func_cos_sim)\n",
    "\n",
    "# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=inputs_embeds,\n",
    "    baselines=baseline_embeds,\n",
    "    additional_forward_args=(attention_masks, reference_embeddings, ), \n",
    "    n_steps=50,  # Number of steps for approximation\n",
    "    return_convergence_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be5b7c",
   "metadata": {},
   "source": [
    "Note that the change in cosine similarity seems pretty smooth and is not dense around 0, indicating that 50 steps are more than enough even in high dimensional spaces. \n",
    "\n",
    "At last, we visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8b963398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-wise attributions:\n",
      "       [CLS] : 0.0034\n",
      "        this : 0.0786\n",
      "          is : 0.1174\n",
      "           a : 0.1469\n",
      "        test : 0.2484\n",
      "    sentence : 0.1549\n",
      "           . : 0.0995\n",
      "       [SEP] : 0.0836\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0.93</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> test                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sentence                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get word tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[target_idx])\n",
    "\n",
    "# Aggregate attribution scores across embedding dimension like before (Current shape of attributions: [1, seq_len, emb_dim])\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [seq_len]\n",
    "\n",
    "# Printing token + attribution\n",
    "print(\"Token-wise attributions:\")\n",
    "for token, score in zip(tokens, token_attributions):\n",
    "    print(f\"{token:>12} : {score.item():.4f}\")\n",
    "\n",
    "# We also use the captum-visualisation since it is pretty, though most variables will be irrelevant\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,        # token-wise attributions\n",
    "                        0, # prediction probability (not relevant for distance attribution)\n",
    "                        0, # predicted class            -\"-\n",
    "                        0, # ground truth class         -\"-\n",
    "                        0, # attributing to this class  -\"-\n",
    "                        token_attributions.sum(),  # summed attribution score (NA)\n",
    "                        tokens,                    # tokens for the question and choice\n",
    "                        delta,                     # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # get return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77c3ca",
   "metadata": {},
   "source": [
    "Note that the table scores are set to zero, as no label prediction is made here. Still, I suppose we see that the by far most important word is \"test\", which seems intuitively reasonable, though gives only a quick proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84999749",
   "metadata": {},
   "source": [
    "## Application III: Finding Where Facts are Stored in the Model\n",
    "\n",
    "### Layer-wise attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90998b7",
   "metadata": {},
   "source": [
    "Often, we may want to use input attribution to get a rough idea of where a feature may be located in a model. For instance, the fact that Paris is the capitol of France may be stored in some MLP-layer of the model (3blue1brown, likely simplified). Once the layer is identified, the specific neurons may ideally be localized with intervention methods (see notebook 1). This section makes an attempt at implementing such a rough fact-layer search on an encoder for multiple choice, but the results are not great.\n",
    "\n",
    "First of all, we have seen that there are issues with the attribution score on multiple choice questions. A better way to do this, then, might be to just take the norm of the embedding space contribution of each layer wrt to the question and use the maximal one as the most contributing layer. This is more in line with typical MI, and I belive it might be implemented in part 1.\n",
    "\n",
    "Still, the following does demonstrate some Captum, though it is essentially only an application of the BERT-tutorial on a MpC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "a9b9e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realoding toy BERT model for piecewise notebook runs - nothing new here\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affc17e",
   "metadata": {},
   "source": [
    "When doing layer-wise attribution, we want to leave the option open to attribute with respect to the preembed layer, in which case we should feed the input_ids rather than the inputs_embeds to the forward function (otherwise, the preembed layer is skipped in the forward pass of the model). This is generally allowed for Layer integrated gradients, but not for Integrated gradients. Since it is also simpler, we will do this from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24443b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_MpC_ids(input_ids, attention_mask, token_type_ids=None): \n",
    "    \"\"\"Custom forward pass for Captum Layer Integrated Gradients. Like before, but without preembedding.\"\"\"\n",
    "    # shape fixes as explained before since running multiple choices\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=input_ids, # note: ids now (transformers-models take either)\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007c5fb",
   "metadata": {},
   "source": [
    "We can then attribute with respect to the preembedding layer, like in the BERT Captum-demo (we made Layer attribution an option in the functions from before, so this is just a quick application):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "a68ca0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit shape from forward pass: torch.Size([1, 3])\n",
      "Question: The capital of France is...\n",
      "Predicted Answer: 2) Paris\n",
      "Probabilities: tensor([[0.0098, 0.1466, 0.8436]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 12])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "torch.Size([1, 12, 768])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000, -0.2914, -0.0339,  0.0222,  0.0984, -0.4163, -0.2280, -0.4292,\n",
      "        -0.5261,  0.0000, -0.4663,  0.0000])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-2.27</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_layer = model.bert.embeddings\n",
    "attr_layer = embed_layer # if attr wrt different layer, change here\n",
    "\n",
    "do_everything(forward_func_MpC_ids, tokenizer, embed_layer, type_tokens=True, ids=True, attr_layer=attr_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93adda",
   "metadata": {},
   "source": [
    "Note the negative overall attribution score; this should indicate that the initial embedding layer did not prefer this choice above the baseline, and that only later layers corrected for this (though previous results indicate that there might be issues with the attribution score). If facts are mostly contained in MLP-layers (ref. 3blue1brown), that makes perfect sense: The basic lookuptable of the preembed layer should not be able to answer MpC-questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b0409",
   "metadata": {},
   "source": [
    "We could now loop over and attribute wrt each MLP-layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "92e4518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-1.08</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.66</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.81</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.25</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1.37</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.01</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1.05</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.81</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.13</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.47</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.22</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.84)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> capital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg.debug = False # avoiding clutter\n",
    "for i in range(len(model.bert.encoder.layer)):\n",
    "    print(i)\n",
    "    attr_layer = model.bert.encoder.layer[i].output # if the hidden mlp layer, use .intermediate. Can also add .dense to do pre-(layerNorm/activation)\n",
    "    do_everything(forward_func_MpC_ids, tokenizer, embed_layer, type_tokens=True, n_steps=10, ids=True, attr_layer=attr_layer)\n",
    "    # break # To avoid long runtime, comment if you want to run all this for some reason\n",
    "cfg.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245a8c4",
   "metadata": {},
   "source": [
    "The highest attribution score is for layer 9, so this might be the best candidate for a \"fact-layer\". However, there are multiple serious issues: i) there is likely an issue with the attribution score; previous examples show negative scores for the preferred choice, and it is highly unstable when adding softmax instead of using the logits, like we do here. ii) the attribution scores are spread out over multiple layers, iii) the hypothesis that a \"Paris is the capitol of France\"-MLP-layer exists is not very well founded rendering this project potentially hopeless. The method should preferably be tested on a model where features have already been identified, to see if we can re-identify them with attribution+ablation, but we have no such model available in a reasonable size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a53911",
   "metadata": {},
   "source": [
    "The better way to do this, is with Captum's Layer Conductance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0143be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From here on down, I have not been too careful / reviewed yet - there might be errors (but the code runs).\n",
    "\n",
    "from captum.attr import LayerConductance\n",
    "\n",
    "# For plotting heatmaps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Returning to smaller BERT-model\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# Absurd way to get choice idx for testing only\n",
    "# layer = model.bert.embeddings\n",
    "# choice_idx = do_everything(forward_func_softmax, layer, model=model, tokenizer=tokenizer, token_types=True)\n",
    "\n",
    "# Forward func taking embeds\n",
    "def forward_func_softmax_embed(input_emb, attention_mask=None):\n",
    "    \n",
    "    input_emb = input_emb.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=input_emb,\n",
    "        attention_mask=attention_mask,\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits.softmax(dim=1)\n",
    "\n",
    "# From demo, to get input_embs and ref_input_embs (running ids through emb layer)\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 768]) torch.Size([3, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "ref_token_type_ids = torch.zeros_like(token_type_ids)  # Reference token type ids, all zeros for BERT\n",
    "position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=device)  # Position ids for the input sequence\n",
    "ref_position_ids = torch.zeros_like(position_ids)  # Reference position ids, all zeros for BERT\n",
    "\n",
    "input_embs, ref_input_embs = construct_whole_bert_embeddings(\n",
    "    input_ids, ref_input_ids, token_type_ids=token_type_ids, \n",
    "    ref_token_type_ids=ref_token_type_ids, position_ids=position_ids, ref_position_ids=ref_position_ids\n",
    ") # maybe zero vector is better baseline?\n",
    "\n",
    "if cfg.debug: print(input_embs.shape, ref_input_embs.shape)\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients # Not tested yet\n",
    "\n",
    "# IntegratedGradients\n",
    "def get_token_attributions_for_model(layer, forward_func, choice_idx, input_embs, ref_input_embs, attention_mask, token_type_ids=None, n_steps=50):\n",
    "\n",
    "    if cfg.debug: print(input_embs.shape)\n",
    "    \n",
    "    # LayerIntegratedGradients for attribution\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    # Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, layer_output_dim])\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=input_embs,\n",
    "        baselines=ref_input_embs,\n",
    "        additional_forward_args=(attention_mask, token_type_ids),\n",
    "        target=choice_idx,  # Target the chosen answer, uses [0,target]\n",
    "        n_steps=n_steps,  # Number of steps for approximation\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    # Sum across embedding dimensions to get token-level importance\n",
    "    token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "    token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "\n",
    "    if cfg.debug: \n",
    "        print('Token attributions:', token_attributions.shape)\n",
    "        print('Attributions per token at choice_idx:', token_attributions[choice_idx])\n",
    "    \n",
    "    return token_attributions, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7dad3e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 1: -1.2461051386781037\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 2: -1.2421367736533284\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 3: -1.315762855578214\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 4: -1.0917498202761635\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 5: -1.1474649207666516\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 6: -1.0865842448547482\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 7: -1.0264790678629652\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 8: -1.032434618100524\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 9: -1.0169967371039093\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 10: -1.081912085879594\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 11: -1.2508521545678377\n",
      "Logits from forward pass: torch.Size([1, 153])\n",
      "Layer 12: -0.9224346876144409\n"
     ]
    }
   ],
   "source": [
    "layer_attrs = [] # Empty list to store attributions for each layer\n",
    "\n",
    "for i in range(model.config.num_hidden_layers): # attrubuting tokenwise for each layer\n",
    "    lc = LayerConductance(forward_func_softmax_embed, model.bert.encoder.layer[i])\n",
    "    layer_attributions = lc.attribute(inputs=input_embs, baselines=ref_input_embs, additional_forward_args=(attention_masks,), \\\n",
    "                                      target=choice_idx, n_steps=50, ) # reduced steps to speed up, sacrificing acc, though the trend holds up\n",
    "    layer_attrs.append(summarize_attributions(layer_attributions).cpu().detach().tolist())\n",
    "    print(f\"Layer {i+1}:\", np.sum(layer_attrs[i][choice_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "36170947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3, 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if cfg.debug: print(np.array(layer_attrs).shape)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "xticklabels=tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "yticklabels=list(range(1, model.config.num_hidden_layers+1))\n",
    "ax = sns.heatmap(np.array(layer_attrs)[:,choice_idx,:], xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2) #, annot=True\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725dbcb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "In this way we can try to use LIG to indicate what layer has stored the fact \"Paris is the capital of France\" (alas, there are 24 (!) layers in this model, so we can only do this for a toy model - even here, all layers will by random chance prefer some alternative, so without tracing the full information path thorough the model it is nontrivial to do this in practice). \n",
    "\n",
    "The naive approach to find the imporant MLP-layers is given below (not nearly enough steps to get very accurate attributions; the delta-s should be examined if making a through analysis). We see that certain layers (Mainly MLP output-layer 0) give a strong push towards the correct answer, while the later layers are hardly involved or pushes in the opposite direction, and one could try to find a Paris node/feature in the eary layers of the model with patching.\n",
    "\n",
    "However, there is a much better way to do this with LayerConductance in Captum, which will follow below.\n",
    "\n",
    "<details>\n",
    "  <summary> Obvious application [click to expand]  </summary>\n",
    "\n",
    "Take a toy BERT model which can answer very easy MpCs (like capitals, or easier) and find the \"Paris in France\" **layer**, then use patching from Frederik to find specific **nodes**. I will not pursue this unless I have time in the end, since it is a straightforward (though not very likely to be successful) application of the tools in this notebook and the patching-notebook.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a2cda",
   "metadata": {},
   "source": [
    "### Multi-layer attribution\n",
    "\n",
    "Wrong caption:\n",
    "As a final step with IG-attribution from captum, let's look at how to attribute the whole model rather than only a layer. The only difference is that we provide embeddings instead of ids to the IG attribution method (which is also an option with LIG, but ids are easier), since the ids are discrete/not diffable. So we need to tokenize *and* word/position/type-embed before inputting, and get the attribution for the layers that are not the embedding layers (which are essentially look-up-tables anyway/not the most interesting part of a transformer).\n",
    "\n",
    "This is really the most interesting for the application described above, while the layer attribution is most useful for coarse feature search before intervening on specific nodes in that layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ce8d1",
   "metadata": {},
   "source": [
    "\n",
    "We can also do LayerConductance to see attr-scores for each token across layers - this is for locating the feature to a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73e1ba",
   "metadata": {},
   "source": [
    "These results make no sense, crap. In fact, they seem to be inverted by the plot below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af172f",
   "metadata": {},
   "source": [
    "It is also possible to look closely at specific tokens and how they are processed by each layer, see the very end of the [first Captum BERT demo](https://captum.ai/tutorials/Bert_SQUAD_Interpret) (This should be pretty much similar for our case when treating only the choice idx and using only one attribution rather than start and end). Attn interp can be seen in the attention notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5138c0a",
   "metadata": {},
   "source": [
    "## Appendix: Some Attention Visualisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

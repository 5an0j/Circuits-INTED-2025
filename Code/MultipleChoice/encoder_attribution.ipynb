{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37453546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8189023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval() # Evaluation mode\n",
    "model.zero_grad() # Clear summed gradients\n",
    "\n",
    "# Mean Pooling - To compute embeddings (from huggingface)\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text, model=model, tokenizer=tokenizer):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt') # Tokenize\n",
    "\n",
    "    with torch.no_grad(): # Compute token embeddings\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling and normalize\n",
    "    embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "# Compute baseline embedding\n",
    "baseline = ['Dette er et eksempel'] # Baseline input\n",
    "baseline_embedding = embed(baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0b5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance from baseline embedding\n",
    "def predict_baseline_distance(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)\n",
    "\n",
    "    embedding = mean_pooling(output, inputs['attention_mask'])\n",
    "    embedding = F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    return F.cosine_similarity(embedding, baseline_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe401a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_attention_mask(input_ids): # we don't need masking, so just 1's\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "ref_input_ids = \n",
    "\n",
    "input_ids = tokenizer.encode(baseline[0])\n",
    "input_ids = torch.tensor([input_ids], device=device)\n",
    "\n",
    "attention_mask = construct_attention_mask(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6435d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m lig = LayerIntegratedGradients(predict_baseline_distance, model.embeddings)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m attributions, delta = \u001b[43mlig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Convert attributions to list for visualization\u001b[39;00m\n\u001b[32m      8\u001b[39m attribution_scores = attributions.sum(dim=-\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Sum over embedding dim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:521\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_ids = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.forward_func, \u001b[33m\"\u001b[39m\u001b[33mdevice_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m inputs_layer = \u001b[43m_forward_layer_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m input_layer_list: List[Tuple[Tensor, ...]]\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# if we have one output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:210\u001b[39m, in \u001b[36m_forward_layer_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_layer_eval\u001b[39m(\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\u001b[39;00m\n\u001b[32m    202\u001b[39m     forward_fn: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     grad_enabled: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    209\u001b[39m ) -> Union[Tuple[Tensor, ...], List[Tuple[Tensor, ...]]]:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_forward_layer_eval_with_neuron_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[6]: For 3rd argument expected `Module` but got\u001b[39;49;00m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  `ModuleOrModuleList`.\u001b[39;49;00m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_neuron_selector\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:506\u001b[39m, in \u001b[36m_forward_layer_eval_with_neuron_grads\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[39m\n\u001b[32m    503\u001b[39m grad_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gradient_neuron_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_enabled\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(grad_enabled):\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     saved_layer = \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m device_ids = _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[32m    514\u001b[39m \u001b[38;5;66;03m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\gradient.py:339\u001b[39m, in \u001b[36m_forward_layer_distributed_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[39m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    336\u001b[39m         all_hooks.append(\n\u001b[32m    337\u001b[39m             single_layer.register_forward_hook(hook_wrapper(single_layer))\n\u001b[32m    338\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m output = \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    348\u001b[39m output = cast(Tensor, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\anaconda3\\envs\\embed\\Lib\\site-packages\\captum\\_utils\\common.py:588\u001b[39m, in \u001b[36m_run_forward\u001b[39m\u001b[34m(forward_func, inputs, target, additional_forward_args)\u001b[39m\n\u001b[32m    585\u001b[39m inputs = _format_inputs(inputs)\n\u001b[32m    586\u001b[39m additional_forward_args = _format_additional_forward_args(additional_forward_args)\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m output = \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch.futures.Future):\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output.then(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x.value(), target))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mpredict_baseline_distance\u001b[39m\u001b[34m(inputs, token_type_ids, position_ids, attention_mask)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      4\u001b[39m     output = model(inputs, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m embedding = mean_pooling(output, \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      7\u001b[39m embedding = F.normalize(embedding, p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.cosine_similarity(embedding, baseline_embedding)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(predict_baseline_distance, model.embeddings)\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                    additional_forward_args=(attention_mask,),\n",
    "                                    return_convergence_delta=True)\n",
    "\n",
    "# Convert attributions to list for visualization\n",
    "attribution_scores = attributions.sum(dim=-1).squeeze(0)  # Sum over embedding dim\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# Visualization\n",
    "viz_data_records = [viz.VisualizationDataRecord(\n",
    "    word_attributions=attribution_scores.cpu().detach().numpy(),\n",
    "    pred_prob=0.0,\n",
    "    pred_class=\"\",\n",
    "    true_class=\"\",\n",
    "    attr_class=\"\",\n",
    "    attr_score=attribution_scores.sum().item(),\n",
    "    raw_input=tokens,\n",
    "    convergence_score=delta.item()\n",
    ")]\n",
    "\n",
    "viz.visualize_text(viz_data_records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "We perform word attribution for the embedding layer of a BertForMultipleChoice-model on the FCI using Captum.\n",
    "\n",
    "Overview:\n",
    "1) set up model/tokenizer\n",
    "2) define custom forward pass, returning a vector with length (choices)\n",
    "3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee078050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question and choices\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Berlin\", \"Madrid\", \"Paris\"]\n",
    "ground_truth_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug atm)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune'\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(input_ids, attention_mask, token_type_ids):\n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. Captum wants [batch size, seq_len] while mulitple choice/classification\n",
    "    models expect [batch size, num_choices, seq_len]. We return the logits for each choice.\"\"\"\n",
    "\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 5])\n",
      "--------------------------------------------------\n",
      "Question: Two metal balls are the same size but one weighs twice as much as the other. The balls are dropped from the roof of a single story building at the same instant of time. The time it takes the balls to reach the ground below will be:\n",
      "Predicted Answer: 1) About half as long for the lighter ball as for the heavier one\n",
      "tensor([[8.5428, 8.5783, 3.6178, 7.4995, 7.3461]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get input ids, attn masks, token type ids and baseline for Captum attribution methods.\n",
    "\n",
    "# Tokenize for multiple choice and get input ids, attention masks, and token type ids\n",
    "encoding = tokenizer(\n",
    "    [question] * len(choices),\n",
    "    choices,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids      = encoding[\"input_ids\"]      # shape: [choices, seq_len]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "# Compute model prediction and get choice index\n",
    "logits = forward_func(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "choice_idx = torch.argmax(logits).item()\n",
    "\n",
    "print('-'*50)\n",
    "print('Question:', question)\n",
    "print('Predicted Answer:', f'{choice_idx})', choices[choice_idx])\n",
    "\n",
    "if cfg.debug: print(logits)\n",
    "\n",
    "\n",
    "# Create baseline input: [CLS] [PAD] ... [PAD] [SEP] [PAD] ... [PAD] [SEP] for choice index expanded as input_ids\n",
    "ref_token_id = tokenizer.pad_token_id       # padding\n",
    "sep_token_id = tokenizer.sep_token_id       # sepatator\n",
    "cls_token_id = tokenizer.cls_token_id       # start of sequence\n",
    "\n",
    "ref_tokens = [cls_token_id]\n",
    "for i, token in enumerate(input_ids[choice_idx, 1:]):\n",
    "    if token == sep_token_id:\n",
    "        ref_tokens += [sep_token_id]\n",
    "    else:\n",
    "        ref_tokens += [ref_token_id]\n",
    "\n",
    "ref_input_ids = torch.tensor(ref_tokens, dtype=torch.long).expand_as(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 66]) torch.Size([5, 66]) torch.Size([5, 66])\n",
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Logits from forward pass: torch.Size([1, 250])\n",
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Token attributions: torch.Size([5, 66])\n",
      "Attributions per token at choice_idx: tensor([ 0.1777, -0.0664,  0.1701,  0.2068, -0.0117,  0.1075,  0.1174,  0.0669,\n",
      "         0.0917, -0.0483,  0.0640,  0.1771,  0.0833,  0.1283,  0.0704, -0.0792,\n",
      "        -0.0006, -0.0668,  0.0262,  0.0692, -0.0239,  0.0951,  0.0349,  0.0808,\n",
      "        -0.1655, -0.0013, -0.0053,  0.1474, -0.0500, -0.1920, -0.0729, -0.1197,\n",
      "         0.0561,  0.2201, -0.0320,  0.1257,  0.0912,  0.0641,  0.0221,  0.0254,\n",
      "         0.1329, -0.0117,  0.1142,  0.0886, -0.0128,  0.0718, -0.0201, -0.1662,\n",
      "         0.0271,  0.0223,  0.1301, -0.0011,  0.2600,  0.2209,  0.0982,  0.1799,\n",
      "        -0.0386, -0.1950,  0.3507,  0.0265, -0.0633, -0.2807, -0.1878,  0.1227,\n",
      "        -0.1256, -0.0590])\n"
     ]
    }
   ],
   "source": [
    "if cfg.debug: print(input_ids.shape, attention_mask.shape, token_type_ids.shape)\n",
    "\n",
    "# LayerIntegratedGradients for attribution\n",
    "layer = model.bert.embeddings\n",
    "lig = LayerIntegratedGradients(forward_func, layer)\n",
    "\n",
    "# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=ref_input_ids,\n",
    "    additional_forward_args=(attention_mask, token_type_ids),\n",
    "    target=choice_idx,  # Target the chosen answer, uses [0,target]\n",
    "    n_steps=50,  # Number of steps for approximation\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum across embedding dimensions to get token-level importance\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "if cfg.debug: print('Token attributions:', token_attributions.shape)\n",
    "\n",
    "print('Attributions per token at choice_idx:', token_attributions[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.38)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0.01</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> metal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> same                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> size                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> weighs                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> twice                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dropped                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> roof                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> single                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> story                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> same                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> instant                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> takes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reach                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ground                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> below                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> half                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> long                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lighter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ball                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> heavier                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "choice_attributions = token_attributions[choice_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        choice_attributions,                        # word attributions\n",
    "                        torch.max(torch.softmax(logits, dim=1)),    # prediction probability\n",
    "                        torch.argmax(logits),                       # predicted class\n",
    "                        ground_truth_idx,                           # ground truth class\n",
    "                        str(choice_idx),                            # attributing to this class\n",
    "                        attributions.sum(),                         # summed attribution score\n",
    "                        tokens,                                     # tokens for the question and choice\n",
    "                        delta,                                      # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # get return object to avoid passing the vis object to the ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

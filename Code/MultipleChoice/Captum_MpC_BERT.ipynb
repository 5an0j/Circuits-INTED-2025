{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "We use Integrated Gradients from Captum to compute word attributions on the FCI for a BERT-model fine-tuned for answering mulitple choice questions. Specifically, we want to see whether the emphasised words indicate that the model exhibits the same common misconceptions as humans. In the example below, even if the model gives a common wrong answer (eg. that the pushing car exerts a larger force, or that the heavier object exerts a larger force) we can perhaps use attribution to see precisely which words throw the model off, and then modify the prompt to find prcisely what the model's misconception is. Alas, I cannot compute the gradients for large models with long inputs on my laptop, and our MpC-models are not trained for long physics questions, so the models used here are not sufficiently good to get particualrily interesting results. The code should be easily modifiable for different models, and in the end, we show as an example how to modify it for RoBERTa.\n",
    "\n",
    "Example question: *A large truck breaks down out on the road and receives a push back into town by a small compact car. After the car reaches the constant cruising speed at which its driver wishes to push the truck:*\n",
    "\n",
    "0) *The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.*\n",
    "1) *The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.*\n",
    "2) *The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.*\n",
    "3) *The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot push back against the car. The truck is pushed forward simply because it is in the way of the car.*\n",
    "4) *Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee078050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally, we would make a reference file with all FCI questions and choices which one can simply import, but for now:\n",
    "\n",
    "# Question and choices (early for easy changes)\n",
    "question = \"Two metal balls are the same size but one weighs twice as much as the other. The balls \\\n",
    "are dropped from the roof of a single story building at the same instant of time. The time it takes \\\n",
    "the balls to reach the ground below will be:\"\n",
    "\n",
    "choices = [\"About half as long for the heavier ball as for the lighter one\",\n",
    "           \"About half as long for the lighter ball as for the heavier one\",\n",
    "           \"About the same for both balls\",\n",
    "           \"Considerably less for the heavier ball, but not necessarily half as long\",\n",
    "           \"Considerably less for the lighter ball, but not necessarily half as long\"]\n",
    "\n",
    "ground_truth_idx = 2 # Correct answer index, just for reference in the final plot\n",
    "\n",
    "# # Question and choices\n",
    "# question = [\"A large truck breaks down out on the road and receives a push back into town by a small compact car. \\\n",
    "# After the car reaches the constant cruising speed at which its driver wishes to push the truck:\"]\n",
    "\n",
    "# choices = [\n",
    "# \"The amount of force with which the car pushes on the truck is equal to that with which the truck pushes back on the car.\",\n",
    "# \"The amount of force with which the car pushes on the truck is smaller than that with which the truck pushes back on the car.\",\n",
    "# \"The amount of force with which the car pushes on the truck is greater than that with which the truck pushes back on the car.\",\n",
    "# \"The car's engine is running so the car pushes against the truck, but the truck's engine is not running so the truck cannot \\\n",
    "# push back against the car. The truck is pushed forward simply because it is in the way of the car.\",\n",
    "# \"Neither the car nor the truck exert any force on the other. The truck is pushed forward simply because it is in the way of the car.\"\n",
    "# ]\n",
    "\n",
    "# ground_truth_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug atm)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'jonastokoliu/multi_choice_bert-base-uncased_swag_finetune' # Pretrained (bad) MpC model\n",
    "model = BertForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(input_ids, attention_mask, token_type_ids=None): \n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. Captum wants [batch size, seq_len] while mulitple choice/classification\n",
    "    models expect [batch size, num_choices, seq_len]. We return the logits for each choice.\n",
    "    Some models (eg. RoBERTa) do not use token_type_ids, hence the optional parameter.\"\"\"\n",
    "\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Question: Two metal balls are the same size but one weighs twice as much as the other. The balls are dropped from the roof of a single story building at the same instant of time. The time it takes the balls to reach the ground below will be:\n",
      "Predicted Answer: 1) About half as long for the lighter ball as for the heavier one\n",
      "tensor([[8.5428, 8.5783, 3.6178, 7.4995, 7.3461]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get input ids, attn masks, token type ids and baseline for Captum attribution methods.\n",
    "def get_encoding_and_predict(question, choices, tokenizer, token_types=True, print_output=True):\n",
    "    \"\"\"Tokenizes the question and choices and makes prediction, returning input_ids, attention_masks, and token_type_ids, \n",
    "    logits and choice index. token_type_ids is None if token_types is False. The input ids are shaped as [num_choices, seq_len].\"\"\"\n",
    "    \n",
    "    # Tokenize for multiple choice and get input ids, attention masks, and token type ids\n",
    "    encoding = tokenizer(\n",
    "        [question] * len(choices),\n",
    "        choices,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]               # shape: [choices, seq_len]\n",
    "    attention_masks = encoding[\"attention_mask\"]    # -\"-\n",
    "    if token_types:\n",
    "        token_type_ids = encoding[\"token_type_ids\"] # -\"-\n",
    "    else:\n",
    "        token_type_ids = None\n",
    "\n",
    "\n",
    "    # Compute model prediction and get choice index\n",
    "    logits = forward_func(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "    choice_idx = torch.argmax(logits).item()\n",
    "\n",
    "    if print_output:\n",
    "        print('Question:', question)\n",
    "        print('Predicted Answer:', f'{choice_idx})', choices[choice_idx])\n",
    "    if cfg.debug: print('Logits:', logits) # To gauge model confidence\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids, logits, choice_idx\n",
    "\n",
    "\n",
    "# Baseline input for Integrated Gradients\n",
    "def get_baseline(tokenizer, input_ids, choice_idx):\n",
    "    ref_token_id = tokenizer.pad_token_id   # padding\n",
    "    sep_token_id = tokenizer.sep_token_id   # sepatator\n",
    "    cls_token_id = tokenizer.cls_token_id   # start of sequence\n",
    "\n",
    "    # Create baseline input: [CLS] [PAD] ... [SEP] ... [PAD] [SEP] for choice index expanded as input_ids\n",
    "    ref_tokens = [cls_token_id]\n",
    "    for i, token in enumerate(input_ids[choice_idx, 1:]):\n",
    "        if token == sep_token_id: # We keep only the separators, else we use padding\n",
    "            ref_tokens += [sep_token_id]\n",
    "        else:\n",
    "            ref_tokens += [ref_token_id]\n",
    "        \n",
    "    return torch.tensor(ref_tokens, dtype=torch.long).expand_as(input_ids)\n",
    "\n",
    "\n",
    "# Compute\n",
    "input_ids, attention_masks, token_type_ids, logits, choice_idx = get_encoding_and_predict(question, choices, tokenizer)\n",
    "ref_input_ids = get_baseline(tokenizer, input_ids, choice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main computation block, can take several minutes depending on the model, input length and number of steps.\n",
    "# If cfg.debug is True, it will print intermediate results giving a progress bar.\n",
    "\n",
    "def get_token_attributions_for_layer(layer, forward_func, choice_idx, input_ids, ref_input_ids, attention_mask, token_type_ids=None, n_steps=50):\n",
    "\n",
    "    if cfg.debug: print(input_ids.shape)\n",
    "    \n",
    "    # LayerIntegratedGradients for attribution\n",
    "    lig = LayerIntegratedGradients(forward_func, layer)\n",
    "\n",
    "    # Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, layer_output_dim])\n",
    "    attributions, delta = lig.attribute(\n",
    "        inputs=input_ids,\n",
    "        baselines=ref_input_ids,\n",
    "        additional_forward_args=(attention_mask, token_type_ids),\n",
    "        target=choice_idx,  # Target the chosen answer, uses [0,target]\n",
    "        n_steps=n_steps,  # Number of steps for approximation\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    # Sum across embedding dimensions to get token-level importance\n",
    "    token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "    token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "\n",
    "    if cfg.debug: \n",
    "        print('Token attributions:', token_attributions.shape)\n",
    "        print('Attributions per token at choice_idx:', token_attributions[choice_idx])\n",
    "    \n",
    "    return token_attributions, delta\n",
    "\n",
    "\n",
    "# Compute for embedding layer\n",
    "layer = model.bert.embeddings\n",
    "token_attributions, delta = get_token_attributions_for_layer(\n",
    "    layer, forward_func, choice_idx, input_ids, ref_input_ids, attention_masks, token_type_ids\n",
    ")\n",
    "\n",
    "# Get the attributions for the chosen answer and convert input ids to readable tokens\n",
    "choice_attributions = token_attributions[choice_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "702aba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.38)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>-2.87</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> metal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> same                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> size                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> weighs                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> twice                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dropped                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> roof                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> single                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> story                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> same                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> instant                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> takes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balls                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reach                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ground                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> below                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> half                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> long                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lighter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ball                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> heavier                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        choice_attributions,                        # word attributions\n",
    "                        torch.max(torch.softmax(logits, dim=1)),    # prediction probability\n",
    "                        torch.argmax(logits),                       # predicted class\n",
    "                        ground_truth_idx,                           # ground truth class\n",
    "                        str(choice_idx),                            # attributing to this class\n",
    "                        token_attributions.sum(),                   # summed attribution score\n",
    "                        tokens,                                     # tokens for the question and choice\n",
    "                        delta,                                      # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # Save return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ea568",
   "metadata": {},
   "source": [
    "Note the small attribution score (the model is not good enough to answer, so the words hardly matter), though the foci make sense. Also, the attribution takes ages (with no gpu for the differentiation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517a5f8",
   "metadata": {},
   "source": [
    "Let's do the same with RoBERTa. The only real modification is that it does not use type_token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question and choices\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Berlin\", \"Madrid\", \"Paris\"]\n",
    "ground_truth_idx = 2\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"LIAMF-USP/roberta-large-finetuned-race\"\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Question: What is the capital of France?\n",
      "Predicted Answer: 2) Paris\n",
      "tensor([[-0.4239, -2.1472,  4.5936]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 13])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 150])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Token attributions: torch.Size([3, 13])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000, -0.0511,  0.1184,  0.0214,  0.5090, -0.0007,  0.2750,  0.6631,\n",
      "        -0.3209, -0.2486, -0.0994, -0.1841,  0.0000])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.68</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġthe                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġcapital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġof                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ĠFrance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #pad                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def do_everything(forward_func, layer, question=question, choices=choices, ground_truth_idx=ground_truth_idx, model=model, tokenizer=tokenizer, token_types=False):\n",
    "    \"\"\"A quick function to do everything: tokenize, make prediction, compute attributions and visualize.\"\"\"\n",
    "    # Tokenize for multiple choice and get input ids, attention masks, WITHOUT token type ids (token_types=False), \n",
    "    # and make prediction to get logits and choice index\n",
    "    input_ids, attention_masks, token_type_ids, logits, choice_idx = get_encoding_and_predict(question, choices, tokenizer, token_types=token_types)\n",
    "    ref_input_ids = get_baseline(tokenizer, input_ids, choice_idx)\n",
    "\n",
    "    # Compute attributition for embedding layer\n",
    "    token_attributions, delta = get_token_attributions_for_layer(\n",
    "        layer, forward_func, choice_idx, input_ids, ref_input_ids, attention_masks, token_type_ids\n",
    "    )\n",
    "\n",
    "    # Get the attributions for the chosen answer and convert input ids to readable tokens\n",
    "    choice_attributions = token_attributions[choice_idx]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "\n",
    "    # Visualize\n",
    "    choice_attributions = token_attributions[choice_idx]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "    vis = viz.VisualizationDataRecord(\n",
    "                            choice_attributions,                        # word attributions\n",
    "                            torch.max(torch.softmax(logits, dim=1)),    # prediction probability\n",
    "                            torch.argmax(logits),                       # predicted class\n",
    "                            ground_truth_idx,                           # ground truth class\n",
    "                            str(choice_idx),                            # attributing to this class\n",
    "                            token_attributions.sum(),                   # summed attribution score\n",
    "                            tokens,                                     # tokens for the question and choice\n",
    "                            delta,                                      # convergence delta\n",
    "    )\n",
    "\n",
    "    viz.visualize_text([vis])\n",
    "\n",
    "\n",
    "# Compute attributions wrt's RoBERTa embeddings layer\n",
    "layer = model.roberta.embeddings\n",
    "do_everything(forward_func, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf97a0",
   "metadata": {},
   "source": [
    "As expected, 'capital' and 'France' (and also the question mark...) are most important for the model choice. Note that it has very strong convivtion here, as opposed to the difficult FCI questions BERT faced. The RoBERTa model is better, but slightly too large for running comfortably with long inputs on my laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9ac4c",
   "metadata": {},
   "source": [
    "Since a change in input can increase ALL logits, we might want to use the softmaxed output to get at the words most inmportant for *differentiating* the choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da94c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Question: What is the capital of France?\n",
      "Predicted Answer: 2) Paris\n",
      "tensor([[-0.4239, -2.1472,  4.5936]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 13])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 150])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Logits from forward pass: torch.Size([1, 3])\n",
      "Token attributions: torch.Size([3, 13])\n",
      "Attributions per token at choice_idx: tensor([ 0.0000, -0.0667, -0.2955, -0.2296,  0.2589,  0.0310,  0.4667,  0.3114,\n",
      "        -0.1623, -0.2125, -0.3267, -0.2078,  0.0000])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>-0.53</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġis                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġthe                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġcapital                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġof                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ĠFrance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #pad                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_func_softmax(input_ids, attention_mask, token_type_ids): # softmax wrapper\n",
    "    return forward_func(input_ids, attention_mask, token_type_ids).softmax(dim=1)\n",
    "\n",
    "do_everything(forward_func_softmax, layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

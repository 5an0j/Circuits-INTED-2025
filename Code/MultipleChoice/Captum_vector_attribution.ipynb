{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "We perform word attribution for the embedding layer of a BertForMultipleChoice-model on the FCI using Captum.\n",
    "\n",
    "Overview:\n",
    "1) set up model/tokenizer\n",
    "2) define custom forward pass, returning a vector with length (choices)\n",
    "3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee078050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question and choices\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"Berlin\", \"Madrid\", \"Paris\"]\n",
    "ground_truth_idx = 2\n",
    "\n",
    "# Question and choices (overwrite)\n",
    "# question = \"Two metal balls are the same size but one weighs twice as much as the other. The balls \\\n",
    "# are dropped from the roof of a single story building at the same instant of time. The time it takes \\\n",
    "# the balls to reach the ground below will be:\"\n",
    "\n",
    "# choices = [\"About half as long for the heavier ball as for the lighter one\",\n",
    "#            \"About half as long for the lighter ball as for the heavier one\",\n",
    "#            \"About the same for both balls\",\n",
    "#            \"Considerably less for the heavier ball, but not necessarily half as long\",\n",
    "#            \"Considerably less for the lighter ball, but not necessarily half as long\"]\n",
    "\n",
    "# ground_truth_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForMultipleChoice\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug atm)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"LIAMF-USP/roberta-large-finetuned-race\"\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(input_ids, attention_mask, token_type_ids=None):\n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients. Captum wants [batch size, seq_len] while mulitple choice/classification\n",
    "    models expect [batch size, num_choices, seq_len]. We return the logits for each choice.\"\"\"\n",
    "\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    if token_type_ids is not None: token_type_ids = token_type_ids.unsqueeze(0)\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    ).logits\n",
    "\n",
    "    if cfg.debug: print(\"Logits from forward pass:\", logits.shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from forward pass: torch.Size([1, 5])\n",
      "--------------------------------------------------\n",
      "Question: Two metal balls are the same size but one weighs twice as much as the other. The balls are dropped from the roof of a single story building at the same instant of time. The time it takes the balls to reach the ground below will be:\n",
      "Predicted Answer: 3) Considerably less for the heavier ball, but not necessarily half as long\n",
      "tensor([[-0.7417,  1.9860, -3.9501,  5.7018,  5.5120]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get input ids, attn masks, token type ids and baseline for Captum attribution methods.\n",
    "\n",
    "# Tokenize for multiple choice and get input ids, attention masks, and token type ids\n",
    "encoding = tokenizer(\n",
    "    [question] * len(choices),\n",
    "    choices,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids      = encoding[\"input_ids\"]        # shape: [choices, seq_len]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "# token_type_ids = encoding[\"token_type_ids\"] # No token type ids for RoBERTa\n",
    "\n",
    "\n",
    "# Compute model prediction and get choice index\n",
    "logits = forward_func(input_ids, attention_mask=attention_mask)\n",
    "choice_idx = torch.argmax(logits).item()\n",
    "\n",
    "print('-'*50)\n",
    "print('Question:', question)\n",
    "print('Predicted Answer:', f'{choice_idx})', choices[choice_idx])\n",
    "\n",
    "if cfg.debug: print(logits)\n",
    "\n",
    "# Note that RoBERTa has two separators between question and choice\n",
    "# Create baseline input: [CLS] [PAD] ... [PAD] [SEP] [SEP] [PAD] ... [PAD] [SEP] for choice index expanded as input_ids\n",
    "ref_token_id = tokenizer.pad_token_id       # padding\n",
    "sep_token_id = tokenizer.sep_token_id       # sepatator\n",
    "cls_token_id = tokenizer.cls_token_id       # start of sequence\n",
    "\n",
    "ref_tokens = [cls_token_id]\n",
    "for i, token in enumerate(input_ids[choice_idx, 1:]):\n",
    "    if token == sep_token_id:\n",
    "        ref_tokens += [sep_token_id]\n",
    "    else:\n",
    "        ref_tokens += [ref_token_id]\n",
    "\n",
    "ref_input_ids = torch.tensor(ref_tokens, dtype=torch.long).expand_as(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 68]) torch.Size([5, 68])\n",
      "Logits from forward pass: torch.Size([1, 5])\n",
      "Logits from forward pass: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "if cfg.debug: print(input_ids.shape, attention_mask.shape)\n",
    "\n",
    "# LayerIntegratedGradients for attribution\n",
    "layer = model.roberta.embeddings\n",
    "lig = LayerIntegratedGradients(forward_func, layer)\n",
    "\n",
    "# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=ref_input_ids,\n",
    "    additional_forward_args=(attention_mask),\n",
    "    target=choice_idx,  # Target the chosen answer, uses [0,target]\n",
    "    n_steps=50,  # Number of steps for approximation\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum across embedding dimensions to get token-level importance\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [num_choices, seq_len]\n",
    "token_attributions = token_attributions / torch.norm(token_attributions)  # Normalize\n",
    "if cfg.debug: print('Token attributions:', token_attributions.shape)\n",
    "\n",
    "print('Attributions per token at choice_idx:', token_attributions[choice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>0.68</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġthe                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġcapital                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġof                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ĠFrance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Paris                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #pad                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The RoBERa tokenizer returns ugly word-tokens - #s is the start-token, the #/s is a separator and the dotted G is a space\n",
    "\n",
    "choice_attributions = token_attributions[choice_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[choice_idx])\n",
    "\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        choice_attributions,                        # word attributions\n",
    "                        torch.max(torch.softmax(logits, dim=1)),    # prediction probability\n",
    "                        torch.argmax(logits),                       # predicted class\n",
    "                        ground_truth_idx,                           # ground truth class\n",
    "                        str(choice_idx),                            # attributing to this class\n",
    "                        token_attributions.sum(),                   # summed attribution score\n",
    "                        tokens,                                     # tokens for the question and choice\n",
    "                        delta,                                      # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # get return object to avoid passing the vis object to the ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

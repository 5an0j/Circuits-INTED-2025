{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6026db04",
   "metadata": {},
   "source": [
    "Integrated gradients on the cosine-distance of prompt to its embedding. That is, we run the model to obtain the original embedding and differentiate the cosine similarity between the mean pooled model output to this vector.\n",
    "\n",
    "Alternative approach for paired inputs: use distance to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f07e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config (only for debug atm)\n",
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e778438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "# model # Uncomment to print architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee078050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Mean Pooling - To compute embeddings (from huggingface)\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def embed_tokens(input_ids, attention_mask=None): # Uses global model\n",
    "    model_output = model(input_ids, attention_mask) # attn mask only needed for padding tokens on acausal models, and we have a single input\n",
    "    # Perform pooling and normalize\n",
    "    embedding = mean_pooling(model_output, attention_mask)\n",
    "    return F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "# Compute baseline embedding\n",
    "text = ['This is a test sentence.']\n",
    "target_idx = 0 # only one input\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt') # Tokenize\n",
    "\n",
    "# Model input\n",
    "input_ids = encoding['input_ids'] # Get input ids, shape: [batch_size, seq_len]\n",
    "attention_masks = encoding['attention_mask'] # Get attention masks (for each batch, same shape as input_ids)\n",
    "\n",
    "# Baseline embedding for IG\n",
    "baseline_token_id = tokenizer.pad_token_id \n",
    "baseline_ids = torch.full_like(input_ids, baseline_token_id)\n",
    "\n",
    "# Reference embedding for distance computation\n",
    "reference_embeddings = embed_tokens(input_ids, attention_masks) # shape: [batch_size, embedding_dim]\n",
    "\n",
    "if cfg.debug: print(input_ids.shape, attention_masks.shape, baseline_ids.shape, reference_embeddings.shape, ) # Debug shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a4da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_cos_sim(input_ids, attention_mask=None, reference_embeddings=reference_embeddings):\n",
    "    \"\"\"Custom forward pass for Captum Integrated Gradients for distance...\"\"\"\n",
    "    \n",
    "    embeddings = embed_tokens(input_ids, attention_mask)\n",
    "    sim = F.cosine_similarity(reference_embeddings, embeddings, dim=1) # Compute cosine distance with baseline embedding\n",
    "\n",
    "    if cfg.debug: print(\"Similarity:\", sim.shape)\n",
    "    return sim # tensor of shape [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f699a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: torch.Size([1])\n",
      "Original distance: 1.0\n",
      "Similarity: torch.Size([1])\n",
      "Similarity: torch.Size([1])\n",
      "Similarity: torch.Size([50])\n",
      "Similarity: torch.Size([1])\n",
      "Similarity: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Compute model prediction\n",
    "start_dist = forward_func_cos_sim(input_ids, attention_masks, reference_embeddings)\n",
    "print(\"Original distance:\", start_dist.item()) # Print original distance\n",
    "\n",
    "layer = model.embeddings # Get the embedding layer\n",
    "lig = LayerIntegratedGradients(forward_func_cos_sim, layer)\n",
    "\n",
    "# Compute attributions for chosen index (Captum wants [choices, seq_len] and gives attr shape [choices, seq_len, embedding_dim])\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=input_ids,\n",
    "    baselines=baseline_ids,\n",
    "    additional_forward_args=(attention_masks, reference_embeddings, ), \n",
    "    n_steps=50,  # Number of steps for approximation\n",
    "    return_convergence_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b350c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-wise attributions:\n",
      "       [CLS] : 0.0366\n",
      "        this : 0.0988\n",
      "          is : 0.1103\n",
      "           a : 0.1100\n",
      "        test : 0.2474\n",
      "    sentence : 0.1514\n",
      "           . : 0.0873\n",
      "       [SEP] : 0.0725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0.91</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> test                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sentence                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get word tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[target_idx])\n",
    "\n",
    "# Step 2: Aggregate attribution scores (across embedding dimension)\n",
    "# Shape of attributions: [1, seq_len, emb_dim]\n",
    "token_attributions = attributions.sum(dim=-1).squeeze(0)  # shape: [seq_len]\n",
    "\n",
    "# Step 3: Print token + attribution\n",
    "print(\"Token-wise attributions:\")\n",
    "for token, score in zip(tokens, token_attributions):\n",
    "    print(f\"{token:>12} : {score.item():.4f}\")\n",
    "\n",
    "\n",
    "vis = viz.VisualizationDataRecord(\n",
    "                        token_attributions,        # token-wise attributions\n",
    "                        0, # prediction probability (not relevant for distance attribution)\n",
    "                        0, # predicted class            -\"-\n",
    "                        0, # ground truth class         -\"-\n",
    "                        0, # attributing to this class  -\"-\n",
    "                        token_attributions.sum(),  # summed attribution score (NA)\n",
    "                        tokens,                    # tokens for the question and choice\n",
    "                        delta,                     # convergence delta\n",
    ")\n",
    "\n",
    "visualisation = viz.visualize_text([vis]) # get return object to avoid passing the vis object to the ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c2cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de6ed10",
   "metadata": {},
   "source": [
    "Okay, but we want to attribute not layer-wise, but for the whole model. That might just be as simple as using IG from captum, though we will have to pre-embed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be modified:\n",
    "\n",
    "# From demo, to get input_embs and ref_input_embs (running ids through emb layer)\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

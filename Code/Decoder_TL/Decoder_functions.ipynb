{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0ea2b7",
   "metadata": {},
   "source": [
    "# Collection of functions for decoders\n",
    "\n",
    "This is a collections I have written as part of my work with mechanical interpretability with decoders (primarily gpt2-small) and TransformerLens. Some are implementations of methods, others are visualizing results more flexibly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f85980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some necessary imports\n",
    "import circuitsvis as cv\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from functools import partial\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens.utils as utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0be88",
   "metadata": {},
   "source": [
    "Visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3b884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attn_patterns(model, text, layers, compact=True):\n",
    "    ''' \n",
    "    Visualize attention patterns for a chosen number of layers.\n",
    "    '''\n",
    "    str_tokens = model.to_str_tokens(text)\n",
    "    logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "    if compact:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "    \n",
    "    else:\n",
    "        for layer in layers:\n",
    "            attention_pattern = cache[\"pattern\", layer]\n",
    "            \n",
    "            display(cv.attention.attention_heads(tokens=str_tokens, attention=attention_pattern))\n",
    "\n",
    "def imshow_patching_result(model, patching_results, corrupted_prompt, corrupted_answer):\n",
    "    '''\n",
    "    Visualizes the logit differences caused by activation patching in a heat map. If the answer has more than one token, \"patching_results\" must be a list of results.\n",
    "    '''\n",
    "    if isinstance(patching_results, list):\n",
    "        len_ans = len(patching_results)\n",
    "        for i in range(len_ans):\n",
    "            tokens = model.to_str_tokens(corrupted_prompt+corrupted_answer)\n",
    "            labels = [f'{token}_{index}' for index, token in enumerate(tokens)][:-len(patching_results)+i]\n",
    "            if not torch.all(patching_results[i] == 0):\n",
    "                #print(patching_results[i])\n",
    "                px.imshow(patching_results[i].detach(), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", x=labels, labels={\"x\": \"Position\", \"y\": \"Layer\"}, title=\"Patching Results\").show()\n",
    "    else:\n",
    "        tokens = model.to_str_tokens(corrupted_prompt)\n",
    "        labels = [f'{token}_{index}' for index, token in enumerate(tokens)]\n",
    "        px.imshow(patching_results.detach(), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", x=labels, labels={\"x\": \"Position\", \"y\": \"Layer\"}, title=\"Patching Results\").show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f7e89",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64e068",
   "metadata": {},
   "source": [
    "We now want to demonstrate the activation patching technique on a small problem. We will first run a forward pass on the clean prompt and cache the activations. Then, we will run a forward pass on the corrupted run and at each layer and position exchange the corrupted activations with the clean activations from the cache. To do this, we use the run_with_hooks method from TransformerLens and make a suitable hook function, activation_patching_hook. We make an activationg_patching function that returns the results, as well as an activation_patching_mult function to use if the answer has more than a single token. In the latter case, we need to run through the multiple separate times, one for each token in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(resid_pre, hook, position, clean_cache):\n",
    "    clean_activation = clean_cache[hook.name]\n",
    "    resid_pre[:, position, :] = clean_activation[:, position, :]\n",
    "    return resid_pre\n",
    "\n",
    "def activation_patching(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer):\n",
    "    '''\n",
    "    Performs activation patching of the clean prompt onto the corrupted prompt. The prompts must have the same number of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    model: The transformer lens model\n",
    "    clean_prompt (str): The clean prompt we will patch from\n",
    "    corrupted_prompt (str): The corrupted prompt we will patch onto\n",
    "    clean_answer (str): The answer (or next prediction) of the clean prompt\n",
    "    corrupted_answer (str): The answer (or next prediction) of the corrupted prompt\n",
    "\n",
    "    Returns: \n",
    "    patching_results (list[tensor[layers, positions]]): The logit difference after patching\n",
    "    patched_logits (list[tensor[num_tokens, logits]]): The logits of the tokens after patching\n",
    "    '''\n",
    "    \n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_prompt)\n",
    "    corrupted_logits = model(corrupted_prompt)\n",
    "    print(\"Clean answer:\",clean_answer)\n",
    "    print(\"Corrupted answer:\", corrupted_answer)\n",
    "    clean_index = model.to_single_token(clean_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "\n",
    "    clean_diff = clean_logits[0, -1, clean_index] - clean_logits[0, -1, corrupted_index]\n",
    "    corrupted_diff = corrupted_logits[0, -1, clean_index] - corrupted_logits[0, -1, corrupted_index]\n",
    "\n",
    "    clean_tokens = model.to_tokens(clean_prompt)\n",
    "    corrupted_tokens= model.to_tokens(corrupted_prompt)\n",
    "    num_positions = len(model.to_tokens(clean_prompt)[0])\n",
    "\n",
    "    assert len(clean_tokens[0]) == len(corrupted_tokens[0]), \"The prompts must have the same number of tokens.\"\n",
    "\n",
    "   \n",
    "    patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "    for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(num_positions):\n",
    "            # We use a temporary hook with functool.partial to patch at each position\n",
    "            temp_hook = partial(activation_patching_hook, position=position, clean_cache=clean_cache)\n",
    "            # We then run the model with hooks as usual\n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, \n",
    "                                                  fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), temp_hook)])\n",
    "            \n",
    "            # We then calculate the logit difference\n",
    "            patched_diff = (patched_logits[0, -1, clean_index] - patched_logits[0, -1, corrupted_index]).detach()\n",
    "            # We then store the result in the patching_result tensor, normalizing it\n",
    "            if abs(clean_diff-corrupted_diff) < 1e-16:\n",
    "                patching_result[layer, position] = 0\n",
    "            else:\n",
    "                patching_result[layer, position] = abs((patched_diff - corrupted_diff) / (clean_diff - corrupted_diff))\n",
    "    print(patched_logits.shape)\n",
    "    return patching_result, patched_logits\n",
    "\n",
    "def activation_patching_mult(model, clean_prompt, corrupted_prompt, clean_answer, corrupted_answer):\n",
    "    ''' \n",
    "    Performs activation patching on prompts with multi-word answers by using separate run-throughs.\n",
    "    The answers must have the same number of tokens\n",
    "    '''\n",
    "    patching_result = []\n",
    "    patched_logits = []\n",
    "    clean_answers_tokens = model.to_str_tokens(clean_answer)[1:]\n",
    "    corrupted_answers_tokens = model.to_str_tokens(corrupted_answer)[1:]\n",
    "    print(\"Number of run throughs:\", len(clean_answers_tokens))\n",
    "    for i in range(len(clean_answers_tokens)):\n",
    "        p_result, p_logits = activation_patching(model, clean_prompt, corrupted_prompt, \n",
    "                                                 clean_answers_tokens[0], corrupted_answers_tokens[0])\n",
    "        patching_result.append(p_result)\n",
    "        patched_logits.append(p_logits)\n",
    "        clean_prompt += clean_answers_tokens[0]\n",
    "        clean_answers_tokens = clean_answers_tokens[1:]\n",
    "        corrupted_prompt += corrupted_answers_tokens[0]\n",
    "        corrupted_answers_tokens = corrupted_answers_tokens[1:]\n",
    "\n",
    "    return patching_result, patched_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2adeec",
   "metadata": {},
   "source": [
    "As an example, we choose the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a8713",
   "metadata": {},
   "source": [
    "Miscellaneous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265813ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, logits, num_top=5):\n",
    "    ''' \n",
    "    Returns the most likely words and their corresponding probabilities.\n",
    "    '''\n",
    "    if isinstance(logits, list):\n",
    "        for i in range(len(logits)):\n",
    "            logits_i = logits[i][0, -1, :]\n",
    "            probs = logits_i.softmax(dim=-1)\n",
    "            top_probs, top_indices = probs.topk(num_top)\n",
    "            top_tokens = [model.to_string(index.item()) for index in top_indices]\n",
    "\n",
    "            for token, prob in zip(top_tokens, top_probs):\n",
    "                print(f'{token:>15}: {prob.item():.4f}, ', end='')\n",
    "            print()\n",
    "    \n",
    "    else:\n",
    "        logits_i = logits[0, -1, :]\n",
    "        probs = logits_i.softmax(dim=-1)\n",
    "        top_probs, top_indices = probs.topk(num_top)\n",
    "        top_tokens = [model.to_string(index.item()) for index in top_indices]\n",
    "\n",
    "        for token, prob in zip(top_tokens, top_probs):\n",
    "            print(f'{token}: {prob.item():.4f}, ', end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

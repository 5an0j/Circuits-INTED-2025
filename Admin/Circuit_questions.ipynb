{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2b931a",
   "metadata": {},
   "source": [
    "# CIRCUITS GROUP – Question Sheet\n",
    "\n",
    "**This file has very brief notes for the introductory questions posed by Ane on Teams written off the top of my head/based on only general research. There may be errors, so double check if using**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96795a18",
   "metadata": {},
   "source": [
    "Part A. Foundations (What is a circuit?)\n",
    "* In your own words, what is a circuit inside a transformer model?\n",
    "(Try to include what kinds of components it involves, and what it helps the model do.)\n",
    "  * See circuits_intro.ipynb\n",
    "\n",
    "* What does it mean if the same circuit is activated by many different inputs?\n",
    "   * If the inputs have a common feature, the circuit is likely a detection algortihm for this feature.\n",
    "  \n",
    "* How is a circuit different from just “a neuron firing”?\n",
    "(Think in terms of what makes a circuit more informative than a single unit.)\n",
    "   * A circuit is an algorithm computing a feature from previous features. This says something about the info processing of the model, that is, HOW it comnputes something. Ideally, we want to understand circuits so well that we can reimplement the algorithms ourselves (the idea of a Microscope AI is this taken to the extreeme).\n",
    "\n",
    "* What kind of model behavior can circuits help explain?\n",
    "(Give one concrete example, like **copying, structure, or comparing**.)\n",
    "   * it would be nice if all model behaviour could be reduced to understandable circuits of ever-increasingly complex features. Examples of known circuits in convnets are curvedetectors, dog head detectors etc, and in decoder-style transformers, induction circuits for pattern recognintion. There is less academic work on encoder-style mech interp, but the \"detection-style\" circuits are almost certainly also present here.\n",
    "\n",
    "* If two texts activate the same circuit but use different words, what does that suggest about what the model is focusing on?\n",
    "How might circuits relate to what ends up in the final embedding?\n",
    "(Think: does the circuit affect what meaning gets encoded?)\n",
    "   * The concepts/meaning rather than specific words, or non-semantic structures in the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a97f7",
   "metadata": {},
   "source": [
    "\n",
    "Part B: Literature and Tools\n",
    "\n",
    "* What is one open-source tool or library for exploring circuits in language models? What kinds of models does it support?\n",
    "(Example answers: TransformerLens, EasyTransformer, etc.)\n",
    "   * one can gererally use Transformers (for Jina AI). TransformerLens and EasyTransformer are made for decoder-style transformers, but the basic functionality is likely to carry over. Perhaps we could make a wrapper to make more advanced functionaity/visualisations work for non-causal models.\n",
    "\n",
    "* Find a published blog post or paper about a known circuit (e.g. induction heads, copying patterns). What behavior does the circuit support, and how was it discovered?\n",
    "   * Induction heads gives (long distance) pattern recognition. It was discovered in a toy model, and since it is a previous token head followed by an induction head it is easy to find in complex models once you know what to look for. All (sufficiently complex/two layer attention with enough training) have induction heads and they give a phase transition in training.\n",
    "\n",
    "* In one sentence, describe what “path patching” is and why it's used in circuit analysis.\n",
    "    * Path patching is slightly different than activation patching, and more complex to implement iirc (there are two variants, one where you look at all paths and one where you look only at previous node paths). **I will look at the details when done answering the rest**.\n",
    "\n",
    "* Skim the Anthropic “Zoom in: an introduction to circuits” post. Summarise 3 takeaway ideas you think are the most interesting.\n",
    "    * see circuis_intro.ipynb\n",
    "\n",
    "* Look at a code demo or tutorial from one circuit analysis tool. What are two pieces of information the tool lets you visualize or extract from the model?\n",
    "   * TransformerLens: attention patterns, logit differences after patching, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56ef22",
   "metadata": {},
   "source": [
    " \n",
    "Part C. Circuits and Representation Space\n",
    "\n",
    "* If 500 articles cluster tightly and activate the same early-layer circuit, what does this suggest about the role of that circuit in shaping representations?\n",
    "    * That circuit likely detects some important common feature of the articles. The residual stream positions are hardly updated after the early layers, so we may focus on this circuit to understand what the model is deeming important.\n",
    "\n",
    "* If a set of short stories all activate the same circuit and follow similar paths in embedding space, and they turn out to share a plot structure, what might the circuit be encoding?\n",
    "    *  that plot structure. To make sure, use feature visualisation with GD (genereate input to maximally activate that circuit) and test on examples with same structure but varying other variables.\n",
    "\n",
    "* If two articles end up near each other in space but use different internal circuits, what does that suggest about the assumptions we make when clustering?\n",
    "   * either, the model does not deem this important, or these two circuits are themselves part of a unioning-circuit (an XOR-gate, essentially, like \"directional dog head\"-detectors unioning to give a general dog head detector), or **(a lot of other possibilities, slighly unclear to me what 'the assumptions we make when clustering' means)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c47730",
   "metadata": {},
   "source": [
    "\n",
    "Part D. Circuits in Education and Meaning **This part is interesting, and not something covered in mech interp research afaik; I save these and will look at them if relevant later**\n",
    "* You find a shared circuit in student answers that reflects a reasoning strategy (e.g., define-before-use). How might this circuit differ between novice and expert explanations?\n",
    "\n",
    "* If correct student answers all activate a problem-solving circuit, but incorrect answers activate noisy or partial ones, how might this inform feedback tools or adaptive assessment?\n",
    "    * \n",
    "\n",
    "* Some math explanations follow a \"recall →  rule → justification\" path that activates a known circuit. What might this tell us about procedural vs conceptual understanding in embeddings?\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3006f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Part E. Synthesis and Extrapolation\n",
    "* Why is it useful to know whether a cluster of documents shares an internal circuit vs just a surface-level similarity?\n",
    "    * If they share an internal circuit, we know WHY they map together, that is, we know WHY the model thinks they are similar. THis is very different from seeing that they are similar, and that the model knows they are similar, but not HOW the model knows they are similar.\n",
    "\n",
    "* How could circuit tracing help you identify whether a group of texts shares a common conceptual frame, even across different topics?\n",
    "    * By finding circuits capturing structures/concepts rather than topics. Though it seems largely dependent on the traninig data what the model focuses on as \"differences\" when placing the embeddings (some models might be topic-sensitive while others are structure-sensitive)\n",
    "\n",
    "* What are some limitations of using circuit tracing to explain high-level concepts like “clarity” or “discipline”?\n",
    "    * Interference from other circuits - if we need to change more than one word (in a non-canonical way) we will not be able to isolate these concepts in any meaningful way. We would need to understand the lower level features/circuits before doing this, so that is pobably not possible for many years still."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955b9f7",
   "metadata": {},
   "source": [
    "Part F. More Scenarios\n",
    "* You find that 200 science articles about different physical systems all activate a shared circuit early in the network. However, this circuit is not specific to content — it seems to respond to sentences with causal structure. What kind of errors might this cause in domain-based clustering? How might you identify other clusters driven by abstract structure?\n",
    "    * If we want domain based clustering, we should supress these circuits (ablation). That is actually a very nice application of mech interp!\n",
    "\n",
    "* In a dataset of essays, a subset that switches between narrative and analysis mid-text activates two distinct sub-circuits in alternation. These essays also jump between clusters in embedding space depending on where you cut them. What does this suggest about how local structure influences global embeddings?\n",
    "    *  Highly important, in this case. That shows a certain instability.\n",
    "\n",
    "* Two articles with almost identical embeddings activate very different internal circuits: one focuses on legal precedent, the other on case studies in medicine. What does this tell you about embedding degeneracy? How might this affect how we use proximity for search or summarization?\n",
    "    * If the embedding space is clearly degenerate, the model might rather place very different (never co-ocurring in training data) topics together than similar ones which often co-ocurr and should be separated in training, to avoid interference. It seems pretty hard to avoid this, but luckily almost orthogonal vecotrs grow exponentially with dimension, so (strong) degeneracy might be less and less likely as models grow. (alternatively, the model could be very topic-insensitive and look for structural/other similarities, if that is what it is trained to do).\n",
    "\n",
    "* You discover that articles using analogies activate the same high-level reasoning circuit regardless of domain, and shift embeddings along a nonlinear arc. What does this suggest about shared abstract features, and how might they interfere with simple linear clustering?\n",
    "    * **Speculative**: if analogies are placed on a non-linear subset (an arc/1D smooth manifold), this breaks the linear representation hypothesis. Let's hope that (by potentially expanding the residual stream dimension) the model will prefer linear representations (It is certainly mathematically possible to unfold all arcs in a space without intersection by increasing the dimension, and a linear representation is probably computationally efficient since most model operations are linear)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

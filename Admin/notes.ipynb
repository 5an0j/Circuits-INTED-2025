{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a52941",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Ane\n",
    "\n",
    "- Jina AI (encoder-style transformers / non-causal) tilfredsstiller ikke antagelsene i TransformerLens (non-causal, post-layer norm, mm.). Er du interessert i en wrapper slik at man kan bruke biblioteket på disse? Alternativt kan man bruke transformers.\n",
    "\n",
    "- Interpretability er (antagelig) vanskeligere når outputtet ikke er human-interpretable - f.eks. fordi det ikke finnes noe \"riktig\" output (slik som \"The capitol of Norway is ...\", som lar oss enkelt lage corrupted prompts for activation patching i Gpt-style modeller; \"The capitol of Sweden is ...\" - kanksje må vi bruke causal tracing i stedet, det gir ca. samme info, men uten behov for corrupted prompt). Har vi dessuten en naturlig metrikk for å måle hvor mye outputtet har endret seg? Vi bør se på tapsfunksjonen for Jina AI for å finne noe rimelig (noe alla logit-differnece funker kanksje her også?) \n",
    "\n",
    "- Vil skrive kode som lar deg enkelt utføre: Ablation og activation patching på Jina AI. Vi må finne en god metrikk for encoder-only modeller, og vi må bruke modellens base-output som baseline. (Kanskje dette bare er å implementere causal scrubbing med \"expected loss recovered, ie “what fraction of the expected loss on the subproblem we’re studying does our scrubbed circuit recover, compared to the original model with no edits”\")\n",
    "\n",
    "- Har vi funnet noen circuits fra før i Jina AI som vi kan teste på? Det er ikke klart at de fra GPT-stil transformers også dukker opp her (hverken previous token heads, pga. ALiBi, eller induction heads, pga. non-causal), men kanksje vi finner andre... MLP-circuits er ikke nødvendig vis annerledes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Kan du si litt om hvordan du vil bruke notebooken? Eventuelt, prosjektbeskrivelse?\n",
    "\n",
    "Skal vi gjøre *circuit detection* eller *interventions på pre-etablerte circuits*?\n",
    "\n",
    "Hva bør input/output i notebooken være? \n",
    "\n",
    "Er det et spesifikt datasett vi bør analysere? En bestemt modell? \n",
    "\n",
    "Jina AI har ikke causal masking. \n",
    "\n",
    "-Er vi opptatt av embeddingene til tekstene i det semantiske rommet (og bruker sammenhengen mellom embeddingene til å si noe om kretsene?)\n",
    "\n",
    "\n",
    "\n",
    "#### Theory (til GPT/research)\n",
    "Hvordan velges tokens, og hvordan velger modellen alltid den \"lengste mulige\" token (hvis f.eks. ' cat' og ' ', 'c', 'a', 't' er tokens)?\n",
    "\n",
    "\n",
    "Hvordan bør vi visualisere en transformer - HVA er 'nodene' (i f.eks. Neuronopedia)? Ser vi på et helt attention head som en node? HVa med individuelle noder i MLP-lagene?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b0365",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "**Transformers**\n",
    "- Causal/autoregressive: Uses only previous tokens for prediction, achieved by masking the attention pattern.\n",
    "- Logits: Output vector (length d_vocab) after unembedding but before softmax.\n",
    "- LayerNorm: After transformer block we translate by mean value and normalize (to get variance 1, NB - this is non-linear) adding robustness param $\\epsilon$, and then multiply by weights and add a bias, before passing on to next block.\n",
    "- Attend to: We say that position $k$ attends to position $q$ if the corresp. attention pattern value is large.\n",
    "- Residual stream: Main channel in transformer diagram, tracking the cumulative sum giving the updated embedding vector.\n",
    "\n",
    "- Transformer config:\n",
    "    * batch: input index in batch.\n",
    "    * position: token position.\n",
    "    * d_model: residual stream/embedding dimension\n",
    "    * d_vocab: vocab size\n",
    "    * n_ctx: context size\n",
    "    * d_head: dimension of key/query space in attention head\n",
    "    * d_mlp: mpl hidden layer size (4 x d_model)\n",
    "    * n_heads: number of attention heads (d_model/d_head)\n",
    "    * n_layers: number of layers\n",
    "- Transformer params:\n",
    "    * LayerNorm weights and biases\n",
    "    * $W_E$: Embedding matrix (d_vocab, d_model)\n",
    "    * $W_p$: Position embedding matrix, to give sequential information (n_ctx, d_model)\n",
    "    * $W_U$: Unembedding matrix (d_model, d_vocab) [we may include bias to allow folding in LayerNorm]\n",
    "\n",
    "    * W_{QK}: (product of two matrices - important for interpretability as the component-entries are entangled)\n",
    "        * $W_Q$: Query matrix, for each head in each layer (n_heads, d_model, d_head)¨\n",
    "        * $W_K$: Key matrix (n_heads, d_model, d_head)\n",
    "    * W_{VO}:\n",
    "        * $W_{V\\downarrow}$ and $W_{O\\uparrow}$: Value matrix (decomposed) (n_heads, d_model, d_head) and (n_heads, d_head, d_model)\n",
    "    * Attention biases\n",
    "\n",
    "    * MLP weights and biases (two layers / one hidden layer)\n",
    "\n",
    "**Mech Interp**\n",
    "\n",
    "*General hypotheses/concepts*\n",
    "- Linear representation hypothesis (LH): LLMs choose to use (only) linear representations: Features correspond to directions (Nodes are basis vectors (because of non-linear activations picking out these as a privileged direction), and directions are linear combinations of them)\n",
    "- Superposition: The model knows more concepts (features) than basis directions, hence each node encodes multiple concepts - we thus do not expect features to be represented by a single basis vector (challenge for interpretability). The number of almost orthogonal directions grows exponentially with dimension, so there is room for as many concepts as one would ever want in large models with minimal interference.\n",
    "    - Polysemanticity: A single neuron looks for several (completely unrelated) things (eg for a visual model: cat faces, car fronts and cat legs - we know they are independent by feature visualisation). [Speculative: We want to 'unfold' polysemanticity with SAEs, decomposing into interpretable features in a higher dim space?]\n",
    "- Universality (UH): Analagous features and circuits form across models and tasks (and even in biological brains). Mech Interp goal: 'Periodic table of features/circuits'. If untrue, Mech Interp will fail/should only focus on concrete models of high societal importance, but it seems mostly true.\n",
    "\n",
    "\n",
    "*Methods/keywords*\n",
    "- Activation patching: To identify important model activations, we define a performance metric and make a clean and a corrupted prompt, and pick a specific model activation. We run the model on the corrupted prompt, but intervene on the activation with the clean prompt values to see how much of original performance is restored by this activation.\n",
    "\n",
    "- Attribution: the process of determining which parts of a model (tokens, features, neurons) are responsible for a given output. \n",
    "- Intervention: the act of changing the internal components of a model (like features or activations) to see how output changes.\n",
    "- Hook point: Allows us to intervene/edit the corresp. activation. We apply a hook function which replaces the model activation with our desired intervention (the function output). We can extract an activation by returning nothing on the hook.\n",
    "- Ablation/knockout: 'deleting' one activation (zero/mean/random ablation are different approaches)\n",
    "- Toy model: Simple / very small model for easy interpretability.\n",
    "\n",
    "- TransformerLens: Nanda's library for Mech Interp on GPT2-style LMs, specifically accessing/editing hook points in models ([demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb#scrollTo=Eo1vbABrq9Ba)).\n",
    "\n",
    "*Concepts*\n",
    "- Features: Directions in semantic space / idealised nodes - a basic concept the model has grasped.\n",
    "- Circuits: Connection of features by weights to grasp complex concepts (eg car = wheel + body + windows + doors etc.). Tightly linked features making a subgraph.\n",
    "    - Ex: curve detectors, consisting of mulitple units to span all orientations, can be viewed as one idealised node; a feature, whcich later feeds into circuits which do eg circle detection.\n",
    "- Feature visualisation: By using gradient descent with cost function maximizing the firing of a (known) feature/circuit, we can tweak the model input to maximally represent the feature (this uses LH, and gives a causal link).\n",
    "- Feature implementation: A known circuit can be reimplemented by hand - if the behaviour remains, it is an isolated algorithm, like we want.\n",
    "\n",
    "*Circuits* (sufficiently small subgraphs that one can make falsafiable predictions and reconstruct by hand)\n",
    "- Motifs: Recurring abstract patterns in circuits such as equivariance, superposition, unioning:\n",
    "\n",
    "    - Equivariant circuits: Exhibit a symmetry in the weights corresponding to a symmetry in the problem to be solved. For instance, a curve detector (orientation invariant) is composed of directed curve detectors, and these are essentially rotations of each other.\n",
    "\n",
    "    - Unioning over cases: Often a concept is the union of several cases, eg the concept curve/dog is orientation invariant. The network might separately detect either (inhibiting each other on the way, XOR-properties) and then unionize at the end to get at the general concept.\n",
    "\n",
    "    - Phenomenon superposition: A pure neuron is pushed forward to the output through superpositions to save on neurons (rather than keeping a separate 'trivial' stream encoding dog - dog - dog - ... until the output). Superposition is most useful when the concepts are mostly separate, so that the model can retrieve the correct concept without interference.\n",
    "\n",
    "- Induction circuits: Circuit to detect/continue repeated subsequences. Composed of two consecutive heads (previous token head + induction head, which attends to the next token after previous instance of current token). Work for arbitrary length patterns. Not to hard to find, since they attend to tokens with same spacing.\n",
    "\n",
    "\n",
    "**Jina AI** (BERT-style)\n",
    "- non-causal encoder-style with contrastive training using a mean pooling layer ([arXiv](https://arxiv.org/pdf/2310.19923))\n",
    "- NO positional embeddings, instead mirrored ALiBi (attention with linear bias)\n",
    "- GLU for the FFNN (elementwise sigmoid - not sure of the details of the diffenrence, but it is wider (less interpretable?) than the std mlp archit.)\n",
    "- post-layer normalization only\n",
    "- In GPT-style, you get predicitons for all \n",
    "\n",
    "- Next-token prediction er et diskret problem, mens encoding er kontinuerlig - vi finner ikke analoger til induction circuits, der neste token i et repetert mønster er heldeterminert av en enkel induksjonskrets. \n",
    "- Blir PAD-tokens relevant i non-causal attention?\n",
    "- Can we find few-shot learning heads?\n",
    "\n",
    "\n",
    "- polysemanticity = standard basis is non-interpretable\n",
    "- superposition = there is no interpretable basis (it is over-complete)\n",
    "- Neurons and heads are distinct - in the computation graph view (feature graph), they may both be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5577c5",
   "metadata": {},
   "source": [
    "## Articles/Resources\n",
    "\n",
    "- Arena tutorials: https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp\n",
    "\n",
    "- Neel Nanda Quickstart Guide\n",
    "- Neel Nanda Reading List\n",
    "\n",
    "Demos/tutorials for neuronpedia: \n",
    "\n",
    "\n",
    "- Artikkel: https://transformer-circuits.pub/2025/attribution-graphs/biology.html\n",
    "\n",
    "- Github: https://github.com/safety-research/circuit-tracer?tab=readme-ov-file\n",
    "\n",
    "- Intervention demo: https://github.com/safety-research/circuit-tracer/blob/main/demos/intervention_demo.ipynb\n",
    "- Attribution demo: https://github.com/safety-research/circuit-tracer/blob/main/demos/attribute_demo.ipynb\n",
    "- Circuit tracing tutorial: https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb\n",
    "\n",
    "\n",
    "- Exploration demo: https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb\n",
    "\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a52941",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "#### Ane\n",
    "Bruker du TranformerLens biblioteket?\n",
    "\n",
    "\n",
    "#### Theory (til GPT/research)\n",
    "Hvordan velges tokens, og hvordan velger modellen alltid den \"lengste mulige\" token (hvis f.eks. ' cat' og ' ', 'c', 'a', 't' er tokens)?\n",
    "Hva er hooks i transformers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b0365",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "**Transformers**\n",
    "- Causal/autoregressive: Uses only previous tokens for prediction, achieved by masking the attention pattern.\n",
    "- Logits: Output vector (length d_vocab) after unembedding but before softmax.\n",
    "- LayerNorm: After transformer block we translate by mean value and scale (to get variance 1, NB - this is non-linear) adding robustness param $\\epsilon$, and then multiply by weights and add a bias, before passing on to next block.\n",
    "- Attend to: We say that position $k$ attends to position $q$ if the corresp. attention pattern value is large.\n",
    "- Residual stream: Main channel in transformer diagram, tracking the cumulative sum giving the updated embedding vector.\n",
    "\n",
    "- Transformer config:\n",
    "    * batch: input index in batch.\n",
    "    * position: token position.\n",
    "    * d_model: residual stream/embedding dimension\n",
    "    * d_vocab: vocab size\n",
    "    * n_ctx: context size\n",
    "    * d_head: dimension of key/query space in attention head\n",
    "    * d_mlp: mpl hidden layer size (4 x d_model)\n",
    "    * n_heads: number of attention heads (d_model/d_head)\n",
    "    * n_layers: number of layers\n",
    "- Transformer params:\n",
    "    * LayerNorm weights and biases\n",
    "    * $W_E$: Embedding matrix (d_vocab, d_model)\n",
    "    * $W_p$: Position embedding matrix, to give sequential information (n_ctx, d_model)\n",
    "    * $W_U$: Unembedding matrix (d_model, d_vocab) [we may include bias to allow folding in LayerNorm]\n",
    "\n",
    "    * $W_Q$: Query matrix, for each head in each layer (n_heads, d_model, d_head)\n",
    "    * $W_K$: Key matrix (n_heads, d_model, d_head)\n",
    "    * $W_{V\\downarrow}$ and $W_{O\\uparrow}$: Value matrix (decomposed) (n_heads, d_model, d_head) and (n_heads, d_head, d_model)\n",
    "    * Attention biases\n",
    "\n",
    "    * MLP weights and biases (two layers / one hidden layer)\n",
    "\n",
    "**Mech Interp**\n",
    "\n",
    "*General hypotheses/concepts*\n",
    "- Superposition:\n",
    "- Linearity hypothesis:\n",
    "- Universality: \n",
    "\n",
    "*Methods/keywords*\n",
    "- Activation patching: To identify important model activations, we define a performance metric and make a clean and a corrupted prompt, and pick a specific model activation. We run the model on the corrupted prompt, but intervene on the activation with the clean prompt values to see how much of original performance is restored by this activation.\n",
    "\n",
    "- Attribution: the process of determining which parts of a model (tokens, features, neurons) are responsible for a given output. \n",
    "- Intervention: the act of changing the internal components of a model (like features or activations) to see how output changes.\n",
    "- Hook point: Allows us to intervene/edit the corresp. activation. We apply a hook function which replaces the model activation with our desired intervention (the function output).\n",
    "- Ablation/knockout: 'deleting' one activation (zero/mean/random ablation are different approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5577c5",
   "metadata": {},
   "source": [
    "## Articles/Resources\n",
    "\n",
    "- Arena tutorials: https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp\n",
    "\n",
    "- Neel Nanda Quickstart Guide\n",
    "- Neel Nanda Reading List\n",
    "\n",
    "Demos/tutorials for neuronpedia: \n",
    "\n",
    "\n",
    "-Artikkel: https://transformer-circuits.pub/2025/attribution-graphs/biology.html\n",
    "\n",
    "\n",
    "-Github: https://github.com/safety-research/circuit-tracer?tab=readme-ov-file\n",
    "\n",
    "-Intervention demo: https://github.com/safety-research/circuit-tracer/blob/main/demos/intervention_demo.ipynb\n",
    "-Attribution demo: https://github.com/safety-research/circuit-tracer/blob/main/demos/attribute_demo.ipynb\n",
    "-Circuit tracing tutorial: https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
